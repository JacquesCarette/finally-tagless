I think we should keep our notes separate from the original JFP reviews.
Probably the best thing is to start these 'notes' here from the reviews,
but keep annotating these until we are happy with the results.  From this
we should also be able to generate the response to the editor on what we
did.  So right now, I will 'dump' the original reviews in here, and expect
that this file will get edited lots -- but we'll always be able to refer to
the original reviewer's text in jfp-reviews.txt.

Jacques

\oleg{I took care of simple comments such as references, bibtex
entries, grammatical corrections. I have removed those from this
file. In the reply to referees, we can summarily state that we have
fixed all the grammatical and other such suggestions and inserted
recommended references to show the historical development.}

\oleg{After the second ====... line, please see the reviewers'
comments that are already taken care of. We include them in the
reply.}

\jacques{Since the APLAS paper has appeared, we have been cited twice.
The citations are interesting (and we should cite them here) as they
draw out a different feature of our solution: the importance of
'repr', in other words, the importance of being able to abstract over
a type constructor.  This allows us to simultaneously 1) abstract over
'interpretations', and 2) nevertheless only allow well-typed terms
to be created.  Certainly we need to add something to the 'related work',
but perhaps also to the contributions?}

Another buried TODO:
add a ref to  Lars Birkedal's master thesis
 http://www.itu.dk/people/birkedal/papers/paresm.ps.gz
and thank Eijiro Sumii
In the end, we may also have to thank others (like Neil Jones) if we 
end up making changes that came to us because of our subsequent discussion.

==========================================================================
		Comments yet to address

Mogensen (PEPM 1995) \cite{mogensen-self-applicable}
Sperber (PE 1996) \cite{sperber-self-applicable}

REPORT FROM REFEREE #1

Page 25: The discussion of context, holes and plugging should be made
clearer and more concrete, using a tiny example if possible.  For
instance, the statement that "context is a euphemism for a polymorphic
argument" is puzzling, since a context can also be seen as a function
from term to term, with the context's hole being the argument.  So the
reader wonders whether you or he has misunderstood something or
whether you are proposing a reinterpretation of contexts (as I think
you are).
\jacques{Ken, you started working on this - are you still working on it?}

Page 26 line -2: I don't think there is an universally agreed
distinction between "metacircular interpreter" and "self-interpreter",
so this statement requires some clarification.
\jacques{Three comments:
1) it appears from what I have now read that a consensus is building 
that a "metacircular interpreter" is a special case of a "self-interpreter"
where the language is homoiconic, i.e. the representation of programs
is as a native data-structure of the language itself, ie like LISP.
So \citet{laod93} did not build a metacircular interpreter.
2) I am not even sure that \citet{laod93} actually have a self-interpreter!
They rely way too much on features of the meta-language for their
implementation (like HOAS in particular) which isn't in their object
language).  I do not see a self-interpreter in either Figure 2 or 4 of
that paper.
3) it is very much tagged, Exp is defined as
data Exp a = Exp Rep a
and 'Rep' here is a tag.
4) it is not type-safe!  One can easily build total junk like
three = Exp Succ 3
which will 'type' but not work.  They bypass GADTs by not insuring that
the terms that can be built are well-typed.}

Page 28 section 8 line -3: This is the paper's first mention of
primitive recursion.  This should be discussed much earlier, when the
term representation is introduced.  Also, it is unclear what would be
gained by a non-primitive recursive term representation.


REPORT FROM REFEREE #2

OVERALL EVALUATION

The main weakness of the paper its sloppiness with respect to any
formal claim. Several propositions are stated without offering any
proof. Only a few of them have obviously trivial proofs, but for some
of them I don't have a good idea of how to formalize these claims
properly, let alone how to prove them. 
\jacques{I have implemented a number of fixes for that.}

    \oleg{\ccshan{In response to the message of Neil Jones as of Mar
    31, 2008} I wonder if we can claim that our approach automatically
    assures at least some conditions of the Galois connection. So, the
    type system of the meta-language assures that
	    abstraction . concretization == id
	    abstraction . concretization . abstraction == abstraction
    I wonder is these are the better propositions than the ones in the
    present JFP paper.}
    \jacques{These would most definitely be better propositions.
    However, I don't see how to ``formalize'' that without a fair
    bit more work.  Mention it in the conclusion?}
    \oleg{see my e-mail as of Sep 2}


As it stands, the paper is not sufficiently focused and the main
contribution is sometimes drowned in distracting details. Some of the
"historical context" is also missing in the present submission and
should be added. Furthermore, some technical sloppiness should be
tightened up.
\jacques{Some changes have been made to clean that up -- sufficient?}

DETAILS

The main ideas in the paper date back a long time. I believe they can
be found in more or less diluted form in \cite{Holst-AMIX}
\jacques{Anyone able to get a copy of this?}

The idea that terms can either be encoded by data types or by
higher-order functions (in an executable way) is also due to Reynolds
(might also be in his other 1974 paper):
\oleg{we already have this reference: \cite{reynolds-relation} and
cite him, although in a different context}
\jacques{I don't think this paper is the 'right' one.  In fact,
I think it is in \cite{reynolds-definitional}.  All the papers are
available at ftp://ftp.cs.cmu.edu/user/jcr/, so others can read and
check}

Then there is the typed version of the (Thiemann 1996) paper that you
are citing. It presents a typed partial evaluator encoded exactly like
your interpreters and pe functions, but for offline partial
evaluation. It does not make use of the module system and hence does
not parametrize over different semantics.
\cite{Thiemann-combinators}
\jacques{Aha, now *this* is a very important paper we missed.  I have
just finished reading it, and really does contain a number of our
ideas [except for Symantics!].  It does not deal with self-application
at all, although it quickly discusses 'fold'.  We need to devote a
full paragraph this this paper.  This paper is 43 pages, but that's 
because there are a lot of "proofs" in it.}

There is also a Haskell version using type classes which appeared in
some obscure place. \cite{Thiemann-GenClass}


The idea to parametrize (abstract) semantics and express them in
combinator style is due to Flemming Nielson \cite{Nielson88}
\oleg{63-page paper!}

The idea of dual representation which is attributed to Asai also
appears in \cite{Sperber-SelfApplicable}




p2, section 1
    The translation between the object language and its encoding
    should be made explicit. I was very surprised when section 6
    returned to the object notation without any explanation.
    \jacques{Really?  Sure, that can be done, but L = lambda, A is
    application, all of that is really standard, no?  I guess we could
    mention that, in different guises, we use the language of Fig.1
    throughout the paper.}
    \oleg{I have doubts about Sec 6; Perhaps indeed the encoding
    should be made explicit, just for clarity}

p4, section 1.3
    Clearly the encoding is monadic but you are not mentioning this
    fact at all. I miss a discussion why an abstraction over a monad
    would not be sufficient to achieve all you want to achieve.
    \jacques{We don't mention it because we don't use it, not 
    directly.  We do implement some kind of fold, but over what exactly
    is not clear.  This would be distracting.  I guess we should mention
    that our encoding is indeed monadic, but we (currently?) do not
    make use of this fact}
    \oleg{What does it mean for an encoding to be monadic?
    Where in the R or C interpreters we fix the evaluation order?}

    It should also be discussed in what way the type system of the
    metalanguage restricts the expressible type systems for the object
    language.
    \jacques{Now, that is a much more interesting comment.  To be 
    addressed fully could take a fair bit of space.  We should however
    discuss this a little.  Probably not in 1.3, but perhaps at the end
    of section 4?}

p5, section 1.4
    "even call-by-name CPS"
    Why is CBN more difficult or more complicated than CBV in this
    context? 

p11, proposition 4
     This is not formalized, but really obvious (unlike the
     others). However, the mentioned properties of .~ and .< >. that
     you rely on are tricky and non-obvious.
     \jacques{made this much more formal, and added a comment about the
     properties.  Still missing is a reference}

p11, section 4
     "Surprisingly ..."
     Not that surprising in the light of Filinski's work on
     normalization by evaluation.

     It would be appropriate for you to state that you are after
     online partial evaluation in this section and to briefly explain
     what that means.
     \oleg{I agree; we are after online PE...}



p15, section 4.4
     btw, I do not find these names R, L, and C very intuitive or
     readable. 
     \jacques{We could insert a 'key', where
     evaluator (i.e. run)  : R
     length                : L
     compile               : C
     partial evaluator     : P
     example               : E
     evaluator (by-name)   : RCN
     evaluator (by-value)  : RCV
     CPS transformer       : CPST
     state-passing CBN CPS : RCPS}
     \oleg{Not a bad idea...}

p16
     The optimizing online pe is completely besides the point of this
     work and obscures the code in Fig 5. 
     \jacques{Sigh.  Quite true - but then the results are quite ugly.
     The 'optimizing' is so easy, does it really obscure things so much?
     I would prefer to keep this minimal optimization in, we've already
     cut out significant parts of the 'optimization' present in our
     working code}
     \oleg{I agree; let's keep our optimizations and argue for them in
     our response to the reviewer.}

p16
     Also, online pe is intrinsically tagged so offline pe would
     provide a better demonstration.
     \jacques{I quite disagree that offline would provide a better
     demonstration!  Offline introduces an extraneous BTA pass and
     annotations which would be even more distracting.  Also, I am
     not sure I understand how "online pe is intrinsically tagged" --
     all PE requires binding-time annotations, which are usually done
     with tags, whether online or offline.  So I think this comment is
     just off the mark}
     \oleg{Perhaps we should argue for that in our response and don't
     change the paper. At least we should ask for more clarification
     from the reviewer.}

p17, "partial evaluation ... gives the desired result:"
     But the good result is due to the extra cleverness put into the
     implementation of 'add' and 'mul': without those, the result
     would not be much different.
     \jacques{Exactly!  And that extra cleverness is clear, localized,
     and simple.  So why not do it?  The beauty of this approach is that
     it makes it clear where to put these optimizations.  But that is
     really part of a 'next' paper.  I feel like leaving this in, but
     adding some words to the conclusion about this being an area to
     pursue.}
     \oleg{I agree with you, as I said before...}     

p18, CPS interpretation
     The cbv version is almost literally in the above-cited paper
     (Thiemann 1999) \cite{Thiemann-combinators}
     \jacques{Of course!  It is the obvious way to do it.  We do not
     claim originality for that, do we?  The originality is that
     CPS interpretation 'fits' the whole scheme}

     A few details of the representation should be discussed in the
     paper. In particular, you should discuss why you cannot pull this
     strategy off for a Plotkin-style CBV-CPS interpreter (you need to
     use Reynolds style as shown later in the paper, but without
     explanation why it has to be this complicated). 
     \jacques{I am sure one of you would be better suited to deal with 
     this one.}

     Actually, you can write a CBV-CPS interpreter which implements
     lam f in exactly the same way as stated in the paper by extending
     the Symantics signature accordingly.
     \jacques{Sure, but why extend if it's not needed?}

p19, starting with CPST
     It's nice to see that you can unify all these transformations
     under one signature, but is this still a natural way of dealing
     with the transformations? It seems to me like each transformation
     needs its specific type infrastructure. To me, the interesting
     question here is, where does this specialization stop? Which
     features are needed to obtain a "fixpoint"?
     \jacques{Not 'each'.  And while this is an interesting question,
     it is out-of-scope of this current paper.  In any case, without a nice set
     of test cases, it would be quite hard to make the required set
     of features precise.  In all cases though, it looks like only
     simple (i.e. total) type-level functions are needed.}

p20, section 5.3
     This feels like a hack to me. Why can't you write everything in
     monadic style and then plug in a state monad when needed?

     If you do something specialized for state, then a polymorphic
     treatment of reference operators would be more convincing to me!

p21, lapp is monadic let!
     It's only needed to drive the CPS transformation. 
     Its presence should be explained like that.

     Why is the ... result type in RCPS needed?

p22, section 6
     I was confused by the switch in syntax (which makes sense once
     you think about it). Some comments in the text as to why you
     switch notation  would make it easier to your readers.
     
     "optimal with respect to si"
     Why the change in typeface? Is this another SI?

p24, "pre-encoding of a let-"
     Why not encode a let as a function like any other syntax
     constructor? It is a bit counter-intuitive that you don't encode
     it as "let 'e1' (\x. 'e2')"

     To me, using the contexts and context plugging is changing the
     problem: The problem statement says "apply the SI".
     The really interesting question is which feature set is needed to
     obtain an SI without changing the question.

p25, proposition 5
     proof?

     "Our partial evaluator ..."
     should start on a new line.

     proposition 6
     It is immediate that PE (SI['e']) is observationally equivalent
     to PE(e), but I do not see how you prove that they are
     alpha-equivalent.

p25, section 6.3
     The second paragraph contains rather haphazard statements. These
     statements need to be properly formulated and require proof. It
     is not enough to just throw in a few provocative statements:
     proof is needed!
\oleg{I'm inclined to say that this paragraph does not describe
technical results. Rather, it states intuitions that arose during our
attempts to write a self-interpreter. Intuitions (points of view) are
necessarily informal. One may say that they are more important than
the technical results because they show a way to derive technical
results. Because of the informality, they cannot be applied
mechanically and so cannot, alas, benefit every reader. It still seems
that their value outweighs the drawback: intuition is what
distinguishes an informal mathematical proof from the formalized one.
The reason mathematicians prefer informal proofs is that they are
interested not only in what has been proven but why (refer to Thurston).}
\jacques{agreed}


p28, "This shift also underlies ..."
     I don't see the connection to the present work.
     \jacques{I must admit that this is indeed not obvious.  Should
     we add some weasel words like "We believe that this shift..."?}

     "higher-kind polymorphism"
     Where is that needed in the present work?
     \jacques{should we put some explicit sentence about higher-kind
     polymorphism at the end of 6.1?  [as in, if we had xxx feature
     in the object language, we coudl do it].  Then we could put in 
     a backlink.}
     \oleg{If we remove Sec 6, that point would be moot...}

==========================================================================
	Comments that are already taken care of

We have fixed all the grammatical and other such suggestions and
inserted recommended references to show the historical development.

Rev1:
The main shortcoming of the paper is the lack of discussion of which
features of the meta languages are essential for typed and tag-less
term representation.  For instance, it seems to be important that one
can abstract over a signature (a type and its operations), whether
done using Haskell constructor classes or OCaml signatures and
functors.  But is this just an accidental feature of this technique,
or essential to solving the problem? ...
In any case the paper should present, early on, what features of the
meta languages (Haskell and OCaml) are used, and discuss to what
extent similar encodings would be possible in Standard ML (using only
signatures, functors and the core type system as defined by Milner et
al 1997).
\ccshan{We have clarified in Section 1.4 that indeed, our use of Haskell
constructor classes or OCaml/SML signatures and functors is essential.}

Rev1:
Also, the paper should discuss up front what is lost when using this
encoding.  For instance, could it happen that an existing well-formed
term (written as an OCaml functor, say) will become ill-typed or may
have to be rewritten when the object language (the functor argument)
is extended with new syntactic constructs?  Or will such existing
terms be oblivious to pure extensions?
\ccshan{We are not sure what "lost" means, but we now state in Sections
1.4 and 5.3 that "extending the language does not invalidate terms
already encoded."}

Rev1:
Page 12:
Torben Mogensen was the first to use a dual representation of terms
and values (in his HOAS-based self-interpreter for the untyped lambda
calculus, JFP ca 1994)
\ccshan{We now discuss Mogensen's use of Church encoding in Section 1.3,
and his (subtly and slightly different) dual representation of terms
and values in Section 7.}

Rev1:
Page 17:
Some people would say that Haskell has call-by-need rather than
call-by-name.
\ccshan{We note the difference.}

Rev1:
Page 4 line -5: Is there some underlining or other part of the OCaml
error message missing?  One has to read further to see that "this
expression" refers to the argument "()".
\oleg{I underlined the term that the error message complained about.
please search for \underline.}

Rev1, Page 17: Introduce the abbreviation CPS somewhere.
\oleg{it has been done already, in the last contribution, in Sec 1.4}

Rev1, Page 20: Say whether the additional phantom type parameter
to repr causes any real problems (other than verbosity).
\ccshan{We say no now.}

Rev1, Page 12: Please give mnemonic help to the constructor names "S0" and
"E0".
\oleg{E0 is now renamed to D0 for better mnemonic; added a comment
that 0 is a version suffix}.

Rev2: p13, section 4.3
     I do not understand the purpose of this section. 
     * It leads to a dead end and clarifies nothing (for me).
     * It rephrases known results.
     * Its interpreter still involves tagging.
     * It runs counter the advertizing in the abstract that no GADTS
       are needed for the purposes of this paper.
     Hence, I suggest that this subsection be removed.
    \oleg{We have removed that in the APLAS version. Perhaps we can
    remove it here, too. There is no need to use Haskell then in the
    paper. I think we should still keep the Symantics type class in
    Sec 2.1, for a good comparison. After that, we can say that we
    have Haskell code. I think we already say that in the APLAS
    version.}
\oleg{this section is removed}

Rev1, Page 14: In the definition of "abstr" maybe use C.int, C.bool and so
on to indicate where these terms come from.
\oleg{We removed that section and that code}

Rev1, Page 18: 
Here the OCaml higher-rank record types seem to play an essential
role.  This raises the question whether the same encoding could be
made in Standard ML (as defined by the Milner et al book 1997)?  This
question must be answered in the paper.
\ccshan{We state that RCN can also be defined as a functor parameterized
over the answer type, to avoid the use of higher-rank polymorphism in
the core language.}
\jacques{Be even more explicit about the functor variant, and explicitly
mention Standard ML -- but bemoan the polymorphism and usability loss}

Rev1:
The rapid change between OCaml and Haskell is not always helpful. It
would be better to stick to one formulation throughout the paper
and explain the other e.g. in an appendix.
\jacques{I have finished removing all of the Haskell that was not
essential to the paper.}

Rev2, p3, line -1
    "the universal type solution is unsatisfactory because it does not
    preserve typing" 
    I don't understand this remark. The encoding certainly preserves
    typing. But that typing does not reflect the type of the encoded
    term. 
\jacques{Correct, I used the above phrasing instead.}

Rev2, p6, "inference-preserving module system"
    What's that?
    \oleg{I guess I meant this: in ML, we can write structures without
    explicit signature annotations. The signature will be
    inferred. Mainly, for any structure (whether it has been annotated
    with a signature or not) we can just say "open M" and use the terms of
    M in any ML expression without any need to write any type
    annotation. By contrast, consider an object system of OCaml (or of
    any other typed OOP). An object system can be thought of as a module
    system (and often plays this role, e.g. in Java and C++). Here, in
    class declarations, we must write full annotations. Mainly, when
    we use objects, we have to, occasionally, write type
    annotations (e.g., when writing coercions). So, in OCaml, objects
    is an example of a non-inference preserving module system.}
    \jacques{"inference-preserving" has already been removed from the text}

Rev2, p7, proposition 1
    This statement sounds fine on the surface, but how would you prove
    it? What is the relation between object types and the types of
    their encodings? How is the statement generalized to open terms
    (as it would be necessary for a proof by induction)?
    \jacques{The relation is really on the type-system rules, and that
    relation is an easy bijection.  I have inserted a 'proof' of this
    proposition which I think really clarifies this.}

Rev2, p8, section 2.2
    you are abstracting the interpreter over the term representation,
    not the other way round (or I'm missing something).
    "Each interpreter [is] ..."
    -> [can be expressed as]
    \jacques{The reviewer is missing something.  I have nevertheless 
    rephased the first sentence to make it clear that our term
    representation is independent of interpretations.  The sentence
    "Each interpreter [is]..." stays as it is correct.

Rev2, p9, module R
    You claim in the abstract that you are not using dependent
    types. However, using the module system means that you are, in
    fact, using dependent types, at least in a stylized way. Hence, as
    it stands, the abstract is incorrect.
    \jacques{We make very heavy use of type-dependent-types, but not do
    not use value-dependent-types, which is what is usually understood
    when talking about 'dependent types'.}
    \jacques{We might want to make a comment about this in the text itself,
    but I am unsure where}
Rev2, p9,
    proposition 2
    makes no distinction between meta types and object types
    \jacques{statement clarified}

Rev2,p9,
    proposition 3
    This is very fuzzy. I have no immediate intuition how to formalize
    the statement of this proposition.
    \jacques{I have tried to rectify this.  It needs to be double-checked}

Rev2,p16, "the type equation for repr_pe above"
     * "above" is fuzzy
     * it expresses the data definition, but not the type function
     \jacques{fixed}

\jacques{I have added parts of the following 2 paragraphs from Ref 2
into the text}

The authors have picked up ideas that were lying around for a long
time and have put them together in an original way. A very short way
of summarizing the paper would be that it proposes a way to write a
typed fold function over a typed term.

The main contribution of the paper is the realization that the OCaml
module system as well as Haskell's class system can be used to
parametrize the semantics of a particular style of syntax encoding for
lambda terms. The demonstration of this contribution is done
meticulously and convincingly.

Rev1:
p1, abstract
    "no dependant types, ..."
    I believe you want to say "no language extensions beyond the
    published standards"
    \oleg{see his remark about p9. I think modules are NOT instances
    of dependent types in the conventional sense, and so our current
    phrase in the abstract stands. Should be say a few words about it,
    that is, what we mean by dependent types exactly?}
    \jacques{I agree.  Dependent types, in the conventional sense, are
    term-dependent-types, which modules do not give us.}

Rev1:
p1, citation
    The other Reynolds citation that emphasizes the duality between
    first-order and higher-order term representations would be more
    appropriate. 
    \jacques{"other" citation?  He certainly does not mean the other
    Reynolds paper we cite, not from my reading of it (it is available
    from Reynold's home page.  So what does this mean?}
    \oleg{we just leave this as the question to the reviewer}
