Dear Colin,

The referee comments were very useful to us, but required quite a bit
of work on our part to address -- thus the long delay.  

Every small error pointed out by the referees has been fixed.  We have also
added many new references (those pointed out by the referees as well as 
additional references, and appropriate discussion) to put our work in its
proper historical context.  We will not comment any further on these topics.

As we have removed one section (section 6 in the previous version) dealing
with self-interpretation, the referees' comments on this section are now
moot, and will not be commented upon either.  The comments were indeed the
reason we chose to remove this section, as the work in that section was
indeed not ready for publication (and not so easily ``fixed'').

Reviewer 2 wrote:
^^^^^^^^^^^^^^^^^
The authors have picked up ideas that were lying around for a long
time and have put them together in an original way. A very short way
of summarizing the paper would be that it proposes a way to write a
typed fold function over a typed term.

The main contribution of the paper is the realization that the OCaml
module system as well as Haskell's class system can be used to
parametrize the semantics of a particular style of syntax encoding for
lambda terms. The demonstration of this contribution is done
meticulously and convincingly.
-------
We have paraphrased both of these paragraphs in appropriate places in
the text - thanks!

Below, we detail what has been done to address each substantial comment.

==========================================================================

Reviewer 1:


The main shortcoming of the paper is the lack of discussion of which
features of the meta languages are essential for typed and tag-less
term representation.  For instance, it seems to be important that one
can abstract over a signature (a type and its operations), whether
done using Haskell constructor classes or OCaml signatures and
functors.  But is this just an accidental feature of this technique,
or essential to solving the problem? ...
In any case the paper should present, early on, what features of the
meta languages (Haskell and OCaml) are used, and discuss to what
extent similar encodings would be possible in Standard ML (using only
signatures, functors and the core type system as defined by Milner et
al 1997).
--
We have clarified in Section 1.4 that indeed, our use of Haskell
constructor classes or OCaml/SML signatures and functors is essential.


Also, the paper should discuss up front what is lost when using this
encoding.  For instance, could it happen that an existing well-formed
term (written as an OCaml functor, say) will become ill-typed or may
have to be rewritten when the object language (the functor argument)
is extended with new syntactic constructs?  Or will such existing
terms be oblivious to pure extensions?
--
We are not sure what "lost" means, but we now demonstrate in Sections
1.4 and 5.3 that "extending the language does not invalidate terms
already encoded."

Page 12:
Torben Mogensen was the first to use a dual representation of terms
and values (in his HOAS-based self-interpreter for the untyped lambda
calculus, JFP ca 1994)
--
We now discuss Mogensen's use of Church encoding in Section 1.3,
and his (subtly and slightly different) dual representation of terms
and values in Section 7.

Page 17:
Some people would say that Haskell has call-by-need rather than
call-by-name.
--
We note the difference, in footnote 2.

Page 4 line -5: 
Is there some underlining or other part of the OCaml
error message missing?  One has to read further to see that "this
expression" refers to the argument "()".
--
The term that the error message complained about has been underlined.

Page 20: 
Say whether the additional phantom type parameter
to repr causes any real problems (other than verbosity).
--
We now mention that it does not.

Page 12: 
Please give mnemonic help to the constructor names "S0" and "E0".
--
E0 is now renamed to D0 for better mnemonic; added a comment
that 0 is a version suffix.

Page 14: In the definition of "abstr" maybe use C.int, C.bool and
so on to indicate where these terms come from.
--
We removed that section and that code, as recommended by reviewer 2.

Page 18: 
Here the OCaml higher-rank record types seem to play an essential
role.  This raises the question whether the same encoding could be
made in Standard ML (as defined by the Milner et al book 1997)?  This
question must be answered in the paper.
--
We state, at the end of Sec 5.1, that RCN can also be defined as a
functor parameterized over the answer type, to avoid the use of
higher-rank polymorphism in the core language.  In fact, we are quite
explicit about the functor variant, and explicitly mention Standard ML
-- but bemoan the polymorphism and usability loss.


The rapid change between OCaml and Haskell is not always helpful. It
would be better to stick to one formulation throughout the paper
and explain the other e.g. in an appendix.
--
All of the Haskell which was not essential to the paper has been removed.
The accompanying code (available on the bwe) still has both full
implementations.


The idea of dual representation which is attributed to Asai also
appears in [Sperber]
--
    Thank you; we now cite Sperber alongside Asai (as Sumii and
    Kobayashi do).  We also now mention Consel, who also had something
    similar in Schism.

Reviewer 1, Page 28 section 8 line -3: This is the paper's first mention of
primitive recursion.  This should be discussed much earlier, when the
term representation is introduced.  Also, it is unclear what would be
gained by a non-primitive recursive term representation.
--
The issue is mentioned much earlier now, along with the relation to folds.

==========================================================================

Reviewer 2: 

p13, section 4.3
     I do not understand the purpose of this section. 
     * It leads to a dead end and clarifies nothing (for me).
     * It rephrases known results.
     * Its interpreter still involves tagging.
     * It runs counter the advertizing in the abstract that no GADTS
       are needed for the purposes of this paper.
     Hence, I suggest that this subsection be removed.
--
This section is removed

p3, line -1
    "the universal type solution is unsatisfactory because it does not
    preserve typing" 
    I don't understand this remark. The encoding certainly preserves
    typing. But that typing does not reflect the type of the encoded
    term. 
--
Correct, the above better phrasing is now used instead.

p4, section 1.3
    It should also be discussed in what way the type system of the
    metalanguage restricts the expressible type systems for the object
    language.
--
Now discussed at the end of Section 5.3.

p6, "inference-preserving module system"
    What's that?
--
This particular phrase has been removed.  However, what we meant was that
in ML, we can write structures without
explicit signature annotations. The signature will be
inferred. Mainly, for any structure (whether it has been annotated
with a signature or not) we can just say "open M" and use the terms of
M in any ML expression without any need to write any type
annotation. By contrast, consider an object system of OCaml (or of
any other typed OOP). An object system can be thought of as a module
system (and often plays this role, e.g. in Java and C++). Here, 
we must write full annotations, not only in the interface declarations
but also in all class declarations. Mainly, when
we use objects, we have to, occasionally, write type
annotations (e.g., when writing coercions). So, in OCaml, objects
is an example of a non-inference preserving module system.

p7, proposition 1
p9, proposition 2
p11, proposition 3
p16, proposition 4
    (about prop 1)This statement sounds fine on the surface, but how would you
    prove it? What is the relation between object types and the types of
    their encodings? How is the statement generalized to open terms
    (as it would be necessary for a proof by induction)?
    [similar comments about other propositions elided]
Reviewer 2, Overall
    The main weakness of the paper its sloppiness with respect to any
    formal claim. Several propositions are stated without offering any
    proof. Only a few of them have obviously trivial proofs, but for some
    of them I don't have a good idea of how to formalize these claims
    properly, let alone how to prove them. 
--
All propositions have now been made precise.  For the more 
important ones, proof sketches have been added.  Significant work has gone
into this aspect of the paper.

p8, section 2.2
    you are abstracting the interpreter over the term representation,
    not the other way round (or I'm missing something).
    "Each interpreter [is] ..."
    -> [can be expressed as]
--
We really are abstracting over both the term representation as well as the
meaning of 'interpreter'.  We rephased the first sentence to make it
clear that our term
representation is independent of interpretations.  The sentence
"Each interpreter [is]..." stays as it is correct.

p9, module R
    You claim in the abstract that you are not using dependent
    types. However, using the module system means that you are, in
    fact, using dependent types, at least in a stylized way. Hence, as
    it stands, the abstract is incorrect.
--
We make very heavy use of type-dependent-types, but not do
not use value-dependent-types, which is what is usually understood
when talking about 'dependent types'.  We explicitly mention type-indexed-types
in the text (and cite references).

p16, "the type equation for repr_pe above"
     * "above" is fuzzy
     * it expresses the data definition, but not the type function
--
Fixed.

p2, section 1
    The translation between the object language and its encoding
    should be made explicit.
--
An explicit encoding has been added in section 2.1

p11, section 4
     "Surprisingly ..."
     Not that surprising in the light of Filinski's work on
     normalization by evaluation.
     It would be appropriate for you to state that you are after
     online partial evaluation in this section and to briefly explain
     what that means.
--
At the beginning of Section 4, we now clarify
that what's surprising is that the family of interpretations can
encompass partial evaluation, and state up front that our partial
evaluator is online.

p16
     Also, online pe is intrinsically tagged so offline pe would
     provide a better demonstration.
--
Our paper stresses a family of multiple
interpretations for the same object language, and offline PE
would force us to change the object language to one annotated
with binding times, so online PE distracts from our point less.
Note that all PE, offline or online, is tagged in one way or another.

p16
     The optimizing online pe is completely besides the point of this
     work and obscures the code in Fig 5. 
p17, "partial evaluation ... gives the desired result:"
     But the good result is due to the extra cleverness put into the
     implementation of 'add' and 'mul': without those, the result
     would not be much different.
--
These optimizations are indeed a side point
and do not affect the result of EX(P) much, but we find the
ease of adding these optimizations and the clarity of where to
put them so appealing, simple, and undistracting that we think
the paper is still better for them.  As a matter of fact, our 
implementation provides significantly more optimizations (because they
were so easy to implement).

p18, CPS interpretation
     The cbv version is almost literally in the above-cited paper
     (Thiemann 1999) \cite{Thiemann-combinators}
     \ccshan{Noted at the beginning of Section 5}

     A few details of the representation should be discussed in the
     paper. In particular, you should discuss why you cannot pull this
     strategy off for a Plotkin-style CBV-CPS interpreter (you need to
     use Reynolds style as shown later in the paper, but without
     explanation why it has to be this complicated). 
     Actually, you can write a CBV-CPS interpreter which implements
     lam f in exactly the same way as stated in the paper by extending
     the Symantics signature accordingly.
--
This is now discussed at the beginning of Section 5.2

p19, starting with CPST
     It's nice to see that you can unify all these transformations
     under one signature, but is this still a natural way of dealing
     with the transformations? It seems to me like each transformation
     needs its specific type infrastructure. To me, the interesting
     question here is, where does this specialization stop? Which
     features are needed to obtain a "fixpoint"?
--
In the new Section 5.3, we show a general type
infrastructure that handles all these CPS transformations.

p20, section 5.3
     This feels like a hack to me. Why can't you write everything in
     monadic style and then plug in a state monad when needed?

     If you do something specialized for state, then a polymorphic
     treatment of reference operators would be more convincing to me!
--
We now note at the beginning of Section 5.4 (previously
5.3) that our support for mutable state generalizes to other
monadic effects, because we can always represent a monadic effect
using continuations.  We also note at the end of Section 5.4 that
another way to support state is indeed to "write everything in
monadic style and then plug in a state monad when needed".

p28
     "higher-kind polymorphism"
     Where is that needed in the present work?
--
Symantics defines repr, which is higher-kinded and that is crucial. 
The need for this is mentioned in a couple of places, and in particular
at the end of Section 5.3.

p21, lapp is monadic let!
     It's only needed to drive the CPS transformation. 
     Its presence should be explained like that.
--
     We now explain lapp as "equivalent to `let' in Moggi's
     monadic metalanguage.

p21
     Why is the ... result type in RCPS needed?
--
It was just for internal convenience.  It is now removed from the paper.

*************************************************************************

Reviewer 1:

p1, citation
    The other Reynolds citation that emphasizes the duality between
    first-order and higher-order term representations would be more
    appropriate. 
    \jacques{"other" citation?  He certainly does not mean the other
    Reynolds paper we cite, not from my reading of it (it is available
    from Reynold's home page.  So what does this mean?}
    \oleg{we just leave this as the question to the reviewer}

3 comments from Reviewer 2:

p4, section 1.3
    Clearly the encoding is monadic but you are not mentioning this
    fact at all. I miss a discussion why an abstraction over a monad
    would not be sufficient to achieve all you want to achieve.
    \oleg{What does it mean for an encoding to be monadic?
    Where in the R or C interpreters we fix the evaluation order?}

p15, section 4.4
     btw, I do not find these names R, L, and C very intuitive or
     readable. 
     \jacques{We could insert a 'key', where
     evaluator (i.e. run)  : R
     length                : L
     compile               : C
     partial evaluator     : P
     example               : E
     evaluator (by-name)   : RCN
     evaluator (by-value)  : RCV
     CPS transformer       : CPST
     state-passing CBN CPS : RCPS}
     \oleg{Not a bad idea...}

p28, "This shift also underlies ..."
     I don't see the connection to the present work.
