\hyphenation{meta-language meta-languages Meta-OCaml meta-cir-cu-lar type-case}
\documentclass[preprint]{sigplanconf}
\usepackage{amsmath,amssymb}
\usepackage{mathptmx}
%\usepackage{txfonts} \renewcommand{\ttdefault}{cmtt}
\usepackage[T1]{fontenc}
\usepackage[override]{cmtt}
\usepackage{comment}
\usepackage{hyphenat}
\usepackage{url}
\usepackage{prooftree1} \proofrulebaseline=1.5ex \proofdotseparation=.75ex
\usepackage{mdwlist}
\usepackage{mdwtab}
\usepackage{dblfloatfix}

\newcommand{\jacques}[1]{{\it [Jacques says: #1]}}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}
\newcommand{\ccshan}[1]{{\it [Ken says: #1]}}

\usepackage{natbib1}
\let\cite=\citep

\usepackage[compact]{fancyvrb1}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=\parindent}

\newtheorem{prop}{Proposition}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\BB}{\mathbb{B}}

\newcommand{\fun}[1]{\mathopen{\lambda\mathord{#1}.\,}}
\newcommand{\fix}[1]{\mathopen{\mathrm{fix\,}\mathord{#1}.\,}}
\newcommand{\Forall}[1]{\mathopen{\forall\mathord{#1}.\,}}
\newcommand{\Exists}[1]{\mathopen{\exists\mathord{#1}.\,}}
\newcommand{\cond}[3]{\mathrm{if\ }#1\mathrm{\ then\ }#2\mathrm{\ else\ }#3}
\newcommand{\be}[1]{\mathrm{let\ }#1\mathrm{\ in\ }}
\newcommand{\True}{\mathinner{\mathrm{true}}}
\newcommand{\False}{\mathinner{\mathrm{false}}}
\newcommand{\Encode}[1]{\mathinner{\mathopen{\text{``}}#1\mathclose{\text{''}}}}
\newcommand{\encode}[1]{\mathinner{\mathopen{\text{`}}#1\mathclose{\text{'}}}}
\newcommand{\ident}[1]{\mathinner{\text{\textit{#1\/}}}}
\newcommand{\subst}[3]{#1\left\{#2\mapsto#3\right\}}
\DeclareMathOperator{\pe}{PE}
\DeclareMathOperator{\si}{SI}

% Make space around section headings no less than space around theorems
\makeatletter
\@tempdima\topsep \advance\@tempdima\parsep
\ifdim\@sectionbelowskip   <\topsep \@sectionbelowskip   \topsep \fi
\ifdim\@subsectionbelowskip<\topsep \@subsectionbelowskip\topsep \fi
\makeatother

% Allow things to flow a little more tightly around figures
\renewcommand\floatpagefraction{.95}
\renewcommand\topfraction{.95}
\renewcommand\bottomfraction{.95}
\renewcommand\textfraction{.05}   

% Match the rule inserted by \@makecaption in sigplanconf.cls
\setproofrulebreadth .33pt
\newenvironment{floatrule}
    {\hrule width \hsize height .33pt \vspace{.5pc}}
    {\par\addvspace{1ex}}

% Make some Rel symbols into Bin symbols instead
\DeclareMathSymbol{\to}{\mathbin}{symbols}{"21}
\DeclareMathSymbol{:}{\mathbin}{operators}{"3A}

\begin{document}

\conferenceinfo{ICFP '07}{Freiburg, Germany} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

\makeatletter
\titlebanner{Submitted to \@conferencename}        % These are ignored unless
\preprintfooter{\rlap{\@titletext}}   % 'preprint' option specified.
\def \@formatyear {\llap{\number\year/\number\month/\number\day}}
\makeatother

\title{Finally Tagless Staged Interpreters for Simpler Typed Languages}
%Staged typed type-preserving tagless ``interpreters'', \emph{finally}
%Finally tagless, simply typed, partially evaluated
\subtitle{\raisebox{-2.5em}[0pt][0pt]{\normalsize\it\begin{minipage}{30em}
    It should also be possible to define languages with a highly
    refined syntactic type structure. Ideally, such a treatment should
    be metacircular, in the sense that the type structure used in the
    defined language should be adequate for the defining language.
    \hfill\rm\citep{reynolds-definitional}
\end{minipage}}}

\begin{comment}
\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}
\end{comment}
\authorinfo{}{}{}

\maketitle

\begin{abstract}
We have built the first family of tagless interpretations for a
higher-order typed object
language in a typed metalanguage (Haskell or ML) that
require no dependent
types, generalized algebraic data types, or
postprocessing to eliminate tags.
The interpretations include
an evaluator, a compiler (or staged evaluator), a partial
evaluator, and call-by-name and
call-by-value CPS transformers.

Our main idea is to encode
de Bruijn or higher-order abstract syntax
using cogen functions rather than data constructors.
In other words, we represent object terms not in an initial algebra
but using the coalgebraic structure of the $\lambda$-calculus.
Our representation also simulates inductive maps from types to
types, which are required for typed partial evaluation and CPS transformations.

Our encoding of an object term abstracts over the various ways to
interpret it yet statically assures that the interpreters never get
stuck.  To achieve self\hyp interpretation and show Jones\hyp
optimality, we relate this exemplar of higher-rank and higher-kind
polymorphism (provided by ML functors and Haskell~98 constructor
classes) to plugging a term into a context of let\hyp polymorphic
bindings.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}\label{intro}

A popular way to define and implement a language is to embed it in
another \citep{reynolds-definitional}.  Embedding means to represent
terms and values of the \emph{object language} as terms and values in the
\emph{metalanguage}.  Embedding is especially appropriate for domain\hyp
specific object languages because it supports rapid prototyping and integration
with the host environment \citep{hudak-building}.
If the metalanguage supports \emph{staging}, then
the embedding can compile object programs to the metalanguage and avoid the
overhead of interpreting them on the fly \citep{WalidICFP02}.  A staged
definitional interpreter is thus a promising way to build a domain\hyp specific
language.

\begin{figure}
    \begin{floatrule}
    \begin{proofrules}
        \[ \[ [x:t_1] \proofoverdots e:t_2 \] \justifies \fun{x}e:t_1\to t_2 \]
        \[ \[ [f:t_1\to t_2] \proofoverdots e:t_1\to t_2 \] \justifies \fix{f}e:t_1\to t_2 \]
        \[ e_1:t_1\to t_2 \quad e_2:t_1 \justifies e_1 e_2: t_2 \]
        \[ \text{$n$ is an integer} \justifies n:\ZZ \]
        \[ \text{$b$ is a boolean} \justifies b:\BB \]
        \[ e:\BB \quad e_1:t \quad e_2:t \justifies \cond{e}{e_1}{e_2}:t \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1+e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \times e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \le e_2:\BB \]
    \end{proofrules}
    \end{floatrule}
    \caption{Our typed object language}
    \label{fig:object}
\end{figure}

We focus on embedding a typed object language into a typed metalanguage.
The benefit of types in this setting is to rule out meaningless object terms,
thus enabling faster interpretation and assuring that our interpreters
do not get stuck.
To be concrete, we use the typed object language in
Figure~\ref{fig:object} throughout this paper.  We aim not just for
evaluation of object programs but also for
compilation, partial evaluation, and other processing.

\Citet{WalidICFP02} and \citet{xi-guarded} motivated interpreting
a typed object language in a typed metalanguage as an interesting
problem.  The known solutions to this problem store object terms and
values in the metalanguage in a universal type, a generalized algebraic
data type (GADT), or a dependent type.  In the remainder of this section,
we discuss these solutions, identify their drawbacks, then summarize our
proposal and contributions.  
No matter how we represent the object language in the
metalanguage, the representation can be created either by hand (for example, by
entering object terms at a metalanguage interpreter's prompt) or
by a parser\slash type\hyp checker reading from a text string.
We leave aside the solved problem of writing such a parser\slash type\hyp checker,
whether using dependent types \citep{WalidICFP02} or not \citep{baars-typing}.

\subsection{The tag problem}\label{tagproblem}

%\oleg{see tagless\_interp1.ml, module Tagfull for the complete code.
%  If you change the code in here, please adjust the .ml file
%  accordingly. Let the paper and the accompanying code be in sync.}

It is straightforward to create an algebraic data type, say in OCaml, to
represent object terms such as those in Figure~\ref{fig:object}.
For brevity, we elide treating integers, conditionals, and fixpoint in
this section.
\begin{code}
type var = VZ | VS of var
type exp = V of var | B of bool
         | L of exp | A of exp * exp
\end{code}
We represent each variable using a unary de Bruijn index.
For example, we represent the object term $(\fun{x}x)\True$ as
\begin{code}
let test1 = A (L (V VZ), B true)
\end{code}

Following \citet{WalidICFP02},
we try to implement an interpreter function |eval0|. It takes
an object term such as |test1| above and gives us its value.
The first argument to |eval0| is the environment, initially empty,
which is the list of values bound to free variables in the
interpreted code.
\begin{code}
let rec lookup (x::env) = function
| VZ   -> x
| VS v -> lookup env v

let rec eval0 env = function
| V v       -> lookup env v
| B b       -> b 
| L e       -> fun x -> eval0 (x::env) e
| A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{code}

If our OCaml-like metalanguage were untyped, the code above would be acceptable.
The |L e| line exhibits interpretive overhead:
|eval0| traverses the function body~|e| every time (the result of
evaluating) |L e| is applied. Staging can be used to remove this
interpretive overhead \citep[\S1.1--2]{WalidICFP02}.

However, the function |eval0| is ill-typed
if we use OCaml or some other typed language as the metalanguage.
The line |B b|
says that |eval0| returns a boolean, whereas the next line |L e| says
the result is a function, but all branches of a pattern-match form must
yield values of the same type. 
A related problem is the type of the environment |env|: a regular
OCaml list cannot hold both boolean and function values. 

The usual solution is to introduce a universal type \citep[\S1.3]
{WalidICFP02} containing both booleans and functions.
\begin{code}
type u = UB of bool | UA of (u -> u)
\end{code}
We can then write a typed interpreter
\begin{code}
let rec eval env = function
| V v       -> lookup env v
| B b       -> UB b
| L e       -> UA (fun x -> eval (x::env) e)
| A (e1,e2) -> match eval env e1 with UA f ->
               f (eval env e2)
\end{code}
whose inferred type is |u list -> exp -> u|. Now we can evaluate
our sample |test1| term.
\begin{code}
let test1r = eval [] test1
val test1r : u = UB true 
\end{code}

The unfortunate tag |UB| in the result reflects that |eval| is a partial
function.  First, the pattern match |with UA f| in the line
|A (e1,e2)| is not exhaustive, so |eval| can fail if we apply a boolean,
as in the ill-typed term |A (B true, B false)|.
\begin{code}
let test2 = A (B true, B false)
let test2r = eval [] test2
Exception: Match_failure in eval
\end{code}
Second, the |lookup|
function assumes a nonempty environment, so |eval| can fail if we
evaluate an open term.
\begin{code}
let test3 = A (L (V (VS VZ)), B true)
let test3r = eval [] test3
Exception: Match_failure in lookup
\end{code}
After all, the type |exp| represents object
terms both well-typed and ill-typed, both open and closed.

If we evaluate only closed terms that have been type-checked, then
|eval| would never fail. Alas, this soundness is not obvious to the
metalanguage, whose type system we must still appease with the
nonexhaustive pattern matching in |lookup| and |eval| and the tags |UB|
and |UA| \citep[\S1.4]{WalidICFP02}.  In other words, the algebraic data
types above fail to express in the metalanguage that the object program
is well-typed.  This failure necessitates tagging and nonexhaustive
pattern\hyp matching operations that incur a performance penalty in
interpretation \citep{WalidICFP02} and impair optimality in partial evaluation
\citep{taha-tag}.  In short, the universal\hyp type solution is
unsatisfactory because it does not preserve typing.

\subsection{Solutions using fancier types}

It is commonly thought that to interpret a typed object language in
a typed metalanguage while preserving types is difficult and requires
GADTs or dependent types \citep{taha-tag}.  In fact, this problem
motivated much work on GADTs \citep{xi-guarded,peyton-jones-simple} and
on dependent types \citep{WalidICFP02,fogarty-concoqtion}.

For a metalanguage's type system to allow the well-typed object term
|test1| but disallow the ill-typed object term |test2|, fancier types
such as GADTs or dependent types seem necessary.  Yet other type systems
have been proposed to distinguish closed terms like |test1| from open
terms like |test3|
\citep{WalidPOPL03,NanevskiICFP02,NanevskiJFP05,DaviesJACM01,nanevski-contextual},
so that |lookup| never receives an empty environment.  We discuss these
proposals further in \S\ref{related}; here we just note that many
advanced type systems have been devised to ensure statically that an
object term is well-typed and closed.

\subsection{Our final proposal}\label{ourapproach}

We represent object programs using ordinary functions rather than
data constructors.  The interpreter below provides these functions.
\begin{code}
let varZ env        = fst env
let varS vp env     = vp (snd env)
let b (bv:bool) env = bv
let lam e env       = fun x -> e (x,env)
let app e1 e2 env   = (e1 env) (e2 env)
\end{code}
These functions comprise the entire interpreter.
We now represent our sample term $(\fun{x}x)\True$ as
\begin{code}
let testf1 = app (lam varZ) (b true)
\end{code}
This representation is almost the same as in \S\ref{tagproblem}, only
written with lowercase identifiers. To evaluate an object term is to
apply its representation to the empty environment.
\begin{code}
let testf1r = testf1 ()
val testf1r : bool = true
\end{code}
The result has no tags: the interpreter patently uses no tags and no
pattern matching. The term |b true| evaluates to a boolean and the term
|lam varZ| evaluates to a function, both untagged. The |app| function
applies |lam varZ| without pattern matching. What is more, evaluating an
open term such as |testf3| below gives a type error rather than
a run-time error.
\begin{code}
let testf3 = app (lam (varS varZ)) (b true)
let testf3r = testf3 ()
\end{code}
The type error correctly complains
that the initial environment should be a tuple rather than |()|.
In other words, the term is open.

In sum, by Church\hyp encoding terms using ordinary functions,
we achieve a tagless evaluator
for a typed object language in a metalanguage with a simple type
system \citep{hindley-principal,milner-theory}.  In this \emph{final}
rather than \emph{initial} approach, both kinds of run-time errors
in \S\ref{tagproblem} (applying a nonfunction and evaluating an open
term) are reported at compile time. 
Because the new interpreter uses no
universal type or
pattern matching, it never results in a run-time error (and is in fact
total).  Because this safety is obvious not just to us but also to the
metalanguage implementation, we avoid the serious performance penalty
\citep{WalidICFP02} of error checking.  \Citet{Gluck-jones-optimality}
explains deeper technical reasons that inevitably lead to these performance
penalties.

The evaluator above is wired directly into the
functions |b|, |lam|, |app|, and so on.  In the rest of this paper, 
we explain how to abstract the interpreter so as
to process the same term in many other
ways: compilation, partial evaluation, size
computation, and so forth.

\subsection{Contributions}\label{contributions}

The term ``constructor'' functions |b|, |lam|, |app|, and so on appear
free in the encoding of an object term such as |testf1| above.  Defining
these functions differently gives rise to different interpreters, that
is, different folds on object programs.  Given the same term
representation but varying the interpreter, we can
\begin{itemize*}
    \item evaluate the term to a value in the metalanguage;
    \item measure the size or depth of the term;
    \item compile the term, with staging support such as in MetaOCaml;
    \item even partially evaluate the term, online; and
    \item transform the term to continuation\hyp passing style (CPS),
        including call-by-name (CBN) CPS, so to isolate the evaluation
        order of the object language from that of the metalanguage.
\end{itemize*}
We have programmed our interpreters in OCaml (and, for staging,
\citet{metaocaml}) and standard Haskell. The complete code is available%
\footnote{The URL will appear in the final version of the paper.}
% connot give URL in the ICFP paper due to double-blind reviews
to supplement the paper. Our examples below switch between (Meta)OCaml
and Haskell even though we have implemented each example equivalently in
both metalanguages, because some of our claims are more obvious in one
metalanguage than the other.  For example, MetaOCaml provides
convenient, typed staging facilities.

Our contributions are as follows.
\begin{enumerate*}
\item We build interpreters that evaluate (\S\ref{language}),
    compile (or evaluate with staging) (\S\ref{compiler}), and partially evaluate (\S\ref{PE}) a typed higher-order object language
   in a typed metalanguage, in direct and continuation\hyp passing styles
   (\S\ref{variations}).
\item All these interpreters use no type tags, patently never get stuck,
    and need no advanced type-system features such as GADTs, dependent types,
    or intentional type analysis.
\item The partial evaluator avoids polymorphic lift and delays binding-time
    analysis.  It bakes a type-to-type map into the interpreter
    interface to eliminate the need for GADTs and thus remain portable
    across Haskell 98 and ML.
\item We use the type system of the metalanguage
    to check statically that an object program is well-typed and closed.
\item We show clean, comparable implementations in MetaOCaml and Haskell.
\item We specify a functor signature that encompasses all our interpreters, from
    evaluation and compilation (\S\ref{language}) to partial evaluation and CPS transformation (\S\ref{PE}).
\item We point a clear way to extend the object language with more features
    such as state (\S\ref{state}).
\item We describe an approach to self\hyp interpretation compatible with the
  above (\S\ref{selfinterp}).  Self\hyp interpretation turned out harder than expected.
\end{enumerate*}
Our code is surprisingly simple and obvious in hindsight, but
it has been an open problem to
interpret a typed object language in a typed metalanguage without
tagging or type\hyp system extensions.  For example, \citet{taha-tag}
state that ``expressing such an interpreter in a statically typed
programming language is a rather subtle matter. In fact, it is only
recently that some work on programming type-indexed values in ML
\citep{yang-encoding} has given a hint of how such a function can be
expressed.''  We discuss related work in~\S\ref{related}.

\section{The object language and its tagless interpreters}\label{language}

Figure~\ref{fig:object} shows our object language, a simply-typed
$\lambda$-calculus with fixpoint, integers, booleans, and comparison.
The language is close to \citets{xi-guarded}, without their polymorphic
lift but with more constants so as to express Fibonacci, factorial, and
power.

In contrast to \S\ref{intro}, we encode binding using higher-order
abstract syntax (HOAS) \citep{miller-manipulating,pfenning-higher-order}
rather than de Bruijn indices. This makes the encoding convenient and
also ensures that our object programs are closed.

\subsection{How to make encoding flexible: abstract the interpreter}
\label{encoding}

We embed our language in (Meta)OCaml and Haskell.  In Haskell,
the functions that construct object terms are methods in a type class
|Symantics| (with a parameter |repr| of kind |* -> *|). The class is so named
because its interface gives the syntax of the object language and its
instances give the semantics.
\begin{code}
class Symantics repr where
  int  :: Int  -> repr Int
  bool :: Bool -> repr Bool

  lam :: (repr a -> repr b) -> repr (a -> b)
  app :: repr (a -> b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int -> repr Int -> repr Int
  mul :: repr Int -> repr Int -> repr Int
  leq :: repr Int -> repr Int -> repr Bool
  if_ :: repr Bool -> repr a -> repr a -> repr a
\end{code}
For example, we encode the term |test1|, or $(\fun{x}x)\True$, from
\S\ref{tagproblem} above as |app (lam (\x -> x)) (bool True)|,
whose inferred type is |Symantics repr => repr Bool|.
For another example, the classical $\mathit{power}$ function is
\begin{code}
testpowfix () = 
  lam (\x -> fix (\self -> lam (\n ->
  if_ (leq n (int 0)) (int 1)
      (mul x (app self (add n (int (-1))))))))
\end{code}
and the partial application $\fun{x} \mathit{power}\;x\;7$ is
\begin{code}
testpowfix7 () = 
 lam (\x -> app (app (testpowfix ()) x) (int 7))
\end{code}
The dummy argument |()| above is to avoid the monomorphism
restriction, to keep the type of |testpowfix| and |testpowfix7|
polymorphic in |repr|. Instead of supplying this dummy
argument, we could have given the terms explicit polymorphic
signatures.  We however prefer for
Haskell to infer the object types for us. We could also
avoid the dummy argument by switching off the monomophism restriction
with a compiler flag.

The methods |add|, |mul|, and |leq| are quite similar, and so are
|int| and |bool|. Therefore, we will frequently show implementations of
only one method of each group and elide the rest, to save space. The
accompanying code has the complete implementations.

Comparing |Symantics| with Figure~\ref{fig:object}
shows how to represent every typed, closed object term in the
metalanguage. Moreover, the representation preserves types, in the following
sense.
\begin{prop}
If an object term has the object type~$t$, then its
representation in the metalanguage has the type 
|forall repr.| |Symantics repr => repr |$t$.
\end{prop}
Conversely, the type system of the metalanguage statically checks that the
represented object term is well-typed and closed.
If we err, say replace |int 7| with |bool True| in
|testpowfix7|, Haskell will complain there that the expected type |Int| does not
match the inferred |Bool|.  Similarly, the object term $\fun{x}xx$ and its
encoding |lam (\x -> app x x)| both fail occurs-checks in type checking.
Haskell's type checker also flags syntactically invalid object terms, such as if
we forget |app| somewhere above.

\begin{figure*}
\begin{floatrule}
\begin{code}
module type Symantics = sig
  type ('c, 'dv) repr

  val int : int  -> ('c, int) repr
  val bool: bool -> ('c, bool) repr

  val lam : (('c, 'da) repr -> ('c, 'db) repr) -> ('c, 'da -> 'db) repr
  val app : ('c, 'da -> 'db) repr -> ('c, 'da) repr -> ('c, 'db) repr
  val fix : (('c, 'da -> 'db) repr -> ('c, 'da -> 'db) repr) -> ('c, 'da -> 'db) repr

  val add : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val mul : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val leq : ('c, int) repr -> ('c, int) repr -> ('c, bool) repr
  val if_ : ('c, bool) repr -> (unit -> ('c, 'da) repr) -> (unit -> ('c, 'da) repr) -> ('c, 'da) repr
end
\end{code}
\end{floatrule}
\caption{A simple (Meta)OCaml embedding of our object language}
\label{fig:ocaml-simple}
\end{figure*}

To embed the same object language in (Meta)OCaml, we replace the type
class |Symantics| and its instances by a module signature |Symantics|
and its implementations.  Figure~\ref{fig:ocaml-simple} shows a simple
signature that suffices until~\S\ref{PE}.  The two differences are:
the additional type parameter |c|, an
environment classifier \citep{WalidPOPL03} (required by MetaOCaml for
code generation in~\S\ref{compiler}); and the $\eta$-expanded type for
|fix| and thunk types in |if_| since OCaml is a call-by-value
language.

The functor below encodes our running examples |test1| and $\mathit{power}$.
\begin{code}
module EX(S: Symantics) = struct
  open S
  let test1 () =
    app (lam (fun x -> x)) (bool true)
  let testpowfix () = 
    lam (fun x -> fix (fun self -> lam (fun n ->
    if_ (leq n (int 0)) (fun () -> int 1)
        (fun () -> mul x
          (app self (add n (int (-1))))))))
  let testpowfix7 = 
     lam (fun x -> app (app (testpowfix ()) x)
                       (int 7))
end
\end{code}
The dummy argument to |test1| and |testpowfix| is an artefact of
MetaOCaml, related to monomorphism: in order for us to run a
piece of generated code, it must be polymorphic in its environment
classifier (the type variable |'c| in Figure~\ref{fig:ocaml-simple}).
The value restriction dictates that
the definitions of our object terms must look syntactically like
values. Alternatively, we could have used
the rank-2 record types of OCaml to maintain the necessary polymorphism.

Thus we represent an object expression in
OCaml as a functor from |Symantics| to an appropriate semantic domain. This
is essentially the same as the constraint |Symantics repr =>| in the
Haskell embedding.

\subsection{Two tagless interpreters}
\label{S:interpreter-RL}

Having abstracted our term representation over the interpreter, we are
now ready to present a series of interpreters.  Each interpreter is an
instance of the |Symantics| class in Haskell and a module implementing
the |Symantics| signature in MetaOCaml.

The first interpreter evaluates an object term to its value in the
metalanguage.  The module below interprets each object\hyp language
operation as the corresponding metalanguage operation.
\begin{code}
module R = struct
  type ('c,'dv) repr = 'dv (* no wrappers *)

  let int  (x:int)  = x
  let bool (b:bool) = b

  let lam  f        = f
  let app  e1 e2    = e1 e2
  let fix  f        =
    let rec self n = f self n in self

  let add  e1 e2    = e1 + e2
  let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then et () else ee ()
end
\end{code}

As in~\S\ref{ourapproach},
this interpreter is patently tagless, using neither a universal type nor
any pattern matching: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. To run our
examples, we instantiate the |EX| functor from~\S\ref{encoding} with |R|.
\begin{code}
module EXR = EX(R)
\end{code}
Thus |EXR.test1 ()| evaluates to the untagged boolean value |true|.
In Haskell, we define
\begin{code}
newtype R a = R {unR::a}
instance Symantics R where ...
\end{code}
Although |R| looks like a tag, it is only
a |newtype|.  The types |a| and |R a| are represented differently
only at compile time, not at run time.  Pattern matching against~|R|
cannot ever fail and is assuredly compiled away.
In OCaml, too, it is obvious to the compiler that
pattern matching cannot fail, because there is no
pattern matching. Evaluation can only fail to yield a value
due to interpreting |fix|.
\begin{prop}
If an object term~$e$ encoded in the metalanguage has type~$t$,
then evaluating~$e$ in the interpreter~|R| either continues
indefinitely or terminates with a value of the same type~$t$.
\end{prop}
Generalizing from~|R| to all interpreters, we have the following
broader and more useful, if also more obvious, proposition.
\begin{prop}
  If an implementation of |Symantics| never gets stuck, then
  the type system of the object
  language is sound with respect to the dynamic semantics defined by
  that implementation.
\end{prop}
These propositions follow immediately from the soundness of the
metalanguage's type system.

For variety, we show another interpreter, which measures the \emph{size}
of each object term, defined as the number of term
constructors. The following is slightly abbreviated code (see the
accompanying source code for the complete definition).
\begin{code}
module L = struct
  type ('c,'dv) repr = int
  let int (x:int)  = 1
  let lam f        = f 0 + 1
  let app e1 e2    = e1 + e2 + 1
  let fix f        = f 0 + 1
  let mul e1 e2    = e1 + e2 + 1
  let if_ eb et ee = eb + et () + ee () + 1
end
\end{code}
Now the expression
\begin{code}
let module E = EX(L) in E.test1 ()
\end{code}
evaluates to |3|. This interpreter is not only tagless but also
total. It ``evaluates'' even seemingly divergent terms like
\begin{code}
app (fix (fun self -> self)) (int 1)
\end{code}

\begin{comment}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{comment}

\section{A tagless compiler (or, a staged interpreter)}\label{compiler}

Besides immediate evaluation, we can compile our object language
into OCaml code using MetaOCaml's staging facilities. MetaOCaml
represents future-stage expressions of type~$t$ at the
present stage as values of type |('c,|$t$|) code| where |'c| is the
environment classifier \citep{WalidPOPL03,calcagno-ml-like}. Code values are created
by a \emph{bracket} form |.<|$e$|>.|, which specifies that the expression~$e$ is to be
evaluated at a future stage. The \emph{escape} |.~|$e$ must occur
within a bracket and specifies that the expression~$e$ must be evaluated
at the current stage; its result, which must be a code value, is
spliced into the code being built by the enclosing bracket. The \emph{run} form |.!|$e$ evaluates
the future-stage code value~$e$ by compiling and linking it at run-time.
The bracket, escape, and run are akin to
quasi-quotation, unquotation, and |eval| of Lisp.

Inserting brackets and escapes appropriately into the
evaluator~|R| above yields the simple compiler~|C| below.
% this code does not have the 'sv parameter. It shows up later.
\begin{code}
module C = struct
  type ('c,'dv) repr = ('c,'dv) code

  let int (x:int)   = .<x>.
  let bool (b:bool) = .<b>.

  let lam f         = .<fun x -> .~(f .<x>.)>.
  let app e1 e2     = .<.~e1 .~e2>.
  let fix f = 
    .<let rec self n = .~(f .<self>.) n in self>.
  let add e1 e2     = .<.~e1 + .~e2>.
  let mul e1 e2     = .<.~e1 * .~e2>.
  let leq x y       = .<.~x <= .~y>.
  let if_ eb et ee = 
    .<if .~eb then .~(et ()) else .~(ee ())>.
end
\end{code}
This is a straightforward staging of
|module R|.
This compiler produces
unoptimized code. For example, interpreting our |test1|
\begin{code}
let module E = EX(C) in E.test1 ()
\end{code}
gives the code value |.<(fun x_6 -> x_6) true>.|
of inferred type |('c, bool) C.repr|.  Interpreting |testpowfix7|
with
\begin{code}
let module E = EX(C) in E.testpowfix7
\end{code}
gives a code value with many apparent $\beta$-redexes:
\begin{code}
.<fun x_1 -> (fun x_2 ->
  let rec self_3 = fun n_4 ->
   (fun x_5 -> if x_5 <= 0 then 1 
               else x_2 * (self_3 (x_5 + (-1))))
   n_4 in self_3) x_1 7>.
\end{code}

This compiler does not incur
any interpretive overhead: the
code produced for $\fun{x}x$ is simply |fun x_6 -> x_6| and does not
  call the interpreter, unlike the recursive calls to |eval0| and
  |eval| in the |L e| lines in \S\ref{tagproblem}.
The resulting code obviously contains no tags and no pattern matching.

We have also implemented this compiler in Haskell. Since Haskell
has no (convenient, typed) staging facility, we had to emulate
it. To be precise, we defined a data type |ByteCode| with
constructors such as |Var|, |Lam|, |App|, |Fix|, and |INT|.
Whereas our representation of object terms uses HOAS,
our bytecode uses integer-named
variables to be realistic. We then define 
\begin{code}
newtype C t = C (Int -> (ByteCode t, Int)) 
\end{code}
where |Int| is the counter for creating fresh variable
names. We define the compiler by making |C| an instance of the
class |Symantics|. The implementation\footnote{The implementation uses
GADTs because we also wanted to write a typed interpreter for 
the \texttt{ByteCode} \emph{data type}.} is quite similar (but slightly more
verbose) than the corresponding MetaOCaml code above. The
accompanying code gives the full details.

\section{A tagless partial evaluator}\label{PE}

Surprisingly, we can write a partial evaluator using the idea above,
namely to build object terms using ordinary functions rather than data
constructors.  We present this partial evaluator in a sequence of four
attempts. It uses no universal type and no tags
for object types.  We then discuss residualization and binding-time
analysis.  Our partial evaluator is a modular extension of the evaluator
in~\S\ref{S:interpreter-RL} and the compiler in~\S\ref{compiler}, in
that it uses the former to reduce static terms and the latter to build
dynamic terms.

\subsection{Avoiding polymorphic lift}
\label{S:PE-lift}

Roughly speaking, a partial evaluator interprets each object term as either
a static (present-stage) term (using the evaluator~|R|), or
a dynamic (future-stage) term, (using the compiler~|C|).  To
distinguish between static and dynamic terms, one might try
to define partial evaluation via the Haskell data type
\begin{code}
data P0 t = S0 (R t) | E0 (C t)
\end{code}
To extract a dynamic term from this type, we create the functions
\begin{code}
abstrInt :: P0 Int -> C Int
abstrInt (S0 r) = int (unR r)
abstrInt (E0 c) = c

abstrBool :: P0 Bool -> C Bool
abstrBool (S0 r) = bool (unR r)
abstrBool (E0 c) = c
\end{code}
We then start to define |P0| as an instance of the |Symantics| class.
\begin{code}
instance Symantics P0 where
  int  x = S0 (int x)
  bool x = S0 (bool x)
  add (S0 e1) (S0 e2) = S0 (add e1 e2)
  add e1 e2 = E0 (add (abstrInt e1)
                      (abstrInt e2))
\end{code}
Integer and boolean literals are immediate, present-stage
values. Addition yields a static term (via~|R|) if and only if both operands
are static, otherwise we extract the dynamics terms from the operands and
add them using the compiler~|C|.  Uniformly, we use
the overloaded functions from the |Symantics|
instances for |R| and~|C| to define those for~|P0|.

Whereas |mul| and |leq| are as easy to define as |add|, we encounter
a problem with |if_|.  If the first argument to |if_| is a dynamic term
(type |C Bool| say) and the second and third arguments are a static term
(type |R a|) and a dynamic term (type |C a|), then we need to convert
the static term to dynamic, but there is no polymorphic ``lift''
function, of type |a -> C a|, to send a value to the future stage
\citep{xi-guarded,WalidPOPL03}.\footnote{We note in passing that, if we
were to add polymorphic \texttt{lift} to the type class
\texttt{Symantics repr}, then \texttt{repr} would become an instance of
\texttt{Applicative} and thus \texttt{Functor}:\quad\texttt{fmap
f = app (lift f)}}

Our |Symantics| class includes only separate lifting methods |bool| and
|int|, not a parametrically polymorphic lifting method, for good reason:
When compiling to a first-order target language such as machine code,
booleans, integers, and functions may well be represented differently.
Thus polymorphic lift cannot be compiled without intensional type
analysis.  To avoid the need for polymorphic lift, we turn to
\citearound{'s technique}\citet[see also \citealp{sumii-hybrid}]{asai-binding-time}:
build a dynamic term
alongside every static term.

\subsection{Delaying binding-time analysis}
\label{S:PE-problem}

We switch to the Haskell data type
\begin{code}
data P1 t = P1 (Maybe (R t)) (C t)
\end{code}
so that a partially evaluated term |P1 t| always contains a dynamic
component and sometimes contains a static component.  The two
alternative constructors of a |Maybe| value, |Just| and |Nothing|,
tag each partially evaluated term with a phase: either present or
future.  This tag is not an object type tag: all pattern matching below
is exhaustive.  Because the future-stage component is always present, we
can now define the polymorphic function
\begin{code}
abstr1 :: P1 t -> C t
abstr1 (P1 _ dyn) = dyn
\end{code}
to extract it without requiring polymorphic lift into~|C|.  We then try
to define the interpreter |P1|---and get as far as the first-order
constructs of our object language, including |if_|.
\begin{code}
instance Symantics P1 where
  int  x = P1 (Just (int x)) (int x)
  bool b = P1 (Just (bool b)) (bool b)
  add (P1 (Just n1) _) (P1 (Just n2) _)
    = int (unR (add n1 n2))
  add e1 e2 = P1 Nothing
    (add (abstr1 e1) (abstr1 e2))
  -- mul and leq are analogous and elided
  if_ (P1 (Just s) _) et ef
    = if unR s then et else ef
  if_ eb et ef = P1 Nothing
    (if_ (abstr1 eb) (abstr1 et) (abstr1 ef))
\end{code}

When we come to functions, however, we stumble.  According to our
definition of~|P1|, a partially evaluated object function, such as the
identity $\fun{x}x$ embedded in Haskell as |lam (\x -> x)|\texttt{ ::
}|P1 (a -> a)|, consists of a dynamic part (type |C (a -> a)|) and
maybe a static part (type |R (a -> a)|).  The dynamic part is useful
when this function is passed to another function that is only
dynamically known, as in $\fun{k}k(\fun{x}x)$.  The static part is
useful when this function is applied to a static argument, as in
$(\fun{x}x)\True$.  Neither part, however, lets us \emph{partially}
evaluate the function, that is, compute as much as possible statically
when it is applied to a mix of static and dynamic inputs.  For example,
the partial evaluator should turn $\fun{n}(\fun{x}x)n$ into $\fun{n}n$
by substituting $n$ for~$x$ in the body of $\fun{x}x$ even though $n$ is
not statically known.  Even the same static function, applied to
different static arguments, can give both static and dynamic results: we
want to simplify $(\fun{y}x\times y)0$ to~$0$ but $(\fun{y}x\times y)1$
to~$x$.

To enable these simplifications, we delay binding-time analysis (BTA)
for a static function until it is applied, that is, until |lam f|
appears as the argument of |app|.  To do so, we have to incorporate |f|
as it is into the |P1| data structure: applying the type constructor
|P1| to a function type |a -> b| should yield one of
\begin{code}
data P1 (a -> b) = S1 (P1 a -> P1 b)
                 | E1 (C (a -> b))

data P1 (a -> b) = P1 (Maybe (P1 a -> P1 b))
                      (C (a -> b))
\end{code}
That is, we need a nonparametric data type, something akin to
type-indexed functions and type-indexed types, which
\citet{oliveira-typecase} dub the \emph{typecase} design pattern.

Thus typed partial evaluation, like typed CPS transformation,
inductively defines a map from source types to target types that
performs case distinction on the source type.  The connection with CPS
is not an accident, as we shall see in~\S\ref{S:CPS}.

\subsection{Eliminating tags from typecase}
\label{S:PE-GADT}

Two common ways to provide typecase in Haskell are
GADTs and type-class functional dependencies
\citep{oliveira-typecase}.  These
methods are equivalent, and here we show the GADT way; |incope1.hs|
in the accompanying source code shows the latter.

\begin{figure*}
\begin{floatrule}
\begin{code}
module type Symantics = sig
  type ('c,'sv,'dv) repr

  val int : int  -> ('c,int,int) repr
  val bool: bool -> ('c,bool,bool) repr

  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x) -> ('c,'x,'da->'db) repr
  val app : ('c,'x,'da->'db) repr -> (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x)
  val fix : ('x -> 'x) -> (('c, ('c,'sa,'da) repr -> ('c,'sb,'db) repr, 'da->'db) repr as 'x)

  val add : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val mul : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val leq : ('c,int,int) repr -> ('c,int,int) repr -> ('c,bool,bool) repr
  val if_ : ('c,bool,bool) repr -> (unit -> 'x) -> (unit -> 'x) -> (('c,'sa,'da) repr as 'x)
end
\end{code}
\end{floatrule}
\caption{A (Meta)OCaml embedding of our object language that supports partial evaluation}
\label{fig:ocaml}
\end{figure*}

We introduce a GADT with four data constructors.
\begin{code}
data P t where
  VI :: Int  -> P Int
  VB :: Bool -> P Bool
  VF :: (P a -> P b) -> P (a -> b)
  E  :: C t -> P t
\end{code}
The constructors |VI|, |VB|, and |VF| build static terms (like |S0|
in~\S\ref{S:PE-lift}), and |E| builds dynamic terms (like |E0|).  However,
the type |P t| is no longer parametric in~|t|: the constructor |VF| takes an
operand of type |P a -> P b| rather than |a -> b|. As before, we define a
function to extract a future-stage computation from a value of type |P t|.
\begin{code}
abstr :: P t -> C t
abstr (VI i) = int i
abstr (VB b) = bool b
abstr (VF f) = lam (abstr . f . E)
abstr (E x)  = x
\end{code}
The cases of this function |abstr| are type-indexed.  In particular, the |VF f|
case uses the method |lam| of the |C| interpreter to compile~|f|.

We may now make |P| an instance of
|Symantics| and implement the partial evaluator as follows. We elide
|mul|, |leq|, |if_|, and |fix|.
\begin{code}
instance Symantics P where
  int x  = VI x
  bool b = VB b
  add (VI n1) (VI n2) = VI (n1 + n2)
  add e1 e2 = E (add (abstr e1) (abstr e2))
  lam = VF
  app (VF f) ea = f ea
  app (E f)  ea = E (app f (abstr ea))
\end{code}
The implementations of |int|, |bool|, and |add| are like
in~\S\ref{S:PE-problem}.
The interpretation of |lam f| just wraps the
HOAS function |f|. We can always compile |f| to a code value,
but we delay it to apply |f| to concrete arguments. The interpretation of
|app ef ea| checks to see if |ef| is such a delayed
HOAS function |VF f|. If it is, we apply |f| to the
concrete argument |ea|, giving us a chance to perform static
computations (see example |testpowfix7| in~\S\ref{S:PE-solution}). If |ef| is a
dynamic value |E f|, we residualize.

This solution using GADTs works but is not quite satisfactory. First, it
cannot be ported to MetaOCaml as GADTs are unavailable there.  Second,
the problem of nonexhaustive pattern\hyp matching reappears in |app|
above: the type |P t| has four constructors, of which pattern\hyp
matching in |app| uses only |VF| and~|E|. One may say that the
constructors |VI| and |VB| obviously cannot occur because they do not
construct values of type |P (a -> b)| as required by the type of |app|.
Indeed the metalanguage implementation could reason thus, but it may not; GHC for one
issues warnings.  Although this point may seem minor, it is the heart of
the tagging problem and the purpose of tag elimination. A typed tagged
interpreter contains many pattern\hyp matching forms that look partial
but never fail in reality. The goal is to make this safety
\emph{syntactically} apparent.

\subsection{The ``final'' solution}
\label{S:PE-solution}
Let us re-examine the problem in~\S\ref{S:PE-problem}. What we
would ideally like is to write
\begin{code}
data P t = P (Maybe (repr_pe t)) (C t)
\end{code}
where |repr_pe| is the type function defined
% inductively because P below depends on repr_pe
by
\begin{code}
repr_pe Int      = Int
repr_pe Bool     = Bool
repr_pe (a -> b) = P a -> P b
\end{code}
Although we can use type classes to define this type function
in Haskell, that is not portable to MetaOCaml. However,
these three typecase alternatives are already present in existing
methods of |Symantics|.
A simple and portable solution thus emerges: we bake |repr_pe| 
into the signature |Symantics|. Switching to MetaOCaml to demonstrate this
portability, we recall from Figure~\ref{fig:ocaml-simple} in~\S\ref{encoding} that the |repr| type
constructor took two arguments |'c| and~|'dv|. We add an argument
|'sv| for the result of applying |repr_pe| to~|'dv|.
Figure~\ref{fig:ocaml} shows the new |Symantics| signature.

\begin{figure}
\begin{floatrule}
\begin{code}
module P = struct
  type ('c,'sv,'dv) repr = {st: 'sv option;
                            dy: ('c,'dv) code}
  let abstr {dy = x} = x
  let pdyn x = {st = None; dy = x}

  let int  (x:int)  = {st = Some (R.int x);
                       dy = C.int x}
  let bool (x:bool) = {st = Some (R.bool x);
                       dy = C.bool x}

  let add e1 e2 = match e1, e2 with
  | {st = Some 0}, e | e, {st = Some 0} -> e
  | {st = Some m}, {st = Some n} -> int (R.add m n)
  | _ -> pdyn (C.add (abstr e1) (abstr e2))

  let if_ eb et ee = match eb with
  | {st = Some b} -> if b then et () else ee ()
  | _ -> pdyn (C.if_ (abstr eb) 
                     (fun () -> abstr (et ()))
                     (fun () -> abstr (ee ())))
  let lam f =
  {st = Some f; 
   dy = C.lam (fun x -> abstr (f (pdyn x)))}

  let app ef ea = match ef with
  | {st = Some f} -> f ea
  | _ -> pdyn (C.app (abstr ef) (abstr ea))

  let fix f = 
    let fdyn = C.fix (fun x -> abstr (f (pdyn x)))
    in let rec self = function
       | {st = Some _} as e -> app (f (lam self)) e
       | e -> pdyn (C.app fdyn (abstr e))
       in {st = Some self; dy = fdyn}
end
\end{code}
\end{floatrule}
\caption{Our partial evaluator (\texttt{mul} and \texttt{leq} are elided)}
\label{fig:pe}
\end{figure}

The interpreters |R|, |L| and~|C| above only use the old
type arguments |'c| and~|'dv|, which are treated by the new signature
in the same way.  Hence all that needs to change in these interpreters
to match the new signature is to add a phantom type
argument~|'sv| to~|repr|.
For example, the compiler |C| now begins
\begin{code}
module C = struct
  type ('c,'sv,'dv) repr = ('c,'dv) code
  (* the rest is the same *)
\end{code}
In contrast, the partial evaluator~|P| relies on the type argument |'sv|.

Figure~\ref{fig:pe} shows the partial evaluator~|P|.
Its type |repr| literally expresses the type equation for |repr_pe| above.
The function |abstr|, as in~\S\ref{S:PE-GADT},
extracts a future-stage code value from a result of
partial evaluation.  Conversely, the function |pdyn| injects a
code value into partial evaluation. As
in~\S\ref{S:PE-problem}, we build dynamic terms alongside
any static ones to avoid polymorphic lift.

To illustrate how to add optimizations, we improve |add| (and |mul|,
elided) to simplify the generated code using the monoid (and ring)
structure of~|int|: not only is addition performed statically
(using~|R|) when both operands are statically known, but it is
eliminated when one operand is statically~$0$; similarly for
multiplication by~$0$ or~$1$.  Such algebraic simplifications are easy
to abstract over the specific domain (such as monoid or ring) where they
apply.  These simplifications and abstractions help a lot in a large
language with more base types and primitive operations.

Any partial evaluator must decide how much to unfold recursion
statically: unfolding too little can degrade the residual code, whereas
unfolding too much risks nontermination.  Our partial evaluator is no
exception, because our object language includes |fix|.  The code in
Figure~\ref{fig:pe} takes the na\"\i ve approach of ``going all the
way'', that is, unfold |fix| rather than residualize whenever the
argument is static.  In the accompanying source code is a conservative
alternative |P.fix| that unrolls recursion only once, then residualizes.
Many sophisticated approaches have been developed to make this trade-off
\citep{jones-partial}, but this issue is orthogonal to our presentation.
A separate concern in our treatment of |fix| is possible code bloat in
the residual program, which calls for let-insertion
\citep{SwadiTahaKiselyovPasalic2006}.

Given this implementation of~|P|, our running example
\begin{code}
let module E = EX(P) in E.test1 ()
\end{code}
evaluates to
\begin{code}
{P.st = Some true; P.dy = .<true>.}
\end{code}
of type |('a, bool, bool) P.repr|.  Unlike with~|C| in~\S\ref{compiler},
a $\beta$-reduction has been statically performed to yield |true|.  More
interestingly, whereas |testpowfix7| compiles to a code value with many
$\beta$-redexes in~\S\ref{compiler}, the partial evaluation
\begin{code}
let module E = EX(P) in E.testpowfix7
\end{code}
gives the desired result
\begin{code}
{P.st = Some <fun>;
 P.dy = .<fun x_1 -> x_1 * (x_1 * (x_1 * (x_1 *
                    (x_1 * (x_1 * x_1)))))>.}
\end{code}

Unlike the GADT approach in~\S\ref{S:PE-GADT}, all pattern\hyp matching
in~|P| is \emph{syntactically} exhaustive, so it is patent to the metalanguage
implementation that |P| never gets stuck.  Further, all pattern\hyp matching occurs
during partial evaluation, only to check if a value is known statically,
never what type it has.  In other words, our partial evaluator tags
phases (with |Some| and |None|) but not object types.

Our typed partial evaluator is online and polyvariant.  It reuses the
compiler~|C| and the evaluator~|R| by composing them.  This situation is
simpler than \citets{SperberThiemann:TwoForOne} composition of a partial
evaluator and a compiler, but the general ideas are similar.

\section{Variations}\label{variations}

We show that our |Symantics| and generally our approach accomodates
a number of variants without any (new) difficulties.  In particular,
we show how a call-by-name CPS interpreter, a call-by-value CPS
program transformation.  Finally, we comment on the requirements on the type
system to support our approach.

\subsection{Call-by-name CPS interpreters}\label{S:CPS}

The object language generally inherits the evaluation strategy from
the metalanguage -- call-by-value (CBV) in OCaml, call-by-name (CBN) in Haskell.
One may wonder if it is
possible to represent a call-by-name object language in a
call-by-value metalanguage. The answer, given already by 
\citet{reynolds-definitional,reynolds-relation} and \citet{PlotkinCBN}, is yes
thanks to CPS, which they specifically introduced 
to make the evaluation strategy of a definitional interpreter
independent of the strategy of the metalanguage. We demonstrate the
same independence in the typed setting. Specifically, we
demonstrate CBN evaluation for our object language in OCaml.
We do so by building a CBN CPS interpreter for the object
language.

The interpretation of an object term is a function
mapping a continuation~|k| to the answer
returned by~|k|.
\begin{code}
let int (x:int) = fun k -> k x
let add e1 e2 = fun k ->
  e1 (fun v1 -> e2 (fun v2 -> k (v1 + v2)))
\end{code}
In both cases, the returned value has type 
|(int -> 'w)->'w| where |'w| is the (polymorphic) answer type.

The interpretation of abstraction and application in CBN CPS is
different from CBV CPS and is as follows:
\begin{code}
let lam f = fun k -> k f
let app e1 e2 = fun k -> e1 (fun f -> (f e2) k)
\end{code}
Charateristic of CBN, interpreting the application does not
``evaluate'' the argument |e2|, that is, we do apply it to the
continuation. Rather, we pass the unevaluated |e2| to the abstraction.
The result of |lam (fun x -> add x (int 1))| has the type
|((((int->'w1)->'w1) -> ((int->'w1)->'w1)) ->'w2)->'w2|.
We would like to collect those interpretation functions into a module
with signature |Symantics|, to include the CBN CPS interpreter within our
general framework. Alas, we encounter the same problem we had in
\S\ref{S:PE-problem}.
We cannot say that the result of interpreting an object
expression of type |t| is |(t->'w)->'w| because if |t| happens to
be an arrow type, the result
type obviously does not fit the desired pattern. We need a type
function with a typecase distinction.  Our previous solution 
(\S\ref{S:PE-solution}) where we use an extra type
argument |'sv| of the |repr| type, to encode such type-level functions,
also works here.  Furthermore, we notice that the type 
function |repr_pe| needed for the partial evaluator 
(in~\S\ref{S:PE-solution}) is precisely the same type function we
need for CBN CPS. 

\begin{code}
module RCN = struct
  type ('c,'sv,'dv) repr = 
    {ko: 'w. ('sv -> 'w) -> 'w}
  let int (x:int) = {ko = fun k -> k x}
  let add e1 e2 = 
    {ko = fun k -> e1.ko (fun v1 -> 
                   e2.ko (fun v2 -> k (v1+v2)))}
  let if_ eb et ee = 
    {ko = fun k -> eb.ko 
         (fun vb -> if vb then (et ()).ko k 
                          else (ee ()).ko k)}
  let lam f = {ko = fun k -> k f}
  let app e1 e2 = 
    {ko = fun k -> e1.ko (fun f -> (f e2).ko k)}
  let fix f = 
    let rec fx f n = app (f (lam (fx f))) n in 
    lam (fx f)
  let get_res x = x.ko (fun v -> v)
end
\end{code}

We have made our interpreter fully polymorphic over the answer type.
We could have also made |RCN| a functor, parameterized over
the answer type -- as is common when writing CPS in SML. Here
we chose to use the higher-rank polymorphism offered by
OCaml record types.

Because |RCN| has the signature |Symantics|, we can instantiate our previous
examples with it, and all works as expected.  More interesting
is the example $(\fun{x}1)\bigl((\fix{f}f)\mathinner2\bigr)$, which terminates
under CBN but not CBV\@.
\begin{code}
module EXS(S: Symantics) = struct open S
 let diverg () = app (lam (fun x -> int 1)) 
                     (app (fix (fun f -> f)) (int 2))
end
\end{code}
If we interpret |EXS| with the |R| interpreter of
\S\ref{S:interpreter-RL}
\begin{code}
let module M = EXS(R) in M.diverg ()
\end{code}
we never get any result because the above diverges. In contrast, if we use
the CBN interpreter
\begin{code}
let module M = EXS(RCN) in RCN.get_res (M.diverg ())
\end{code}
we get the result |1|.

\subsection{CBV CPS transformers}

We can easily change our CBN CPS interpreter into a CBV CPS
interpreter, by replacing only one definition:
\begin{code}
module RCV = struct
  include RCN
  let lam f = {ko = fun k -> k (fun e ->
    e.ko (fun v -> f {ko = fun k -> k v}))}
end
\end{code}
Now when we apply an abstraction to an argument, we always force
the evaluation of that argument before proceeding. This approach is in
line with \citet{reynolds-relation} (albeit in a typed setting). The
interpreter RCV is useful for achieving CBV evaluation of the object language
whether the metalanguage is CBV or CBN.

In this section however we show a more general approach to CBV CPS:
a CPS transformer. We take any implementation of |Symantics| (interpreter,
compiler, partial evaluator) and make it evaluate the CPS language
instead. We can view interpreter transformation as a
transformation on the object language, from direct to
continuation-passing style.

The transformation is canonical, as the following (abbreviated) code
shows
\begin{code}
module CPST(S: Symantics) = struct
  let int i = S.lam (fun k -> S.app k (S.int i))
  let add e1 e2 = S.lam (fun k ->
    S.app e1 (S.lam (fun v1 ->
    S.app e2 (S.lam (fun v2 -> S.app k (S.add v1 v2))))))
  ...
  let lam f = S.lam (fun k -> S.app k (S.lam (fun x ->
    f (S.lam (fun k -> S.app k x)))))
  let app e1 e2 = S.lam (fun k -> 
    S.app e1 (S.lam (fun f ->
    S.app e2 (S.lam (fun v -> S.app (S.app f v) k)))))
  let fix f  = S.fix (fun self -> (f self))
end
\end{code}
The code explicitly establishes a mapping from CPS interpretations to
(direct) interpretations, as performed by 
the base interpreter |S|.

A reader may notice that the above module does not define |repr|
and thus does not have signature |Symantics|.
The reason for that is again the result type of |lam f|. Whereas
|int i| or |add e1 e2| have the (abbreviated) 
type |('c,..., (int -> 'w) -> 'w) S.repr|,
the result of |lam (fun x -> add x (int 1))| has type\\
|('c,...((int -> (int -> 'w1) -> 'w1) -> 'w2) -> 'w2) S.repr|. 
Hence, to write the type equation for |CPST.repr| we again need a 
type-discriminating
function similar to the type function |repr_pe| of
\S\ref{S:PE-solution}. Alas, the function we need here is
similar but not identical to |repr_pe|. Thus we need to add 
another type variable to the |repr| type and modify the Symantics
signature accodringly. Again, the terms in previous Symantics module
stay unchanged, but the |repr| type equations in those structures have to
account for a new (phantom) type variable.

To save space, we show a simplified and less principled approach that
lets us use the module |CPST| as is. Because the module does not
have the signature |Symantics|, we cannot apply the |EX| functor to it.
But we can nevertheless write the tests:
\begin{code}
module T = struct
 module M = CPST(C)
 open M
 let test1 () = app (lam (fun x -> x)) (bool true)
 let testpowfix () = ... (* the same as before *)
 let testpowfix7 = (* the same as before *)
    lam (fun x -> app (app (testpowfix ()) x) (int 7))
end
\end{code}
That is, we instantiate |CPST| with the desired base intrepreter (the
compiler |C| in the case above) so that we can use the result |M| to
interpret object level terms. Those terms are \emph{exactly} as before.
Having to textually copy the terms is the
price we pay for this simplified treatment. We shall also encounter this
copying when discussing self-interpretation.  We
will see then that the copying step is not frivolous, but represents
a profound instance of 
``plugging the hole'', which is one of the many faces of polymorphism.

It is instructive to evaluate a few examples. With 
|CPST| instantiated with the compiler~|C| above,
|T.test1| gives
\begin{code}
.<fun x_5 -> ((fun x_2 -> 
        (x_2 (fun x_3 -> fun x_4 -> (x_4 x_3))))
        (fun x_6 -> ((fun x_1 -> (x_1 (true))) 
        (fun x_7 -> ((x_6 x_7) x_5)))))>.
\end{code}

\noindent which shows the na\"{\i}ve CPS transformation of the original code.
The result is sub-optimal with several apparent $\beta$-redexes. 
By instantiating |CPST| with |P| instead, we get the expected
\begin{code}
  P.repr = {P.st = Some <fun>;
            P.dy = .<fun x_5 -> (x_5 (true))>.}
\end{code}

\subsection{State and imperative features}
\label{state}

We can modify the CBV CPS transformation to pass a piece of state along
with the continuation. This technique lets us support mutable state. We
can also add mutable references to the object language using mutable
references of the metalanguage.  The accompanying code illustrates these
extensions.

\section{Self-interpretation}\label{selfinterp}

We turn to interpreting the object language in the object language, to
clarify how expressive our typed object language can be and to argue that our
partial evaluator is Jones\hyp optimal.

Given an \emph{encoding} of each object term~$e$ as an \emph{object} term~$\Encode{e}$,
a \emph{self\hyp interpreter} is usually defined as an object
function~$\si$ such that any object term~$e$ is observationally
equivalent to the object application $\si\Encode{e}$
\citep{jones-partial,taha-tag,Danvy-tagging-encoding}.
A partial evaluator~$\pe$ maps object terms~$e$ to observationally
equivalent object terms~$\pe(e)$.  It is said to be
\emph{optimal} with respect to~|si| if $\pe(\si\Encode{e})$
is equal to~$e$ (up to $\alpha$\hyp conversion, or in some accounts, no
less efficient than~$e$).

Intuitively, self\hyp interpretation is straightforward in our
framework: the functions comprising the interpreters
in~\S\ref{S:interpreter-RL} may just as well be written in our object
language.  In particular, the following \emph{object} functions implement an
evaluator.  (We use the number~$0$ in lieu of a unit value.)
\begin{align*}
    \ident{int} &= \fun{x} x &
    \ident{lam} &= \fun{f} f \\
    \ident{add} &= \fun{x} \fun{y} x+y &
    \ident{app} &= \fun{f} \fun{x} fx \\
    \ident{if\_}&= \fun{b} \fun{t} \fun{e} \cond{b}{t\,0}{e\,0} &
    \ident{fix} &= \fun{g} \fix{f} \fun{x} gfx
\end{align*}
We thus map each object terms~$e$ to an object term~$\encode{e}$ as follows.
We call this mapping \emph{pre-encoding}.
\begin{align*}
    \encode{x} &= x &
    \encode{\fun{x}e} &= \ident{lam} (\fun{x} \encode{e}) \\
    \encode{n} &= \ident{int}\, n &
    \encode{fx} &= \ident{app} \encode{f} \encode{x} \\
    \encode{e_1 + e_2} &= \ident{add} \encode{e_1} \encode{e_2} &
    \encode{\fix{f}e} &= \ident{fix} (\fun{f} \encode{e}) \\
    \encode{\cond{b}{t}{e}} &= \rlap{$\ident{if\_} \encode{b}
        \left(\fun{\_}\encode{t}\right) \left(\fun{\_}\encode{e}\right)$}
\end{align*}
The metavariables $x$ and~$n$ stand for a variable and an integer,
respectively.
This pre-encoding is just like how we represent object terms in the
metalanguage in the preceding sections, but it produces
terms in the object language rather than the metalanguage.

To evaluate~$\encode{e}$, then, we
instantiate the free variables in~$\encode{e}$ such as $\ident{int}$,
$\ident{lam}$, and $\ident{add}$ by their definitions above.  For
example, the familiar object term $(\fun{x}x)\True$ pre-encodes to
\begin{equation*}
    \encode{(\fun{x}x)\True} = \ident{app}
    (\ident{lam} (\fun{x} x))\, (\ident{bool} \True),
\end{equation*}
and to evaluate this pre-encoded term is to evaluate the object term
\begin{equation*}
    (\fun{f} \fun{x} fx) \,
    ((\fun{f} f) (\fun{x} x))\, ((\fun{b} b) \True).
\end{equation*}
Because the evaluator above mostly consists of glorified identity
functions, we expect our simple partial evaluator to reduce the
result of this instantiation to~$e$.  In general, to
interpret~$\encode{e}$ using an interpreter is to instantiate its free
variables by that interpreter's definitions.

\subsection{Avoiding higher polymorphism}

Any approach to self\hyp interpretation needs to spell out first how to
encode object terms~$e$ to object terms~$\Encode{e}$, and then how to
interpret~$\Encode{e}$ in the object language.  For our approach, we
want to define encoding in terms of pre-encoding, and interpretation
using some notion of instantiation.  Unfortunately, the simple type
structure of our object language hinders both tasks.  To continue with
the example term above, we could try to define
\begin{equation*}
    \Encode{e} =
    \fun{\ident{app}} \fun{\ident{lam}} \fun{\ident{bool}} \encode{e},
\end{equation*}
in particular
\begin{equation*}
    \Encode{(\fun{x}x)\True} =
    \fun{\ident{app}} \fun{\ident{lam}} \fun{\ident{bool}}
    \ident{app} (\ident{lam} (\fun{x} x))\, (\ident{bool} \True).
\end{equation*}
To type-check this encoded term, we give the variable $\ident{lam}$ the
simple type $(\BB\to\BB)\to\BB\to\BB$.
We then define the self\hyp interpreter
\begin{equation*}
    \si = \fun{e} e
    (\fun{f} \fun{x} fx)
    (\fun{f} f)
    (\fun{b} b)
\end{equation*}
and apply it to the encoded term.  The result is the object term
\begin{multline*}
    \bigl(\fun{e} e (\fun{f} \fun{x} fx) (\fun{f} f) (\fun{b} b)\bigr)
\\
    \bigl(
    \fun{\ident{app}} \fun{\ident{lam}} \fun{\ident{bool}}
    \ident{app} (\ident{lam} (\fun{x} x))\, (\ident{bool} \True)
    \bigr),
\end{multline*}
which partially evaluates to~$\True$ easily.
However, encoding fails on an term with multiple $\lambda$\hyp
abstractions at different types.  For example, the pre-encoding
\begin{equation*}
    \encode{\fun{f}\fun{x}fx}
    = \ident{lam} (\fun{f} \ident{lam} (\fun{x} \ident{app} f x))
\end{equation*}
does not type-check in any typing environment, because $\ident{lam}$ needs
to take two incompatible types.  In sum, we need more polymorphism in the
object type system to type $\ident{lam}$, $\ident{app}$, $\ident{fix}$,
and~$\ident{if\_}$ (and $\Encode{e}$ and~$\si$).
(The polytypes in |Symantics| given by Haskell's type classes and OCaml's
modules supply this polymorphism.)  Moreover, we need to encode any
polymorphism of the object language \emph{into} the object language to achieve
self\hyp interpretation.

\subsection{Introducing let-bound polymorphism}

Instead of adding higher-rank and higher-kind polymorphism to our object
language (along with polymorphism over kinds!), we add let-bound polymorphism.
As usual,
we can add a new typing rule
\begin{equation*}
    \begin{prooftree}
        e_1:t_1 \quad \subst{e_2}{x}{e_1}:t_2
        \justifies \be{x=e_1} e_2 : t_2
    \end{prooftree}
    .
\end{equation*}
The pre-encoding of a let\hyp expression is trivial.
\begin{align*}
    \encode{\be{x=e_1}e_2} \quad &= \quad \be{x=\encode{e_1}} \encode{e_2}
\intertext{A \emph{context} is an object term with a hole~$[~]$.  The
hole may occur under a binder, so plugging a term into the context may
capture free variables of the term.  By pre-encoding a hole to a hole,
we extend pre-encoding from a translation on terms to one on
contexts.}
    \encode{[~]} \quad &= \quad [~]
\end{align*}
We define an interpreter in the object language to be not a term but
a context.  For example, the evaluator is the context
\begin{align*}
    \si[~] &=
    \begin{tabular}[t]{@{}Ml@{}>{{}}Ml@{}Ml@{}}
        \be{\ident{int} &= \fun{x} x&} \\
        \be{\ident{add} &= \fun{x} \fun{y} x+y&} \\
        \be{\ident{if\_}&= \fun{b} \fun{t} \fun{e} \cond{b}{t\,0}{e\,0}&} \\
        \be{\ident{lam} &= \fun{f} f&} \\
        \be{\ident{app} &= \fun{f} \fun{x} fx&} \\
        \be{\ident{fix} &= \fun{g} \fix{f} \fun{x} gfx&} [~],
    \end{tabular}
\intertext{and the size\hyp measurer is the context}
    \mathrm{SZ}[~] &=
    \begin{tabular}[t]{@{}Ml@{}>{{}}Ml@{}Ml@{}}
        \be{\ident{int} &= \fun{x} 1&} \\
        \be{\ident{add} &= \fun{x} \fun{y} x+y+1&} \\
        \be{\ident{if\_}&= b + t\,0 + e\,0&} \\
        \be{\ident{lam} &= \fun{f} f\,0 + 1&} \\
        \be{\ident{app} &= \fun{f} \fun{x} f + x + 1&} \\
        \be{\ident{fix} &= \fun{g} g\,0 + 1&} [~].
    \end{tabular}
\end{align*}
To interpret an object term~$e$ using an interpreter $I[~]$ is to
evaluate the object term~$I[\encode{e}]$.  $\si$ is
a self\hyp interpreter in the following sense.
\begin{prop}
    $\si[\encode{e}]$ is observationally equivalent to~$e$.
\end{prop}
As a corollary, we can pre-encode the self\hyp interpreter itself as
a context: the term $\si[\encode{\si[\encode{e}]}]$ is observationally
equivalent to $\si[\encode{e}]$, and in turn to~$e$.  In other words,
$\si$ can interpret itself.  Our partial evaluator is optimal with respect to
the self\hyp interpreter~$\si$.
\begin{prop}
    Let $\pe$ be the partial evaluator~|P| in~\S\ref{S:PE-solution}.
    Then the object terms $\pe(\si[\encode{e}])$ and~$\pe(e)$ are
    either both undefined or both defined and
    equal up to $\alpha$\hyp conversion.
\end{prop}

\subsection{Contexts clarify polymorphism}

We type-check a pre-encoded term~$\encode{e}$ and its interpreter~$I[~]$
only together, never separately.  This treatment has the drawback that
we must duplicate a pre-encoded term in order to interpret it in
multiple ways.  The meta-notions of contexts and plugging may seem ad
hoc, but in fact they are just reflections of the type-class and module
machinery that we have been using in this paper all along.

In the presence of let-bound polymorphism, we can understand a term
waiting to be plugged into a context as a higher-rank and higher-kind
abstraction over the context.  Whereas our object language does not support
higher abstraction, our metalanguages do, so they can type-check an object term
separately from its interpreter---either as a functor from a |Symantics| module
containing a type constructor
(in OCaml), or a value with a |Symantics| constraint over a type
constructor (in Haskell).  Thus
``context'' is a euphemism for a polymorphic argument, and ``plugging''
is a euphemism for application.

\begin{comment}
\jacques{But how 
to you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?}
\oleg{Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.}
Again, what is different from the above is that we can
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).

\oleg{Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).}
\end{comment}

\begin{comment}

The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
\texttt{app \_add (app \_int 1) (app \_int 2)} where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is
$$ 
  (\ZZ \rightarrow r \ZZ) \rightarrow
  (\BB \rightarrow r \BB) \rightarrow
  \forall \alpha \beta. (r \alpha \rightarrow r \beta)
      \rightarrow r (\alpha\rightarrow\beta) \rightarrow ... r \tau
$$
% (Int -> r Int) ->
% (Bool -> r Bool) ->
% (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau

or, if we denote the sequence of initial arguments as |S r|, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.

\end{comment}

\section{Related work}\label{related}

Our initial motivation came from several papers 
\citep{WalidICFP02,taha-tag,xi-guarded,peyton-jones-simple} which used
embedded interpreters as a central example to justify looking 
into advanced type systems.  While we admire all this technical work,
there remained the issue that we were not convinced that their 
motivating example actually needed all this machinery.

\citet{WalidICFP02}, when referring to the work of \citet{taha-tag},
claims that this shows that self-interpretation is possible in a
\emph{simply typed $\lambda$-calculus}, but this is somewhat misleading
as their type system does not differentiate between the various ground
types.  In other words, various constants (like \textsf{nil} and 
\textsf{true}) both inhabit the same type $D$.  This is not very satisfying.
Furthermore, as is pointed out in the introduction to \citet{WalidICFP02},
\citet{taha-tag} does not \emph{statically} guarantee that all tags will
be removed - tag elimination must be performed at runtime.

\Citet{WalidICFP02}, \citet{taha-tag}, \citet{xi-guarded}, and
\citet{peyton-jones-simple} seem to argue as follows that a self\hyp
interpreter of a typed language cannot be Jones\hyp optimal.
\begin{enumerate}
\item One needs to encode a typed language in a typed language based on
a sum type (at some level of the hierarchy),
\item It is not possible to write a \emph{direct} interpreter 
for such an encoding of a typed language
in a typed language, without either using a
very advanced type system or incurring tagging/untagging overhead.
\item Thus an indirect interpreter is necessary, which needs a universal
  type (and hence tagging),
\item Thus any self-interpreter must have tags.
\item Hence the self-interpreter cannot be Jones-optimal.
\end{enumerate}
While the logic is sound, we showed that the premise in the very first step
was not valid.

\citet{Danvy-tagging-encoding} discuss Jones optimality at length.
There, the authors demonstrate the usefulness of HOAS for typed
self-interpretation.  However, no claim is made that the source program
is itself typed -- just that the interpreter is.  Our reading is that
the source language is not typed.  However, in sharp contrast with 
their interpreter, none of our evaluators can product pattern-match errors
(unlike the code that accompanies \citet{Danvy-tagging-encoding}).
In other words, their language (and interpreter) does use tags, however
the shift to HOAS allows the partial evaluator to remove all the tags.
In contrast, we have no tags at all.

We used a method from \citet{asai-binding-time} to reduce the amount
of back-and-forth between reify/reflect: in our PE we track both
static and dynamic values in parallel (whenever possible).  In some sense,
we are observing that this method type-checks in Hindley-Milner once we
deforest the static code representation.  Put another way, we have
manually applied type-level partial-evaluation to our typelevel 
functions (see \S\ref{S:PE-solution}) to obtain simpler types 
acceptable to MetaOCaml.  It is worthwhile to note that 
\citet{sumii-hybrid} also takes advantage of this method
\citep[of][]{asai-binding-time} to get interesting results combining online
and offline partial evaluation.  Like \citet{SperberThiemann:TwoForOne},
we strive for modularity by reusing earlier stages.  It would be interesting
to see if we could derive a \emph{cogen} \citep{Thiemann:cogeninsixlines}
in the same manner.

The approach to HOAS via catamorphism and anamorphism of
\citet{Washburn-Weirich-boxes} seems related to what we are doing.
We have not investigated the relationship to any serious depth, but we
feel that one could fruitfully combine their ideas with ours.

A lot of effort has gone into finding ways to give precise types to untyped
terms (see for example \citet{baars-typing,Guillemette-Monier-PLPV}
as well as the messages \citet{haskell-list}).  These 
take advantage of the host language's types to varying extents.  While
very successful for proper terms, results for
ill-typed terms can produce rather obscure error messages.  We use the
host's typing facilities for this, and thus inherit the strengths
and weaknesses of the host language.  While that may not be optimal, it is
certainly much more modular.

When dealing with datatypes which are equivalent but have different
representations, it is customary to use injection-projection pairs
(used in the context of interpreters by \citet{Ramsey-ML-module-mania}
and \citet{Benton-embedded-interpreters}, partial evaluation by
\citet{Danvy-TDPE}, and type-level functions by
\citet{oliveira-typecase}).  Na\"{\i}vely, this would appear to apply to
our implementation of the type-level function |'sv -> 'dv|.  However, in most
of these cases, it is important that this establishes a bijection (between
|'sv| and |'dv| in our case).  Our approach is one-sided: we requite
an operational definition of |'sv -> 'dv| (which is easy), and an
implementation of |'dv| which ``perfectly reflects'' |'sv| but without 
requiring an implementation.

We are not the first to implement a typed interpreter for a typed
language -- \citet{laod93} use a typed version of the SK language
(and typeclasses) to implement a meta-circular interpreter (rather than
a self-interpreter).  Interestingly, their meta-circular interpreter
appears to be tagfree!  However, they could not have implemented a
compiler or a partial-evaluator in the same way since they rely quite
heavily on having injection-projection pairs.

\citet{fiore:nbe-ppdp2002,balat:tdpe-popl2004} also deal with a 
tagfree partial-evaluator using quite a lot of interesting 
technology (amongst them powerful continuation-based control operators).
While they use a reify-reflect injection-projection pair, this appears
to be done ``lightly'', in the sense that the structure of compiled code
is never examined.  The exact relation between their work and ours is 
still a little mysterious to us, but we will continue to investigate it.

We have been unable to find work that establishes that
the \emph{typed} $\lambda$-calculus possesses a final coalgebra structure.
For the untyped $\lambda$-calculus, 
\citet{HonsellLenisa,honsell99coinductive} establish several interesting
results along this line.  Of particular interest is their use of
contexts with a hole \citep[p.13]{honsell99coinductive} for the definition
of \emph{observational equivalence}
(see \S\ref{selfinterp}).  The reader is directed to bibliography of
\citet{honsell99coinductive} for references to the foundational work
in this important area.  Particularly intriguing is the link to the
coinductive aspects of B\"{o}hm trees, as pointed out by
\citet{berarducci-models} and \citet[Example 4.3.4]{jacobs-coalgebra}.

One way to interpret some of our results is that we eschew sums (at either the
term or type, or even kind level) for our terms, but instead use a record type 
(via Functors in OCaml and typeclasses in Haskell).
For the self-interpreter, we then proceed to use a Church encoding for
recursive data types \citep{bohm-automatic}.

\section{Conclusions}\label{conclusion}

We solve the problem of embedding a typed object language in a typed
metalanguage without using GADTs, dependent types, or a universal type.
Our family of interpreters include an evaluator, a compiler, a partial
evaluator, and CPS transformers.  It is patent that they never get stuck,
because we represent object types as metalanguage types.  This work
makes it safer and more efficient to embed domain\hyp specific languages
in practical metalanguages such as Haskell and ML\@.

Our main idea is to represent object programs not in an initial algebra
but using the existing coalgebraic structure of the $\lambda$-calculus.
More generally, to squeeze more invariants out of a type system as
simple as Hindley-Milner, we shift the burden of representation and
computation from consumers to producers: encoding object terms as calls
to metalanguage functions (\S\ref{ourapproach}); build dynamic terms
alongside static ones (\S\ref{S:PE-lift}); simulating type functions for
partial evaluation (\S\ref{S:PE-solution}) and CPS
transformation~(\S\ref{S:CPS}).  This shift also underlies fusion,
functionalization, and amortized complexity analysis.

When the metalanguage does provide higher-rank and higher-kind
polymorphism, we can type-check and compile an object term separately
from any interpreters it may be plugged into.

\acks%\subsection*{Acknowledgments}
We thank Martin Sulzmann and Walid Taha 
for helpful discussions.  Sam Staton, Pieter Hofstra, and Bart Jacobs
kindly provided some useful references on coalgebraic structures in the
$\lambda$\hyp calculus

\bibliographystyle{mcbride}
\bibsep=0pt
\bibliography{tagless}
\end{document}
