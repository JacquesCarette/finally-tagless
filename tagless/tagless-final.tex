\hyphenation{meta-language meta-languages Meta-OCaml meta-cir-cu-lar type-case}
\documentclass[preprint]{sigplanconf}
\usepackage{amsmath,amssymb}
\usepackage{mathptmx}
%\usepackage{txfonts} \renewcommand{\ttdefault}{cmtt}
\usepackage[T1]{fontenc}
\usepackage[override]{cmtt}
\usepackage{comment}
\usepackage{hyphenat}
\usepackage{url}
\usepackage{prooftree1}
\usepackage{mdwlist}
\usepackage{dblfloatfix}

\newcommand{\jacques}[1]{{\it [Jacques says: #1]}}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}
\newcommand{\ccshan}[1]{{\it [Ken says: #1]}}

\usepackage{natbib1}
\let\cite=\citep

\usepackage[compact]{fancyvrb1}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=\parindent}

\newtheorem{prop}{Proposition}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\BB}{\mathbb{B}}

\newcommand{\fun}[1]{\mathopen{\lambda\mathord{#1}.\,}}
\newcommand{\fix}[1]{\mathopen{\mathrm{fix\,}\mathord{#1}.\,}}
\newcommand{\Forall}[1]{\mathopen{\forall\mathord{#1}.\,}}
\newcommand{\Exists}[1]{\mathopen{\exists\mathord{#1}.\,}}
\newcommand{\cond}[3]{\mathrm{if\ }#1\mathrm{\ then\ }#2\mathrm{\ else\ }#3}
\newcommand{\True}{\mathinner{\mathrm{true}}}
\newcommand{\False}{\mathinner{\mathrm{false}}}

% Make space around section headings no less than space around theorems
\makeatletter
\@tempdima\topsep \advance\@tempdima\parsep
\ifdim\@sectionbelowskip   <\topsep \@sectionbelowskip   \topsep \fi
\ifdim\@subsectionbelowskip<\topsep \@subsectionbelowskip\topsep \fi
\makeatother

% Match the rule inserted by \@makecaption in sigplanconf.cls
\setproofrulebreadth .33pt
\newenvironment{floatrule}
    {\hrule width \hsize height .33pt \vspace{.5pc}}
    {\par\addvspace{1ex}}

% Make some Rel symbols into Bin symbols instead
\DeclareMathSymbol{\to}{\mathbin}{symbols}{"21}
\DeclareMathSymbol{:}{\mathbin}{operators}{"3A}

\begin{document}

\conferenceinfo{ICFP '07}{Freiburg, Germany} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Staged typed type-preserving tagless ``interpreters'', \emph{finally}}
\subtitle{Subtitle Text, if any}

\begin{comment}
\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}
\end{comment}
\authorinfo{}{}{}

\maketitle

\begin{abstract}
We have built a family of tagless interpretations for a
higher-order typed object
language in a typed metalanguage (Haskell and ML), 
requiring neither dependent
types, nor generalized algebraic data types, nor a tag elimination 
postprocessing step. That has been an open problem.
Our family of interpreters contains 
an evaluator, a (byte-)compiler and staged evaluator, a partial
evaluator, a term-size evaluator, and call-by-name and
call-by-value CPS transformers.

The main tool is to take functions seriously: we encode the
de Bruijn or higher-order abstract syntax  of object terms 
not by using constructors for
an algebraic data type but rather by invoking cogen functions, which occur as
free variables in the encoded term.  In other words, we eschew initial
algebras for the wilder world of final algebras.
\oleg{Or: we expose the co-algebraic structure of the object language.}
Our approach highlights the importance of order-2 type constructors
(rank-2 polymorphism), available with ML functors and
Haskell98 constructor classes. This lets us encode object terms with
translucent types: enough abstraction is available to interpret the
terms in various ways, and yet enought type information is available 
to typecheck the terms and statically assure their interpretation
never gets stuck.

The encoding of a self-interpreter clarifies our technique 
\oleg{and rank-2 polymorphism?} 
as the
interpretation \oleg{evaluation?} of a term plugged into the context
of let-polymorphic definitions.
\oleg{Discuss on Fri, along with the colclusions.}
\end{abstract}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}\label{intro}

A popular way to define and implement a programming language is to embed it in
another \citep{reynolds-definitional}.  Embedding means to represent
terms and values of the \emph{object language} as terms and values in the
\emph{metalanguage}.  Embedding is especially appropriate for domain\hyp
specific object languages because it supports rapid prototyping and integration
with the host environment \citep{hudak-building}.
If the metalanguage supports \emph{staging}, then
the embedding can compile object programs to the metalanguage and avoid the
overhead of interpreting them on the fly \citep{WalidICFP02}.  A staged
definitional interpreter is thus a promising way to build a domain\hyp specific
language.

\begin{figure}
    \begin{floatrule}
    \begin{proofrules}
        \[ \[ [x:t_1] \proofoverdots e:t_2 \] \justifies \fun{x}e:t_1\to t_2 \]
        \[ \[ [f:t_1\to t_2] \proofoverdots e:t_1\to t_2 \] \justifies \fix{f}e:t_1\to t_2 \]
        \[ e_1:t_1\to t_2 \quad e_2:t_1 \justifies e_1 e_2: t_2 \]
        \[ \text{$n$ is an integer} \justifies n:\ZZ \]
        \[ \text{$b$ is a boolean} \justifies b:\BB \]
        \[ e:\BB \quad e_1:t \quad e_2:t \justifies \cond{e}{e_1}{e_2}:t \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1+e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \times e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \le e_2:\BB \]
    \end{proofrules}
    \end{floatrule}
    \caption{Our typed object language}
    \label{fig:object}
\end{figure}

We focus on embedding a typed object language into a typed metalanguage.
The benefit of types in this setting is to rule out meaningless object terms,
thus enabling faster interpretation and assuring that our interpreters
do not get stuck.
To be concrete, we use the typed object language in
Figure~\ref{fig:object} throughout this paper.  We aim not just for
evaluation of object programs but also for
compilation, partial evaluation, and other processing.

\Citet{WalidICFP02} and \citet{xi-guarded} motivated interpreting
a typed object language in a typed metalanguage as an interesting
problem.  The known solutions to this problem store object terms and
values in the metalanguage in a universal type, a generalized algebraic
data type (GADT), or a dependent type.  In the remainder of this section,
we discuss these solutions, identify their drawbacks, then summarize our
proposal and contributions.  
No matter how we represent the object language in the
metalanguage, the representation can be created either by hand (for example, by
entering object terms at a metalanguage interpreter's prompt) or
by a parser or type checker reading from a text string.
We leave aside the solved problem of writing such a parser or type checker,
whether using dependent types \citep{WalidICFP02} or not \citep{baars-typing}.

\subsection{The tag problem}\label{tagproblem}

%\oleg{see tagless\_interp1.ml, module Tagfull for the complete code.
%  If you change the code in here, please adjust the .ml file
%  accordingly. Let the paper and the accompanying code be in sync.}

It is straightforward to create an algebraic data type, say in OCaml, to
represent object terms such as those in Figure~\ref{fig:object}.
For brevity, we elide treating integers, conditionals, and fixpoint in
this section.
\begin{code}
type var = VZ | VS of var
type exp = V of var | B of bool
         | L of exp | A of exp * exp
\end{code}
We represent each variable using a unary de Bruijn index.
For example, we represent the object term $(\fun{x}x)\True$ as
\begin{code}
let test1 = A (L (V VZ), B true)
\end{code}

Following \citet{WalidICFP02},
we try to implement an interpreter function |eval0|. It takes
an object term such as |test1| above and gives us its value.
The first argument to |eval0| is the environment, initially empty,
which is the list of values bound to free variables in the
interpreted code.
\begin{code}
let rec lookup (x::env) = function
| VZ   -> x
| VS v -> lookup env v

let rec eval0 env = function
| V v       -> lookup env v
| B b       -> b 
| L e       -> fun x -> eval0 (x::env) e
| A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{code}

If our OCaml-like metalanguage were untyped, the code above would be acceptable.
The |L e| line exhibits interpretive overhead:
|eval0| traverses the function body~|e| every time (the result of
evaluating) |L e| is applied. Staging can be used to remove this
interpretive overhead \citep[\S1.1--2]{WalidICFP02}.

However, the function |eval0| is ill-typed
if we use OCaml or some other typed language as the metalanguage.
The line |B b|
says that |eval0| returns a boolean, whereas the next line |L e| says
the result is a function, but all branches of a pattern-match form must
yield values of the same type. 
A related problem is the type of the environment |env|: a regular
OCaml list cannot hold both boolean and function values. 

The usual solution is to introduce a universal type \citep[\S1.3]
{WalidICFP02} containing both booleans and functions.
\begin{code}
type u = UB of bool | UA of (u -> u)
\end{code}
We can then write a typed interpreter
\begin{code}
let rec eval env = function
| V v       -> lookup env v
| B b       -> UB b
| L e       -> UA (fun x -> eval (x::env) e)
| A (e1,e2) -> match eval env e1 with UA f ->
               f (eval env e2)
\end{code}
whose inferred type is |u list -> exp -> u|. Now we can evaluate
our sample |test1| term.
\begin{code}
let test1r = eval [] test1
val test1r : u = UB true 
\end{code}

The unfortunate tag |UB| in the result reflects that |eval| is a partial
function.  First, the pattern match |with UA f| in the line
|A (e1,e2)| is not exhaustive, so |eval| can fail if we apply a boolean,
as in the ill-typed term |A (B true, B false)|.
\begin{code}
let test2 = A (B true, B false)
let test2r = eval [] test2
Exception: Match_failure in eval
\end{code}
Second, the |lookup|
function assumes a nonempty environment, so |eval| can fail if we
evaluate an open term.
\begin{code}
let test3 = A (L (V (VS VZ)), B true)
let test3r = eval [] test3
Exception: Match_failure in lookup
\end{code}
After all, the type |exp| represents object
terms both well-typed and ill-typed, both open and closed.

If we evaluate only closed terms that have been type-checked, then
|eval| would never fail. Alas, this soundness is not obvious to the
metalanguage, whose type system we must still appease with the
nonexhaustive pattern matching in |lookup| and |eval| and the tags |UB|
and |UA| \citep[\S1.4]{WalidICFP02}.  In other words, the algebraic data
types above fail to express in the metalanguage that the object program
is well-typed.  This failure necessitates tagging and nonexhaustive
pattern\hyp matching operations that incur a performance penalty in
interpretation \citep{WalidICFP02} and impair optimality in partial evaluation
\citep{taha-tag}.  In short, the universal\hyp type solution is
unsatisfactory because it does not preserve typing.

\subsection{Solutions using fancier types}

It is commonly thought that to interpret a typed object language in
a typed metalanguage while preserving types is difficult and requires
GADTs or dependent types \citep{taha-tag}.  In fact, this problem
motivated much work on GADTs \citep{xi-guarded,peyton-jones-simple} and
on dependent types \citep{WalidICFP02,fogarty-concoqtion}.

For a metalanguage's type system to allow the well-typed object term
|test1| but disallow the ill-typed object term |test2|, fancier types
such as GADTs or dependent types seem necessary.  Yet other type systems
have been proposed to distinguish closed terms like |test1| from open
terms like |test3|
\citep{WalidPOPL03,NanevskiICFP02,NanevskiJFP05,DaviesJACM01,nanevski-contextual},
so that |lookup| never receives an empty environment.  We discuss these
proposals further in \S\ref{related}; here we just note that many
advanced type systems have been devised to ensure statically that an
object term is well-typed and closed.

\subsection{Our final proposal}\label{ourapproach}

We represent object programs using ordinary functions rather than
data constructors.  The interpreter below provides these functions.
\begin{code}
let varZ env        = fst env
let varS vp env     = vp (snd env)
let b (bv:bool) env = bv
let lam e env       = fun x -> e (x,env)
let app e1 e2 env   = (e1 env) (e2 env)
\end{code}
These functions comprise the entire interpreter.
We now represent our sample term $(\fun{x}x)\True$ as
\begin{code}
let testf1 = app (lam varZ) (b true)
\end{code}
This representation is almost the same as in \S\ref{tagproblem}, only
written with lowercase identifiers. To evaluate an object term is to
apply its representation to the empty environment.
\begin{code}
let testf1r = testf1 ()
val testf1r : bool = true
\end{code}
The result has no tags: the interpreter patently uses no tags and no
pattern matching. The term |b true| evaluates to a boolean and the term
|lam varZ| evaluates to a function, both untagged. The |app| function
applies |lam varZ| without pattern matching. What is more, evaluating an
open term such as |testf3| below gives a type error rather than
a run-time error.
\begin{code}
let testf3 = app (lam (varS varZ)) (b true)
let testf3r = testf3 ()
\end{code}
The type error correctly complains
that the initial environment should be a tuple rather than |()|.
In other words, the term is open.

In sum, by Church\hyp encoding terms using ordinary functions,
we achieve a tagless evaluator
for a typed object language in a metalanguage with a simple type
system \citep{hindley-principal,milner-theory}.  In this \emph{final}
rather than \emph{initial} approach, both kinds of run-time errors
in \S\ref{tagproblem} (applying a nonfunction and evaluating an open
term) are reported at compile time. 
\oleg{Jacques, do we use final and initial here correctly?}
Because the new interpreter uses no
universal type or
pattern matching, it never results in a run-time error (and is in fact
total).  Because this safety is obvious not just to us but also to the
metalanguage implementation, we avoid the serious performance penalty
\citep{WalidICFP02} of error checking.  \Citet{Gluck-jones-optimality}
explains deeper technical reasons that inevitably lead to these performance
penalties.

The evaluator above is wired directly into the
functions |b|, |lam|, |app|, and so on.  In the rest of this paper, 
we explain how to abstract the interpreter so as
to process the same term in many other
ways: compilation, partial evaluation, size
computation, and so forth.

\subsection{Contributions}\label{contributions}

The term ``constructor'' functions |b|, |lam|, |app|, and so on appear
free in the encoding of an object term such as |testf1| above.  Defining
these functions differently gives rise to different interpreters, that
is, different folds on object programs.  Given the same term
representation but varying the interpreter, we can
\begin{itemize*}
    \item evaluate the term to a value in the metalanguage;
    \item measure the size or depth of the term;
    \item compile the term, with staging support such as in MetaOCaml;
    \item even partially evaluate the term, online; and
    \item transform the term to continuation\hyp passing style (CPS),
        including call-by-name (CBN) CPS, so to isolate the evaluation
        order of the object language from that of the metalanguage.
\end{itemize*}
We have programmed our interpreters in OCaml (and, for staging,
\citet{metaocaml}) and standard Haskell. The complete code is available%
\footnote{The URL will appear in the final version of the paper.}
% connot give URL in the ICFP paper due to double-blind reviews
to supplement the paper. Our examples below switch between (Meta)OCaml
and Haskell even though we have implemented each example equivalently in
both metalanguages, because some of our claims are more obvious in one
metalanguage than the other.  For example, MetaOCaml provides
convenient, typed staging facilities.

Our code ensures statically that an object term is well-typed and
closed, yet is surprisingly simple and obvious in hindsight.  We stress
that until now it has been considered impossible to
interpret a typed object language in a typed metalanguage without
tagging or type\hyp system extensions.  For example, \citet{taha-tag}
state that ``expressing such an interpreter in a statically typed
programming language is a rather subtle matter. In fact, it is only
recently that some work on programming type-indexed values in ML
\citep{yang-encoding} has given a hint of how such a function can be
expressed.''  We further discuss related work in~\S\ref{related}.

Our contributions are as follows.
\begin{enumerate}
\item We build interpreters that evaluate (in direct or continuation\hyp passing styles),
  compile, and partially evaluate a typed higher-order object language
   in a typed metalanguage.
\item All these interpreters use no type tags, patently never get stuck,
    and need no advanced type-system features such as GADTs, dependent types,
    or intentional type analysis.
\item The partial evaluator avoids polymorphic lift and delays binding-time
    analysis.
\item We use the type system of the metalanguage
    to check statically for well-typed and closed object programs.
\item We show clean, comparable implementations in MetaOCaml and Haskell.
\item We specify a functor signature that encompasses all our interpreters, from
    evaluation and compilation to partial evaluation and CPS transformation.
\item We point a clear way to extend the object language with more features
  such as state.
\item We describe an approach to self\hyp interpretation compatible with the
  above.  Self\hyp interpretation turned out harder than expected.
\end{enumerate}


The structure of our paper is as follows. In the next Section, we
define the OCaml and Haskell interfaces of the interpreter for our
language and describe two instances, evaluating a term to a
metalanguage value and computing term's size. In
Section~\ref{compiler}, we stage the evaluator, obtaining the tagless
compiler. Section~\ref{PE} describes partial evaluation, in
particular, the problem posed by the need to delay the 
binding-time analysis of term abstractions and the apparent 
need for a typecase. We then show two solutions. The first, imperfect
one, uses GADT. The better and more portable solution bakes in the
typecase idiom into the interpreter interface, eliminating the need for
advanced type systems and permitting implementations in Haskell98 or OCaml.
The discussion Section~\ref{discussion} presents, inter alia, CBN CPS
interpreter and the CBV CPS transformer. We then describe 
self-interpretation (Section~\ref{selfinterp}),
discuss related work (Section~\ref{related}) and then conclude.

\section{The object language and its tagless interpreters}\label{language}

Figure~\ref{fig:object} shows our object language, a simply-typed
$\lambda$-calculus with fixpoint, integers, booleans, and comparison.
The language is close to \citets{xi-guarded}, without their polymorphic
lift but with more constants so as to express Fibonacci, factorial, and
power.

In contrast to \S\ref{intro}, we henceforth encode 
binding in object programs using higher-order
abstract syntax (HOAS) \citep{miller-manipulating,pfenning-higher-order}
rather than de Bruijn indices. This makes the encoding convenient and
also ensures that our object programs are closed.

\subsection{How to make encoding flexible: abstract the interpreter}
\label{encoding}

We embed this language in (Meta)OCaml and Haskell.  In Haskell,
the functions that construct object terms are methods in a type class
|Symantics|. The class is so named because its interface gives the syntax for
the object language and its instances give the semantics.
The parameter |repr| to the class has kind |* -> *|.
\begin{code}
class Symantics repr where

  int  :: Int  -> repr Int
  bool :: Bool -> repr Bool

  lam :: (repr a -> repr b) -> repr (a -> b)
  app :: repr (a -> b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int -> repr Int -> repr Int
  mul :: repr Int -> repr Int -> repr Int
  leq :: repr Int -> repr Int -> repr Bool
  if_ :: repr Bool -> repr a -> repr a -> repr a
\end{code}
For example, we encode the term |test1|, or $(\fun{x}x)\True$, from
\S\ref{tagproblem} above as |app (lam (\x -> x)) (bool True)|,
whose inferred type is |Symantics repr => repr Bool|.
For another example, the classical $\mathit{power}$ function is
\begin{code}
testpowfix () = 
  lam (\x -> fix (\self -> lam (\n ->
  if_ (leq n (int 0)) (int 1)
      (mul x (app self (add n (int (-1))))))))
\end{code}
and the partial application $\fun{x} \mathit{power}\;x\;7$ is
\begin{code}
testpowfix7 () = 
  lam (\x -> app (app (testpowfix ()) x)
                 (int 7))
\end{code}
The dummy argument |()| above is to avoid the monomorphism
restriction, to keep the type of |testpowfix| and |testpowfix7|
polymorphic in |repr|. Instead of supplying this dummy
argument, we could have given the terms explicit polymorphic
signatures.  We however prefer for
Haskell to infer the object types for us. We could also
avoid the dummy argument by switching off the monomophism restriction
with a compiler flag.

The methods |add|, |mul|, and |leq| are quite similar, and so are
|int| and |bool|. Therefore, we will frequently show implementations of
only one method of each group and elide the rest, to save space. The
accompanying code has the complete implementations.

Comparing |Symantics| with Figure~\ref{fig:object}
shows how to represent every typed, closed object term in the
metalanguage. Moreover, the representation preserves types, in the following
sense.
\begin{prop}
If an object term has the object type |t|, then its
representation in the metalanguage has the type 
|forall repr.| |Symantics repr => repr t|.
\end{prop}
Conversely, the type system of the metalanguage checks statically that the
represented object term is well-typed and closed.
If we err, say replace |int 7| with |bool True| in
|testpowfix7|, Haskell will complain there that the expected type |Int| does not
match the inferred |Bool|.  Similarly, the object term $\fun{x}xx$ and its
encoding |lam (\x -> app x x)| both fail occurs-checks in type checking.
Haskell's type checker also flags syntactically invalid object terms, such as if
we forget |app| somewhere above.

\begin{figure*}
\begin{floatrule}
\begin{code}
module type Symantics = sig

  type ('c, 'dv) repr

  val int : int  -> ('c, int) repr
  val bool: bool -> ('c, bool) repr

  val lam : (('c, 'da) repr -> ('c, 'db) repr) -> ('c, 'da -> 'db) repr
  val app : ('c, 'da -> 'db) repr -> ('c, 'da) repr -> ('c, 'db) repr
  val fix : (('c, 'da -> 'db) repr -> ('c, 'da -> 'db) repr) -> ('c, 'da -> 'db) repr

  val add : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val mul : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val leq : ('c, int) repr -> ('c, int) repr -> ('c, bool) repr
  val if_ : ('c, bool) repr -> (unit -> ('c, 'da) repr) -> (unit -> ('c, 'da) repr) -> ('c, 'da) repr

end
\end{code}
\end{floatrule}
\caption{A simple (Meta)OCaml embedding of our object language}
\label{fig:ocaml-simple}
\end{figure*}

To embed the same object language in (Meta)OCaml, we replace the type
class |Symantics| and its instances by a module signature |Symantics|
and its implementations.  Figure~\ref{fig:ocaml-simple} shows a simple
signature that suffices until~\S\ref{PE}.
Two differences between this signature and the |Symantics| class in
Haskell are: the additional type parameter |c|, an
environment classifier \citep{WalidPOPL03} required by MetaOCaml for
code generation in~\S\ref{compiler}; and the $\eta$-expanded type for
|fix| and thunk types in |if_| due to OCaml's being a call-by-value
language.

The functor below encodes our running examples |test1| and $\mathit{power}$.
\begin{code}
module EX(S: Symantics) = struct
  open S
  let test1 () =
    app (lam (fun x -> x)) (bool true)
  let testpowfix () = 
    lam (fun x -> fix (fun self -> lam (fun n ->
    if_ (leq n (int 0)) (fun () -> int 1)
        (fun () -> mul x
          (app self (add n (int (-1))))))))
  let testpowfix7 = 
     lam (fun x -> app (app (testpowfix ()) x)
                       (int 7))
end
\end{code}
The dummy argument to |test1| and |testpowfix| is an artefact of
MetaOCaml, again related to monomorphism: in order for us to run a
piece of generated code, it must be polymorphic in its environment
classifier (the type variable |'c| in Figure~\ref{fig:ocaml-simple}).
The value restriction then dictates that
the definitions of our object terms must look syntactically like
values. Alternatively to giving the |()| argument, we could have used
the rank-2 record types of OCaml to maintain the necessary polymorphism.

Thus we represent an object expression in
OCaml as a functor from |Symantics| to an appropriate semantic domain. This
is essentially the same as the constraint |Symantics repr =>| in the
Haskell embedding.

\subsection{Two tagless interpreters}
\label{S:interpreter-RL}

Having abstracted our term representation over the interpreter, we are
now ready to present a series of interpreters.  Each interpreter is an
instance of the |Symantics| class in Haskell and a module implementing
the |Symantics| signature in MetaOCaml.

The first interpreter evaluates an object term to its value in the
metalanguage.  The module below interprets each object\hyp language
operation as the corresponding metalanguage operation.
\begin{code}
module R = struct
  (* absolutely no wrappers *)
  type ('c,'dv) repr = 'dv

  let int  (x:int)  = x
  let bool (b:bool) = b

  let lam  f        = f
  let app  e1 e2    = e1 e2
  let fix  f        =
    let rec self n = f self n in self

  let add  e1 e2    = e1 + e2
  let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then et () else ee ()

end
\end{code}

As in~\S\ref{ourapproach},
this interpreter is patently tagless, using neither a universal type nor
any pattern matching: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. To run our
examples, we instantiate the |EX| functor from~\S\ref{encoding} with this |R|
module.
\begin{code}
module EXR = EX(R)
\end{code}
Thus |EXR.test1 ()| evaluates to the untagged boolean value |true|.
In Haskell, we define
\begin{code}
newtype R a = R {unR::a}
instance Symantics R where ...
\end{code}
Although |R| looks like a tag, it is only
a |newtype|.  The types |a| and |R a| are represented differently
only at compile time, not at run time.  Pattern matching against~|R|
cannot ever fail and is assuredly compiled away.
In OCaml, too, it is obvious to the compiler that
pattern matching cannot fail, because there is no
pattern matching. Evaluation can only fail to yield a value
due to interpreting |fix|.
\begin{prop}
If the term |e| encoded in the metalanguage has type~|t|,
then evaluating~|e| in the interpreter~|R| either continues
indefinitely or terminates with a value of the same type~|t|.
\end{prop}
Generalizing from~|R| to all interpreters, we have the following
broader and more useful, if also more obvious, proposition.
\begin{prop}
  If an implementation of |Symantics| never gets stuck, then
  the type system of the object
  language is sound with respect to the dynamic semantics defined by
  that implementation.
\end{prop}
These propositions follow immediately from the soundness of the
metalanguage's type system.

For variety, we show another interpreter, which measures the \emph{size}
of each object term, defined as the number of term
constructors. The following is slightly abbreviated code (see the
accompanying source code for the complete definition).
\begin{code}
module L = struct
  type ('c,'dv) repr = int
  let int (x:int)  = 1
  let lam f        = f 0 + 1
  let app e1 e2    = e1 + e2 + 1
  let fix f        = f 0 + 1
  let mul e1 e2    = e1 + e2 + 1
  let if_ eb et ee = eb + et () + ee () + 1
end
\end{code}
Now the expression
\begin{code}
let module E = EX(L) in E.test1 ()
\end{code}
evaluates to |3|. This interpreter is not only tagless but also
total. It ``evaluates'' even seemingly divergent terms like
\begin{code}
app (fix (fun self -> self)) (int 1)
\end{code}

\begin{comment}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{comment}

\section{A tagless compiler (or, a staged interpreter)}\label{compiler}

Besides immediate evaluation, we can compile our object language
into OCaml code using MetaOCaml's staging facilities. MetaOCaml
represents future-stage expressions of type |t| at the
present stage as values of type |('c,t) code| where |'c| is the
environment classifier \citep{WalidPOPL03,calcagno-ml-like}. Code values are created
by a \emph{bracket} form |.<e>.|, which specifies that the expression |e| is to be
evaluated at a future stage. The \emph{escape} |.~e| must occur
within a bracket and specifies that the expression |e| must be evaluated
at the current stage; its result, which must be a code value, is
spliced into the code being built by the enclosing bracket. The \emph{run} form |.!e| evaluates
the future-stage code value~|e| by compiling and linking it at run-time.
The bracket, escape, and run are akin to
quasi-quotation, unquotation, and |eval| of Lisp.

Inserting brackets and escapes appropriately into the
evaluator~|R| above yields the compiler~|C| below.
% this code does not have the 'sv parameter. It shows up later.
\begin{code}
module C = struct

  type ('c,'dv) repr = ('c,'dv) code

  let int (x:int)   = .<x>.
  let bool (b:bool) = .<b>.

  let lam f         = .<fun x -> .~(f .<x>.)>.
  let app e1 e2     = .<.~e1 .~e2>.
  let fix f = 
    .<let rec self n = .~(f .<self>.) n in self>.

  let add e1 e2     = .<.~e1 + .~e2>.
  let mul e1 e2     = .<.~e1 * .~e2>.
  let leq x y       = .<.~x <= .~y>.
  let if_ eb et ee = 
    .<if .~eb then .~(et ()) else .~(ee ())>.

end
\end{code}
This is a completely straightforward staging of
|module R|.
This compiler simply produces
residual code with no optimization. For example, interpreting our |test1|
\begin{code}
let module E = EX(C) in E.test1 ()
\end{code}
gives the code value |.<(fun x_6 -> x_6) (true)>.|
of inferred type |('c, bool) C.repr|.  Similarly, interpreting |testpowfix7|
with
\begin{code}
let module E = EX(C) in E.testpowfix7
\end{code}
gives the code value
\begin{code}
.<fun x_1 -> (fun x_2 ->
  let rec self_3 = fun n_4 ->
   (fun x_5 -> if x_5 <= 0 then 1 
               else x_2 * (self_3 (x_5 + (-1))))
   n_4
  in self_3) x_1 7>.
\end{code}
with many apparent $\beta$-redexes.

This compiler does not incur
any interpretive overhead: the
code produced for $\fun{x}x$ is simply |fun x_6 -> x_6| and does not
  call the interpreter, unlike the recursive calls to |eval0| and
  |eval| in the |L e| lines in \S\ref{tagproblem}.
The resulting code obviously contains no tags and no pattern matching.

We have also implemented this compiler in Haskell. Since Haskell
has no (convenient, typed) staging facilities, we had to emulate
them. To be precise, we defined a data type |ByteCode| with
constructors such as |Var|, |Lam|, |App|, |Fix|, and |INT|.
Whereas our representation of object terms uses HOAS,
our bytecode uses integer-named
variables to be realistic. We then define 
\begin{code}
newtype C t = C (Int -> (ByteCode t, Int)) 
\end{code}
where |Int| is the counter for creating fresh variable
names. We define the compiler by making |C| an instance of the
class |Symantics|. The implementation\footnote{The implementation uses
GADTs because we also wanted to write a typed interpreter for 
the \textsf{ByteCode} \emph{data type}.} is quite similar (but slightly more
verbose) than the corresponding MetaOCaml code given above. The
accompanying code gives the full details.

\section{A tagless partial evaluator}\label{PE}

Surprisingly, we can write a partial evaluator using the idea above,
namely to build object terms using ordinary functions rather than data
constructors.  We present this partial evaluator in a sequence of four
attempts. It uses no universal type and no tags
for object types.  We then discuss residualization and binding-time
analysis.  Our partial evaluator is a modular extension to the evaluator
in~\S\ref{S:interpreter-RL} and the compiler in~\S\ref{compiler}, in
that it uses the former to reduce static terms and the latter to build
dynamic terms.

\subsection{Avoiding polymorphic lift}
\label{S:PE-lift}

Roughly speaking, a partial evaluator interprets each object term either
to a static (present-stage) term, as produced by the evaluator~|R|, or
to a dynamic (future-stage) term, as produced by the compiler~|C|.  To
distinguish between static and dynamic terms, one might try
to define partial evaluation as the Haskell data type
\begin{code}
data P0 t = S0 (R t) | E0 (C t)
\end{code}
To extract a dynamic term from this type, we create the functions
\begin{code}
abstrInt :: P0 Int -> C Int
abstrInt (S0 r) = int (unR r)
abstrInt (E0 c) = c

abstrBool :: P0 Bool -> C Bool
abstrBool (S0 r) = bool (unR r)
abstrBool (E0 c) = c
\end{code}
We then start to define |P0| as an instance of the |Symantics| class.
\begin{code}
instance Symantics P0 where
  int  x = S0 (int x)
  bool x = S0 (bool x)
  add (S0 e1) (S0 e2) = S0 (add e1 e2)
  add e1 e2 = E0 (add (abstrInt e1)
                      (abstrInt e2))
\end{code}
Integer and boolean literals are obviously immediate, present-stage
values. Addition yields a static term if and only if both operands
are static. If so, we use the evaluator~|R| to add the operands.
If not, we extract dynamic terms from the operands and add them
using the compiler~|C|.
Thus we use the overloaded functions |int| and |bool| from the |Symantics|
instances for |R| and~|C| to define those for~|P0|.

Whereas |mul| and |leq| are as easy to define as |add|, we encounter
a problem with |if_|.  If the first argument to |if_| is a dynamic term
(type |C Bool|) and the second and third arguments are a static term
(type |R a|) and a dynamic term (type |C a|), then we need to convert
the static term to dynamic, but there is no polymorphic ``lift''
function |a -> C a| to send a value to the future stage
\citep{xi-guarded,WalidPOPL03}.\footnote{We note in passing that, if we
were to add polymorphic \texttt{lift} to the type class
\texttt{Symantics repr}, then \texttt{repr} would become an instance of
\texttt{Applicative} and thus \texttt{Functor}:\quad\texttt{fmap
f = app (lift f)}}

Our |Symantics| class includes only separate lifting methods |bool| and
|int|, not a parametrically polymorphic lifting method, for good reason:
When compiling to a first-order target language such as machine code,
booleans, integers, and functions may well be represented differently.
Thus polymorphic lift cannot be compiled without intensional type
analysis.  To avoid the need for polymorphic lift, we turn to
\citearound{'s technique}\citet[see also \citealp{sumii-hybrid}]{asai-binding-time}:
build a dynamic term
alongside every static term.

\subsection{Delaying binding-time analysis}
\label{S:PE-problem}

We switch to the Haskell data type
\begin{code}
data P1 t = P1 (Maybe (R t)) (C t)
\end{code}
so that a partially evaluated term |P1 t| always contains a dynamic
component and sometimes contains a static component.  The two
alternative constructors of a |Maybe| value, |Just| and |Nothing|,
tag each partially evaluated term with a phase: either present or
future.  This tag is not an object type tag: all pattern matching below
is exhaustive.  Because the future-stage component is always present, we
can now define the polymorphic function
\begin{code}
abstr1 :: P1 t -> C t
abstr1 (P1 _ dyn) = dyn
\end{code}
to extract it without requiring polymorphic lift into~|C|.  We then try
to define the interpreter |P1|---and get as far as the first-order
constructs of our object language, including |if_|.
\begin{code}
instance Symantics P1 where
  int  x = P1 (Just (int x)) (int x)
  bool b = P1 (Just (bool b)) (bool b)
  add (P1 (Just n1) _) (P1 (Just n2) _)
    = int (unR (add n1 n2))
  add e1 e2 = P1 Nothing
    (add (abstr1 e1) (abstr1 e2))
  -- mul and leq are analogous and elided
  if_ (P1 (Just s) _) et ef
    = if unR s then et else ef
  if_ eb et ef = P1 Nothing
    (if_ (abstr1 eb) (abstr1 et) (abstr1 ef))
\end{code}

When we come to functions, however, we stumble.  According to our
definition of~|P1|, a partially evaluated object function, such as the
identity $\fun{x}x$ embedded in Haskell as |lam (\x -> x)|\texttt{ ::
}|P1 (a -> a)|, consists of a dynamic part (type |C (a -> a)|) and
maybe a static part (type |R (a -> a)|).  The dynamic part is useful
when this function is passed to another function that is only
dynamically known, as in $\fun{k}k(\fun{x}x)$.  The static part is
useful when this function is applied to a static argument, as in
$(\fun{x}x)\True$.  Neither part, however, lets us \emph{partially}
evaluate the function, that is, compute as much as possible statically
when it is applied to a mix of static and dynamic inputs.  For example,
the partial evaluator should turn $\fun{n}(\fun{x}x)n$ into $\fun{n}n$
by substituting $n$ for~$x$ in the body of $\fun{x}x$ even though $n$ is
not statically known.  Even the same static function, applied to
different static arguments, can give both static and dynamic results: we
want to simplify $(\fun{y}x\times y)0$ to~$0$ but $(\fun{y}x\times y)1$
to~$x$.

To enable these simplifications, we delay binding-time analysis (BTA)
for a static function until it is applied, that is, until |lam f|
appears as the argument of |app|.  To do so, we have to incorporate |f|
as it is into the |P1| data structure: applying the type constructor
|P1| to a function type |a -> b| should yield one of
\begin{code}
data P1 (a -> b) = S1 (P1 a -> P1 b)
                 | E1 (C (a -> b))

data P1 (a -> b) = P1 (Maybe (P1 a -> P1 b))
                      (C (a -> b))
\end{code}
That is, we need a nonparametric data type, something akin to
type-indexed functions and type-indexed types, which
\citet{oliveira-typecase} dub the \emph{typecase} design pattern.

Thus typed partial evaluation, like typed CPS transformation,
inductively defines a map from source types to target types that
performs case distinction on the source type.  The connection with CPS
is not an accident, as we shall see in~\S\ref{S:CPS}.

\subsection{Eliminating tags from typecase}
\label{S:PE-GADT}

\begin{figure*}
\begin{floatrule}
\begin{code}
module type Symantics = sig

  type ('c,'sv,'dv) repr

  val int : int  -> ('c,int,int) repr
  val bool: bool -> ('c,bool,bool) repr

  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x) -> ('c,'x,'da->'db) repr
  val app : ('c,'x,'da->'db) repr -> (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x)
  val fix : ('x -> 'x) -> (('c, ('c,'sa,'da) repr -> ('c,'sb,'db) repr, 'da->'db) repr as 'x)

  val add : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val mul : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val leq : ('c,int,int) repr -> ('c,int,int) repr -> ('c,bool,bool) repr
  val if_ : ('c,bool,bool) repr -> (unit -> 'x) -> (unit -> 'x) -> (('c,'sa,'da) repr as 'x)

end
\end{code}
\end{floatrule}
\caption{A (Meta)OCaml embedding of our object language that supports partial evaluation}
\label{fig:ocaml}
\end{figure*}

\begin{figure}
\begin{floatrule}
\begin{code}
module P = struct

  type ('c,'sv,'dv) repr = {st: 'sv option;
                            dy: ('c,'dv) code}

  let abstr {dy = x} = x
  let pdyn x = {st = None; dy = x}

  let int  (x:int)  = {st = Some (R.int x);
                       dy = C.int x}
  let bool (x:bool) = {st = Some (R.bool x);
                       dy = C.bool x}

  let add e1 e2 = match e1, e2 with
  | {st = Some 0}, e | e, {st = Some 0} -> e
  | {st = Some m}, {st = Some n} -> int (R.add m n)
  | _ -> pdyn (C.add (abstr e1) (abstr e2))

  let if_ eb et ee = match eb with
  | {st = Some b} -> if b then et () else ee ()
  | _ -> pdyn (C.if_ (abstr eb) 
                     (fun () -> abstr (et ()))
                     (fun () -> abstr (ee ())))

  let lam f =
  {st = Some f; 
   dy = C.lam (fun x -> abstr (f (pdyn x)))}

  let app ef ea = match ef with
  | {st = Some f} -> f ea
  | _ -> pdyn (C.app (abstr ef) (abstr ea))

  let fix f = 
    let fdyn = C.fix (fun x -> abstr (f (pdyn x)))
    in let rec self = function
       | {st = Some _} as e -> app (f (lam self)) e
       | e -> pdyn (C.app fdyn (abstr e))
       in {st = Some self; dy = fdyn}

end
\end{code}
\end{floatrule}
\caption{Our partial evaluator (\texttt{mul} and \texttt{leq} are elided)}
\label{fig:pe}
\end{figure}

Two common ways to provide typecase in Haskell are
GADTs and type-class functional dependencies
\citep{oliveira-typecase}.  These
methods are equivalent, and here we show the GADT way; |incope1.hs|
in the accompanying source code shows the latter.

We introduce a GADT with four data constructors.
\begin{code}
data P t where
  VI :: Int  -> P Int
  VB :: Bool -> P Bool
  VF :: (P a -> P b) -> P (a -> b)
  E  :: C t -> P t
\end{code}
The constructors |VI|, |VB|, and |VF| build static terms (like |S0|
in~\S\ref{S:PE-lift}), and |E| builds dynamic terms (like |E0|).  However,
the type |P t| is no longer parametric in~|t|: the constructor |VF| takes an
operand of type |P a -> P b| rather than |a -> b|. As before, we define a
function to extract a future-stage computation from a value of type |P t|.
\begin{code}
abstr :: P t -> C t
abstr (VI i) = int i
abstr (VB b) = bool b
abstr (VF f) = lam (abstr . f . E)
abstr (E x)  = x
\end{code}
The cases of this function |abstr| are type-indexed.  In particular, the |VF f|
case uses the method |lam| of the |C| interpreter to compile~|f|.

We may now make |P| an instance of
|Symantics| and implement the partial evaluator as follows. We elide
|mul|, |leq|, |if_|, and |fix|.
\begin{code}
instance Symantics P where
  int x  = VI x
  bool b = VB b
  add (VI n1) (VI n2) = VI (n1 + n2)
  add e1 e2 = E (add (abstr e1) (abstr e2))
  lam = VF
  app (VF f) ea = f ea
  app (E f)  ea = E (app f (abstr ea))
\end{code}
The implementations of |int|, |bool|, and |add| are like
in~\S\ref{S:PE-problem}.
The interpretation of |lam f| just wraps the
HOAS function |f|. We can always compile |f| to a code value,
but we delay it to apply |f| to concrete arguments. The interpretation of
|app ef ea| checks to see if |ef| is such a delayed
HOAS function |VF f|. If it is, we apply |f| to the
concrete argument |ea|, giving us a chance to perform static
computations (see example |testpowfix7| in~\S\ref{S:PE-solution}). If |ef| is a
dynamic value |E f|, we residualize.

This solution using GADTs works but is not quite satisfactory. First, it
cannot be ported to MetaOCaml as GADTs are unavailable there.  Second,
the problem of nonexhaustive pattern\hyp matching reappears in |app|
above: the type |P t| has four constructors, of which pattern\hyp
matching in |app| uses only |VF| and~|E|. One may say that the
constructors |VI| and |VB| obviously cannot occur because they do not
construct values of type |P (a -> b)| as required by the type of |app|.
Indeed the metalanguage implementation could reason thus, but it may not; GHC for one
issues warnings.  Although this point may seem minor, it is the heart of
the tagging problem and the purpose of tag elimination. A typed tagged
interpreter contains many pattern\hyp matching forms that look partial
but never fail in reality. The goal is to make this safety
\emph{syntactically} apparent.

\subsection{The ``final'' solution}
\label{S:PE-solution}
Let us re-examine the problem in~\S\ref{S:PE-problem}. What we
would ideally like is to write
\begin{code}
data P t = P (Maybe (repr_pe t)) (C t)
\end{code}
where |repr_pe| is the type function defined
% inductively because P below depends on repr_pe
by
\begin{code}
repr_pe Int      = Int
repr_pe Bool     = Bool
repr_pe (a -> b) = P a -> P b
\end{code}
Although we can use type classes to define this type function
in Haskell, that is not portable to MetaOCaml. However,
these three typecase alternatives are already present in existing
methods of |Symantics|.
A simple and portable solution thus emerges: we bake |repr_pe| 
into the signature |Symantics|. Switching to MetaOCaml to demonstrate this
portability, we recall from Figure~\ref{fig:ocaml-simple} in~\S\ref{encoding} that the |repr| type
constructor took two arguments |'c| and~|'dv|. We add an argument
|'sv| for the result of applying |repr_pe| to~|'dv|.
Figure~\ref{fig:ocaml} shows the new |Symantics| signature.

The interpreters |R|, |L| and~|C| above only use the old
type arguments |'c| and~|'dv|, which are treated by the new signature
in the same way.  Hence all that needs to change in these interpreters
to match the new signature is to add a phantom type
argument~|'sv| to~|repr|.
For example, the compiler |C| now begins
\begin{code}
module C = struct
  type ('c,'sv,'dv) repr = ('c,'dv) code
  (* the rest is the same *)
\end{code}
In contrast, the partial evaluator~|P| relies on the type argument |'sv|.

Figure~\ref{fig:pe} shows the partial evaluator~|P|.
Its type |repr| literally expresses the type equation for |repr_pe| above.
The function |abstr|, as in~\S\ref{S:PE-GADT},
extracts a future-stage code value from a result of
partial evaluation.  Conversely, the function |pdyn| injects a
code value into partial evaluation. As
in~\S\ref{S:PE-problem}, we build dynamic terms alongside
any static ones to avoid polymorphic lift.

To illustrate how to add optimizations, we improve |add| (and |mul|,
elided) to simplify the generated code using the monoid (and ring)
structure of~|int|: not only is addition performed statically
(using~|R|) when both operands are statically known, but it is
eliminated when one operand is statically~$0$; similarly for
multiplication by~$0$ or~$1$.  Such algebraic simplifications are easy
to abstract over the specific domain (such as monoid or ring) where they
apply.  These simplifications and abstractions help a lot in a large
language with more base types and primitive operations.

Any partial evaluator must decide how much to unfold recursion
statically: unfolding too little can degrade the residual code, whereas
unfolding too much risks nontermination.  Our partial evaluator is no
exception, because our object language includes |fix|.  The code in
Figure~\ref{fig:pe} takes the na\"\i ve approach of ``going all the
way'', that is, unfold |fix| rather than residualize whenever the
argument is static.  In the accompanying source code is a conservative
alternative |P.fix| that unrolls recursion only once, then residualizes.
Many sophisticated approaches have been developed to make this trade-off
\citep{jones-partial}, but this issue is orthogonal to our presentation.
A separate concern in our treatment of |fix| is possible code bloat in
the residual program, which calls for let-insertion
\citep{SwadiTahaKiselyovPasalic2006}.

Given this implementation of~|P|, our running example
\begin{code}
let module E = EX(P) in E.test1 ()
\end{code}
evaluates to
\begin{code}
{P.st = Some true; P.dy = .<true>.}
\end{code}
of type |('a, bool, bool) P.repr|.  Unlike with~|C| in~\S\ref{compiler},
a $\beta$-reduction has been statically performed to yield |true|.  More
interestingly, whereas |testpowfix7| compiles to a code value with many
$\beta$-redexes in~\S\ref{compiler}, the partial evaluation
\begin{code}
let module E = EX(P) in E.testpowfix7
\end{code}
gives
\begin{code}
{P.st = Some <fun>;
 P.dy = .<fun x_1 -> x_1 * (x_1 * (x_1 * (x_1 *
                    (x_1 * (x_1 * x_1)))))>.}
\end{code}
as desired.

Unlike the GADT approach in~\S\ref{S:PE-GADT}, all pattern\hyp matching
in~|P| is \emph{syntactically} exhaustive, so it is patent to the metalanguage
implementation that |P| never gets stuck.  Further, all pattern\hyp matching occurs
during partial evaluation, only to check if a value is known statically,
never what type it has.  In other words, our partial evaluator tags
phases (with |Some| and |None|) but not object types.

Our typed partial evaluator is online and polyvariant.  It reuses the
compiler~|C| and the evaluator~|R| by composing them.  This situation is
simpler than \citets{SperberThiemann:TwoForOne} composition of a partial
evaluator and a compiler, but the general ideas are similar.

\section{Variations and discussion}\label{discussion}

We show that our |Symantics| and generally our approach accomodates
a number of variants without any (new) difficulties.  In particular,
we show how a call-by-name CPS interpreter, a call-by-value CPS
program transformation, as well as how to deal with imperative features.
Finally, we comment on the requirements on the type system to support 
our approach.

\subsection{Call-by-name CPS interpreters}\label{S:CPS}

The object language generally inherits the evaluation strategy from
the metalanguage. Evaluating an application |e1 e2| in OCaml requires
the evaluation of |e2| before performing the application. Therefore,
when evaluating the encoding of the object language application 
|app e1 e2|, |e2| will always have to be evaluated. One may wonder if it is
possible to represent a call-by-name (CBN) object language in a
call-by-value (CBV) metalanguage. The answer, given already by 
\citet{reynolds-definitional,reynolds-relation} and \citet{PlotkinCBN}, is yes thanks to CPS, which they
specifically introduced 
to make the evaluation strategy of a definitional interpreter
independent of the strategy of the metalanguage. We demonstrate the
same independence in the typed setting \oleg{mention the quote from
Reynolds that typed setting would be nice?}\jacques{I think so, but from
which paper? The one ReynoldsPlotkin paper is typed}\ccshan{I think Oleg
meant ``It should also be possible to define languages with a highly
 refined syntactic type structure. Ideally, such a treatment should be
 metacircular, in
 the sense that the type structure used in the defined language should be adequate for the
 defining language'' \citep{reynolds-definitional}, but perhaps Jacques
 meant another paper by ``ReynoldsPlotkin''?}. Specifically, we
demonstrate CBN evaluation for our object language in OCaml, which is
call-by-value. We do so by building a CBN CPS interpreter for the object
language.

The CBN CPS interpretation was introduced by \citet{PlotkinCBN}. As in
CBV CPS, the interpretation of an object term is a function
mapping a continuation~|k| to the answer
returned by~|k|.
\begin{code}
let int (x:int) = fun k -> k x
let add e1 e2 = fun k ->
  e1 (fun v1 -> e2 (fun v2 -> k (v1 + v2)))
\end{code}
In both cases, the returned value has type 
|(int -> 'w)->'w| where |'w| is the (polymorphic) answer type.

The interpretation of abstraction and application in CBN CPS is
different from CBV CPS and is as follows:
\begin{code}
let lam f = fun k -> k f
let app e1 e2 = fun k -> e1 (fun f -> (f e2) k)
\end{code}
In the tell-tale sign of CBN, in interpreting the application
we do not `evaluate' the argument e2, that is, we do apply it to the
continuation. Rather, we pass the unevaluated |e2| to the abstraction.
The result of |lam (fun x -> add x (int 1))| has the type
|((((int->'w1)->'w1) -> ((int->'w1)->'w1)) -> 'w2) -> 'w2|.
We want to collect the above interpretation functions into a module
with the |Symantics| signature, so as to fit the CBN CPS interpreter within our
general framework. Alas, we encounter the same problem we had in
Section~\ref{S:PE-problem}, in writing a type equation for the |repr|
type that can represent both the type of |int i| and the type of 
|lam f|. We cannot say that the result of interpreting an object
expression of type |t| is |(t->'w)->'w| because if |t| happens to
be an arrow type (such as |int->int| in the above example), the result
type obviously does not fit the desired pattern. We need a type
function with a typecase distinction. In Section~\ref{S:PE-solution} we have
shown how to encode such a function, using the extra type
argument |'sv| of the |repr| type. Furthermore, we notice that the type 
function |repr_pe| needed for the partial evaluator 
(in~\S\ref{S:PE-solution}) is precisely the same type function we
need for CBN CPS. This gives us for the following CBN CPS interpreter,
with the |Symantics| signature:

\begin{code}
module RCN = struct
  type ('c,'sv,'dv) repr = 
     {ko: 'w. ('sv -> 'w) -> 'w}
  let int (x:int) = {ko = fun k -> k x}
  let add e1 e2 = 
    {ko = fun k -> e1.ko (fun v1 -> 
                   e2.ko (fun v2 -> k (v1+v2)))}
  let if_ eb et ee = 
    {ko = fun k -> eb.ko 
         (fun vb -> if vb then (et ()).ko k 
                    else (ee ()).ko k)}
  ...
  let lam f = {ko = fun k -> k f}
  let app e1 e2 = 
     {ko = fun k -> e1.ko (fun f -> (f e2).ko k)}
  let fix f = 
     let rec fx f n = app (f (lam (fx f))) n in 
     lam(fx f)

  let get_res x = x.ko (fun v -> v)
end
\end{code}

We have made our interpreter fully polymorphic over the answer type.
We could have just as easily made |RCN| a functor, parameterized over
the answer type -- as is common when writing CPS in SML. In the
present case, we chose to use the higher-rank polymorphism of offered by
OCaml record types.

Because |RCN| has the signature |Symantics|, we can instantiate our previous
examples with it, and all works as expected.  More interesting
is the example $(\fun{x}1)\bigl((\fix{f}f)\mathinner2\bigr)$, which terminates
under CBN but not CBV\@.
\begin{code}
module EXS(S: Symantics) = struct
 open S
 let diverg () = 
   app (lam (fun x -> int 1)) 
       (app (fix (fun f -> f)) (int 2))
end
\end{code}

If we interpret |EXS| with the |R| interpreter of
\S\ref{S:interpreter-RL} realized in OCaml
\begin{code}
let module M = EXS(R) in M.diverg ()
\end{code}
we never get any result because the above diverges. In contrast, if we use
the CBN interpreter
\begin{code}
let module M = EXS(RCN) in RCN.get_res (M.diverg ())
\end{code}
we get the result |1|.

\oleg{mention some formal results? Like indifference theorems by
  Plotkin? And say we generalized their result to the type setting?
  Mention this in the conclusions?}
\jacques{you have strayed outside what I know -- what Plotkin results?
  I always consider mentionning formal results a good thing}

\subsection{CBV CPS transformers}

We can easily change our CBN CPS interpreter into a CBV CPS
interpreter, by replacing only one definition:
\begin{code}
module RCV = struct
  include RCN
  let lam f = {ko = 
    fun k -> k (fun e -> e.ko 
         (fun v -> f {ko = fun k -> k v}))}
end
\end{code}
Now when we apply an abstraction to an argument |e|, we always force
the evaluation of that argument before proceeding. This approach is in
line with \citet{reynolds-relation} (albeit in a typed setting). The interpreter RCV
is useful for achieving CBV evaluation of the object language
no matter whether the metalanguage is CBV or CBN.

In this section however we show a more general approach to CBV CPS:
a CPS transformer. We take any implementation of |Symantics| (interpreter,
compiler, partial evaluator) and make it evaluate the CPS language
instead. We can view interpreter transformation as a
transformation on the object language, from direct to
continuation-passing style.

The transformation is canonical, as the following (abbreviated) code
shows
\begin{code}
module CPST(S: Symantics) = struct
  let int i = S.lam (fun k -> S.app k (S.int i))
  let add e1 e2 = S.lam (fun k ->
    S.app e1 (S.lam (fun v1 ->
    S.app e2 (S.lam (fun v2 -> S.app k (S.add v1 v2))))))
  let if_ ec et ef = S.lam (fun k ->
    S.app ec (S.lam (fun vc ->
    S.if_ vc (fun () -> S.app (et ()) k) (fun () -> S.app (ef ())
    k))))
  ...
  let lam f = S.lam (fun k -> S.app k (S.lam (fun x ->
    f (S.lam (fun k -> S.app k x)))))
  let app e1 e2 = S.lam (fun k -> 
    S.app e1 (S.lam (fun f ->
    S.app e2 (S.lam (fun v -> S.app (S.app f v) k)))))
  let fix f  = S.fix (fun self -> (f self))
end
\end{code}
The code literally establishes a mapping from CPS interpretations to
(direct) interpretations, the latter performed by 
the base interpreter |S|.

A reader may notice that the above module does not define the type
equation for |repr| and so it does not have the signature Symantics.
The reason for that is again the result type of |lam f|. Whereas
|int i| or |add e1 e2| have the (abbreviated) 
type |('c,..., (int -> 'w) -> 'w) S.repr|,
the result of |lam (fun x -> add x (int 1))| has type
|('c,...((int -> (int -> 'w1) -> 'w1) -> 'w2) -> 'w2) S.repr|. 
Hence, to write the type equation for |CPST.repr| we need a 
type-discriminating
function similar to the type function |repr_pe| of
\S\ref{S:PE-solution}. Alas, the function we need here is
similar but not identical to |repr_pe|. Thus we need to add 
another type variable to the |repr| type and modify the Symantics
signature accodringly. Again, the terms in previous Symantics module
stay unchanged, but the |repr| type equations in those structures have to
account for a new (phantom) type variable.

To save space, we show a simplified and less principled approach that
lets us use the module |CPST| as it is. Because the module does not
have the signature Symantics, we cannot apply the |EX| functor to it.
But we can write the tests nevertheless:
\begin{code}
module T = struct
 module M = CPST(C)
 open M
 let test1 () = app (lam (fun x -> x)) (bool true)
 let testpowfix () = ... (* the same as before *)
 let testpowfix7 = (* the same as before *)
    lam (fun x -> app (app (testpowfix ()) x) (int 7))
end
\end{code}
That is, we instantiate |CPST| with the desired base intrepreter (the
compiler |C| in the case above) so that we can use the result |M| to
interpret object level terms. The sample terms are \emph{exactly}
as we used before. We just copied them over. That copying step is the
price we pay for the simplified treatment. We shall also encounter this
copying when discussing the self-interpreter in the next section. We
will see then that the copying step is not frivolous, but represents
a profound instance of 
``plugging the hole'', which is one of the many faces of polymorphism.

It is instructive to see the result of evaluating a few examples. With 
|CPST| instantiated with a straightforward compiler |C| as above, our
running test |T.test1 ()| gives
\begin{code}
.<fun x_5 -> ((fun x_2 -> 
        (x_2 (fun x_3 -> fun x_4 -> (x_4 x_3))))
        (fun x_6 -> ((fun x_1 -> (x_1 (true))) 
        (fun x_7 -> ((x_6 x_7) x_5)))))>.
\end{code}

which shows the naive CPS transform of the original code. The result is
quite unoptimal with several apparent $\beta$-redexes. 
By instantiating |CPST| with the |P| evaluator instead, our
running test gives the expected
\begin{code}
  P.repr = {P.st = Some <fun>; P.dy = .<fun x_5 -> (x_5 (true))>.}
\end{code}

\subsection{State and imperative features}

We can easily modify the CBV CPS transform to pass a piece of state
along with the continuation. This lets us support mutable state. We
can also use reference variables of the metalanguage to emulate
reference variables in the object language.
\oleg{Should we have an example in the code?}
\jacques{In the code, yes.  Probably not in the paper, it is getting 
long as it is}

\subsection{Translucent types}

The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
|app _add (app _int 1) (app _int 2)| where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is

$$ 
  (\ZZ \rightarrow r \ZZ) \rightarrow
  (\BB \rightarrow r \BB) \rightarrow
  \forall \alpha \beta. (r \alpha \rightarrow r \beta)
      \rightarrow r (\alpha\rightarrow\beta) \rightarrow ... r \tau
$$
% (Int -> r Int) ->
% (Bool -> r Bool) ->
% (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau

or, if we denote the sequence of initial arguments as S r, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.

The attempt to encode self-interpreter brings the |*->*| polymorphism
the forefront, as we shall now see.

\section{Self-interpretation}\label{selfinterp}

A self-interpreter |si| is an
object-language term such that, for any object-language term e,

\begin{code}
  si "e"    is observationally equivalent to    e
\end{code}

where |"e"| is an encoding of |e|.  In our approach, we want to encode
each object-language term as a function from a record%
\footnote{Thanks to currying, the object language need not actually support
records.}  of interpreter methods to a generated representation.  
In the case of a self-interpreter, that
record is simply |si|!  So we should define a self-interpreter |si| to be an
object-language term such that, for any object-language term |e|,

\begin{code}
  "e" si    is observationally equivalent to    e
\end{code}

A self-interpreter is then just like the interpreter
|R| in Section 2, except written at the object rather than the meta level.

It is wortwhile noting that, in our class |Symantics|, |add|
is not a term since it does not have type |repr t| for any |t|. We could
have introduced |add| to be of type |repr (Int->Int->Int)| -- but we
didn't. Thus |add| is a special form, which is to be applied in the
meta-language using the meta-language application syntax (i.e. white
space) rather than in the object language (i.e. via |app|). 

\oleg{Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).}

Self-interpretation in our framework is somewhat
tricky due to the need for more polymorphism than is easily available
in the meta-language.  An encoded term should
be polymorphic in the interpreter type-constructor "r".  Furthermore, the
functions "lam", "app", etc. should be polymorphic in the object
types "a" and "b".  Thus the object language seems to require rank-2
polymorphism; and so type annotations will be inevitable.

To neatly bypass the need for rank-2 polymorphism, we use the notion
of let-polymorphism and a `hole', as follows.  A partial evaluator (or
a compiler or interpreter) is an evaluation context that (let-)binds
variables named "app", "lam", etc.  For the object language to
take advantage of let-bound polymorphism in the metalanguage, it
is crucial that we encode the object-level "let" using the meta "let".  

We also (unfortunately need to) define a self-interpreter slightly
differently.  A self-interpreter |SI[]| is an object expression with a
hole |[]| waiting to be plugged with the encoding of an object expression.
The self-interpreter binds the free variables |_lam|, |_app|, |_int|, |_bool|,
etc. in the plugged encoding.  We require |SI["E"]| to be equivalent to |E|,
where |"E"| is the object encoding of the object expression |E|.

This strategy of course limits what kinds of processing on encoded
object expressions can be expressed in the object language.  For
example, unlike if we had simply defined |"E"| to be the \emph{identity
encoding} E, we \emph{can} write a size-measure interpreter |SZ[]|, 
which like |SI[]| is
an object expression with a hole |[]| waiting to be plugged with 
an |"E"|.
However, this notion of size cannot distinguish |let x = E1 in E2[x]|
from |"E2[E1]"|.  Recall that one way to type-check polymorphic let is:
\begin{code}
        E1:T    E2[E1]:T'
    ------------------------
    let x = E1 in E2[x] : T'
\end{code}
That analogy shows that the self-interpreter with a hole is a
euphemism for a higher-rank abstraction. That makes it clear where
the difficulty lies, and how to simulate the higher-rank application
(that is, higher-rank arrow elimination) `by hand'.

We also need to duplicate the encoding of an object expression if we
want to process the same object expression in multiple ways (e.g., both
measure its size and interpret it).  But all this is consistent with (in
fact, amplifies) our `coalgebraic/final/cogen' spirit, so our revised
notion of self-interpretation (and, some might say, of size measurement)
may just squeak by the reader's skepticism as we explain our desire for
kind polymorphism.

\oleg{I guess the let-and-hole argument works out if we don't typecheck the
encoded terms. We can't typecheck the interpreter with the hole
either. We insert the encoded term into the interpreter -- and then
typecheck the whole thing. That'll work. Yet a bit unsatisfactory...}

We achieve
self-interpretation in that the following evaluation context in the
object language defines an interpreter for encoded object terms.
\begin{code}
  let lam = \f -> f in
  let app = \f x -> f x in
  let add = \m n -> m + n in
  let int = \i -> i in
  [hole]
\end{code}
The obvious notion of optimality in this setup is easy to achieve.  For
example, given the context above, our partial evaluator turns
\begin{code}
  app (lam (\x -> add x (int 1))) (int 3)
\end{code}
into the number 4, as desired.  We can also encode the self-interpreter
as the following evaluation context in the object language.
\begin{code}
  let lam = lam (\f -> f) in
  let app = lam (\f -> lam (\x -> app f x)) in
  let add = lam (\m -> lam (\n -> add m n)) in
  let int = lam (\i -> i) in
  [hole]
\end{code}
The "let" and the hole "[hole]" are meta-constructions, which must map to
themselves in any interpretation.  

The 'result' of applying an interpreter to itself needs to be an 
interpreter too!  In fact, if it is Jones-optimal, then you should be 
able to apply it to itself again and get the same answer.  
Our code does that. First, let |twice_inc_3_e| be the
metalanguage encoding of the object term |si["e"]|,
where si is the self-interpreter for the needed components of the object
language (only |lam|, |app|, |add|, and |int| in this case), |""| means the
object-to-object encoding, and |e| is the object term
\begin{code}
  let twice_ f x = f (f x) in let inc_ n = n + 1 in twice_ inc_ 3
\end{code}
Second, |twice_inc_3_ee| is the metalanguage encoding of the object term
|si["si[e]"]|
which is the same as
|si["si"["e"]]|.
The partial evaluator yields 5 on both |twice_inc_3_e| and 
|twice_inc_3_ee|,
as desired.

\jacques{But how 
to you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?}
\oleg{Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.}
Again, what is different from the above is that we can
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).

\section{Related work}\label{related}

Our initial motivation came from several papers 
\citep{WalidICFP02,taha-tag,xi-guarded,peyton-jones-simple} which used
embedded interpreters as a central example to justify looking 
into advanced type systems.  While we admire all this technical work,
there remained the issue that we were not convinced that their 
motivating example actually needed all this machinery.

\citet{WalidICFP02}, when referring to the work of \citet{taha-tag},
claims that this shows that self-interpretation is possible in a
\emph{simply typed $\lambda$-calculus}, but this is somewhat misleading
as their type system does not differentiate between the various ground
types.  In other words, various constants (like \textsf{nil} and 
\textsf{true}) both inhabit the same type $D$.  This is not very satisfying.
Furthermore, as is pointed out in the introduction to \citet{WalidICFP02},
\citet{taha-tag} does not \emph{statically} guarantee that all tags will
be removed - tag elimination must be performed at runtime.

From reading \citet{WalidICFP02,taha-tag,xi-guarded,peyton-jones-simple}, our 
impression is that the claim is:
\begin{enumerate}
\item One needs to encode a typed language in a typed language based on
a sum type (at some level of the hierarchy),
\item It is not possible to write a \emph{direct} interpreter 
for such an encoding of a typed language
in a typed language, without either using a
very advanced type system or incurring tagging/untagging overhead.
\item Thus an indirect interpreter is necessary, which needs a universal
  type (and hence tagging),
\item Thus any self-interpreter must have tags.
\item Hence the self-interpreter cannot be Jones-optimal.
\end{enumerate}
While the logic is sound, we showed that the premise in the very first step
was not valid.

In particular, \citet[\S5]{taha-tag}
states that ``expressing such an interpreter in a statically typed
programming language is a rather subtle matter. In fact, it is only
recently that some work on programming type-indexed values in ML
\citep{yang-encoding} has given a hint of how such a function can be
expressed.''  

\citet{Danvy-tagging-encoding} discusses Jones optimality at length.
There, the authors demonstrate the usefulness of HOAS for typed
self-interpretation.  However, no claim is made that the source program
is itself typed -- just that the interpreter is.  Our reading is that
the source language is not typed.  However, in sharp contrast with 
their interpreter, none of our evaluators can product pattern-match errors
(unlike the code that accompanies \citet{Danvy-tagging-encoding}).
In other words, their language (and interpreter) does use tags, however
the shift to HOAS allows the partial evaluator to remove all the tags.
In contrast, we have no tags at all.

We used a method from \citet{Asai-SAS99} to reduce the amount
of back-and-forth between reify/reflect: in our PE we track both
static and dynamic values in parallel (whenever possible).  In some sense,
we are observing that this method type-checks in Hindley-Milner once we
deforest the static code representation.  Put another way, we have
manually applied type-level partial-evaluation to our typelevel 
functions (see \S\ref{S:pe-solution}) to obtain simpler types 
acceptable to MetaOCaml.  It is worthwhile to note that 
\citet{Sumii-HOSC2001} also takes advanatage of this method
\citep[of][]{Asai-SAS99} to get interesting results combining online
and offline partial evaluation.  Like \citet{SperberThiemann:TwoForOne},
we strive for modularity by reusing earlier stages.  It would be interesting
to see if we could derive a \emph{cogen} \citep{Thiemann:cogeninsixlines}
in the same manner.

The approach to HOAS via catamorphism and anamorphism of
\citet{Washburn-Weirich-boxes} seems related to what we are doing.
We have not investigated the relationship to any serious depth, but we
feel that one could fruitfully combine their ideas with ours.

A lot of effort has gone into finding ways to give precise types to untyped
terms (see for example \citet{baars-typing,Guillemette-Monier-PLPV}
as well as the messages \citet{haskell-019160,haskell-019161}).  There 
take advantage of the host language's types to varying extents.  All 
these approaches are very successful for proper terms, but less so for
ill-typed terms, where error messages can be quite obscure.  We use the
host's typing facilities to do all the work, and thus inherit the strengths
and weaknesses of the host language.  While that may not be optimal, it is
certainly much more modular.

\jacques{Got to here}.
Regarding our implementation of the type-level function |'sv -> 'dv|.
Gibbons et al paper on
the implementations of typecase may be relevant. I have not read it,
but I have the feeling some of their typecase designs may be
relevant (at least to cite). 
However, the typecase paper relies on having a 
bijection between types -- our |'sv| and |'dv| in this case, which is not 
our situation.  We have a one-sided view of things, where |'sv -> 'dv| is 
easy, and |'dv| "perfectly reflects" an |'sv|, but you still can't get at 
that |'sv|.  \jacques{Could we say that we have a meta-level Galois
injection?  We have one direction completely covered at the object
level (and so easy to lift), but 'dv to 'sv can only be done by
an outside mechanism}


Self-Interpretation and Reflection in a Statically Typed Language 
\url{http://webpages.cs.luc.edu/~laufer/papers/reflection93.pdf}
He uses the SK language and some typeclasses. Their interpreter is a
meta-circular interpreter -- rather than self-interpreter. We can
claim to have implemented a far better interpreter for a far richer
language.

co-algebra references.  Many.

\section{Conclusions}\label{conclusion}

We were dissatisfied with current approaches to DSL embedding: either
the performance was hurt by tagging/untagging and other interpretive
overhead, or the offered solutions required too much expressive power
from the host language's type system.  By shifting from an initial
algebra approach to one which leverages the natural co-algebraic structure
of the $\lambda$-calculus, we show that both of these issues can be 
simultaneously dealt with.

% We show how to do many things without tags, in a simple type system, 
% essentially by avoiding any (inductive) types.  

\jacques{Should we mention that algebraic type tags give us a partly
intensional representation of terms.  This nicely allows pattern matching,
but as sits inside an extensional theory, ensuring that all traces 
of these tags disappear seems quite difficult without a very powerful
type system (enter GADTs).}


The overarching theme of our approach -- representation of object terms in 
\S\ref{ourapproach}, future-stage code in \S\ref{S:PE-lift} and static
values in \S\ref{S:PE-solution} and CPS values in \S\ref{S:CPS} --
demonstrates that pushing the computation towards the producers
\oleg{or, constructors. Ken said: functionalization} generally
requires simpler type systems (such as mere HM). The same idea appears
in many other areas (e.g., ammortized heap analysis, TFP/TLCA paper).
\oleg{I was told that speculation and informality is permitted in
  conclusions.}

separate compilation: separate
typechecking of terms without knowing which particular
hole they will be plugged into. 



\subsection*{Acknowledgments}
We would like to thank Martin Sulzmann and Walid Taha 
for helpful discussions.

\bibliographystyle{mcbride}
\bibsep=0pt
\bibliography{tagless}
\end{document}
