\documentclass[preprint]{sigplanconf}
\usepackage{amsmath,amssymb}
\usepackage{comment}
\usepackage{hyphenat}
\usepackage{url}
\usepackage{prooftree1}

\newcommand{\jacques}[1]{{\it [Jacques says: #1]}}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}
\newcommand{\ccshan}[1]{{\it [Ken says: #1]}}

\usepackage{natbib1}
\let\cite=\citep

\usepackage[compact]{fancyvrb1}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=1em,fontsize=\small}

\newtheorem{prop}{Proposition}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\BB}{\mathbb{B}}

\newcommand{\fun}[1]{\mathopen{\lambda\mathord{#1}.\,}}
\newcommand{\fix}[1]{\mathopen{\mathrm{fix\,}\mathord{#1}.\,}}
\newcommand{\Forall}[1]{\mathopen{\forall\mathord{#1}.\,}}
\newcommand{\Exists}[1]{\mathopen{\exists\mathord{#1}.\,}}
\newcommand{\cond}[3]{\mathrm{if\,}#1\mathrm{\,then\,}#2\mathrm{\,else\,}#3}

% Make space around section headings no less than space around theorems
\makeatletter
\@tempdima\topsep \advance\@tempdima\parsep
\ifdim\@sectionbelowskip   <\topsep \@sectionbelowskip   \topsep \fi
\ifdim\@subsectionbelowskip<\topsep \@subsectionbelowskip\topsep \fi
\makeatother

% Match the rule inserted by \@makecaption in sigplanconf.cls
\setproofrulebreadth .33pt
\newenvironment{floatrule}
    {\hrule width \hsize height .33pt \vspace{.5pc}}
    {\par\addvspace{1ex}}

% Make some Rel symbols into Bin symbols instead
\DeclareMathSymbol{\to}{\mathbin}{symbols}{"21}
\DeclareMathSymbol{:}{\mathbin}{operators}{"3A}

\begin{document}

\conferenceinfo{ICFP '07}{Freiburg, Germany} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Staged typed type-preserving tagless ``interpreters'', \emph{finally}}
\subtitle{Subtitle Text, if any}

\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}

\maketitle

\begin{abstract}
We have built a (family of) tagless partial evaluator(s) for a typed object
language (embedded DSL), placing minimal requirements on
the type system of the meta-language:
we need let-bound polymorphism and staging, but not dependent
types, generalized algebraic data types, nor a tag elimination 
postprocessing step.

The main tool is to take functions seriously: we encode the
higher-order abstract syntax of object terms not by using constructors for
an algebraic data type but rather by invoking cogen functions, which occur as
free variables in the encoded term.  In other words, we eschew initial
algebras for the wilder world of final algebras.
Our family of evaluators contains an interpreter, a compiler, a partial
evaluator, a term-size evaluator and a CPS call-by-name transformer.
Our approach highlights the importance of order-2 type constructors
(rank-2 polymorphism), which lets us encode DSL terms with
translucent types: enough abstraction is available to interpret the
terms in various ways, and yet enought type information is available 
to typecheck the terms and statically assure their interpretation will
never get stuck.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}\label{intro}

A popular way to define and implement a programming language is to embed it in
another \citep{reynolds-definitional}.  The embedded language is called the
\emph{object} or \emph{subject} language, whereas the embedding language is the
\emph{metalanguage}.  Embedding is especially appropriate for domain\hyp
specific object languages because it supports rapid prototyping and integration
with the host environment.  If the metalanguage supports \emph{staging}, then
the embedding can compile object programs to the metalanguage and avoid the
overhead of interpreting them on the fly \citep{WalidICFP02}.  A staged
definitional interpreter is thus a promising way to build a domain\hyp specific
language.

\begin{figure}
    \begin{floatrule}
    \begin{proofrules}
        \[ \[ [x:t_1] \proofoverdots e:t_2 \] \justifies \fun{x}e:t_1\to t_2 \]
        \[ \[ [f:t_1\to t_2] \proofoverdots e:t_1\to t_2 \] \justifies \fix{f}e:t_1\to t_2 \]
        \[ e_1:t_1\to t_2 \quad e_2:t_1 \justifies e_1 e_2: t_2 \]
        \[ \text{$n$ is an integer} \justifies n:\ZZ \]
        \[ \text{$b$ is a Boolean} \justifies b:\BB \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1+e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \times e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \le e_2:\BB \]
        \[ e:\BB \quad e_1:t \quad e_2:t \justifies \cond{e}{e_1}{e_2}:t \]
    \end{proofrules}
    \end{floatrule}
    \caption{Our typed object language}
    \label{fig:object}
\end{figure}

We focus on embedding a typed object language into a typed metalanguage.
\ccshan{Collect reasons to type here.}




We have programmed our interpreters in OCaml/MetaOCaml (\cite{metaocaml})
and Haskell. The complete code is available 
\footnote{the URL will appear in the final version of the paper.}
% connot give URL in the ICFP paper due to double-blind reviews
in the supplemental material to the paper. We will use both
(Meta)OCaml and Haskell for our examples throughout the paper.
Each language has its strength and weaknesses; this dual implementation
forced us to clarify our own ideas.  We will continually leverage the
respective strengths of these languages, as certain of our claims are
patently obvious in one implementation, while requiring more argumentation
in the other.

\oleg{should say somewhere in the Introduction that we are interested
  in the typed object language embedded in the typed
  meta-language. Refer to Sec 2 (see the remark below) why this is
  good. We assume the DSL setting: we enter EDSL terms in the
  meta-language program (e.g., GHCi or (Meta)OCaml top-level). A
  different approach is reading EDSL terms from a file. In that case, we
  need to typecheck them first. We do not deal with that problem here,
  because it has been solved. First we mention Walid02 where they use
  dependent typed. However, these don't seem to be needed: refer to
  `typing dependent typing', and the related work section.}
\jacques{I think it is important to emphasize, probably later in the 
paper, that we could write a camlp4 extension (or simply a parser in
Haskell) that would transform a nice syntax for a lambda calculus into
syntactic terms for what we present here.  Even the self-interpreter
terms could be done this way, which would simplify things tremendously.}
\jacques{I quite agree that the point that needs to be revisited is
  that a \emph{direct} interpreter for a (typed) language "cannot" be
  done. see discussion in related work about the direct interpreter,
  with regards to Walid's paper. Perhaps bring some of it here?}
\jacques{I have looked at a fair bit of the literature, and I have not
  seen this done, especially not in the typed 'online' setting.  The
  one question that seems to be open is, how accurate can we make the
  PE without a tough BTA?}
Should we mention some of these points in our contributions subsection
below?

    At POPL 2003, Taha et al~\cite{TahaPOPL03} and Xi et al~\cite{XiPOPL03}
both
interpreter of a typed language in a typed language is an interesting
problem. The known solutions include the use of the Universal type, or
GADTs. The former is not quite satisfactory for performance reasons;
and also because of the presence of partial pattern matching in the
interpreter; the inexhaustiveness of the pattern matching should never
be triggered when executing a well-typed term. Alas, this fact cannot
be made obvious to the compiler. That also exlains unoptimality.
\oleg{Should say somewhere here that our code is surprisingly simple,
  and, in the hindsight, obvious. The problem of typed direct
  intrepreter of a typed language is difficult and thought to require
  GADT or dependent types. 
  In fact, several papers introducing GADTs (Xi03 and SPJ) as well as 
  (language in Walid03) and Concoqtion, written 2003-2007, used the 
  typed tagless intrepreter as the motivating example.
  Taha (in Jones optimality paper, I think) said that typed
  interpreter of the typed language was not solved then.
  Or, should we refer to Sec `Other solutions'}



\subsection{The Tag Problem}\label{tagproblem}

%\oleg{see tagless\_interp1.ml, module Tagfull for the complete code.
%  If you change the code in here, please adjust the .ml file
%  accordingly. Let the paper and the accompanying code be in sync.}

Consider a language with abstraction, application and boolean
literals with de Bruin indices as variables. This is the language used
by \citet{WalidICFP02}, from which we borrow the first part of 
this example.

The language can be described by the following grammar (using OCaml
syntax)


\begin{code}
  type var = VZ | VS of var
  type exp = V of var | B of bool | A of exp * exp | L of exp
\end{code}

with a sample term (application of the identity function to a boolean) as
\begin{code}
  let test1 = A ((L (V VZ)),(B true))
\end{code}

Let us attempt to implement an the interpreter, the function |eval0|. It takes
an object language term such as |test1| and gives us its value.
The first argument to |eval| is the environment, initially empty,
which is the sequence of values associated with free variables in the
interpreted code.
\begin{code}
  let rec eval0 env = function 
  | V VZ      -> List.hd env
  | V (VS v)  -> eval0 (List.tl env) (V v)
  | B b       -> b 
  | L e       -> fun x -> eval0 (x::env) e
  | A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{code}

Suppose our meta-language (the language in which we wrote |test1|
and that interpreter) were untyped. The above code would be acceptable.
The |L e| line exhibits interpretive overhead \cite{WalidICFP02}: 
|eval0 env' e| will be executed every time (the result of
evaluating) |L e| is applied. Staging can be used to remove this
interpretive overhead (sections 1.1 and 1.2 of \cite{WalidICFP02} explain
this well). 

If we use OCaml or some other typed language as the meta-language, 
the function |eval0| is ill-typed. The line |B b|
says that |eval0| returns a boolean, whereas the next line says
the result is a function. All branches of a pattern-match form must
yield values of the same type, however. 
A related problem is the type of the environment |env|: since variables
in the language may be either of boolean or function types, a regular
OCaml list cannot be used for holding the corresponding values. 

The usual solution is to introduce a Universal type (section 1.3 of
\cite{WalidICFP02}) which contains both of these:

\begin{code}
  type u = UB of bool | UA of (u -> u)
\end{code}

We can then write an interpreter which is accepted by the typecheker
\begin{code}
let rec eval env = function
  | V VZ -> List.hd env
  | V (VS v) -> eval (List.tl env) (V v)
  | B b -> UB b
  | L e -> UA (fun x -> eval (x::env) e)
  | A (e1,e2) -> match eval env e1 with UA f -> f (eval env e2)
\end{code}
and whose inferred type is |u list -> exp -> u|. Now we can evaluate
our sample |test1| term:

\begin{code}
let test1r = eval [] test1
val test1r : u = UB true 
\end{code}

Unfortunately, the result is tagged: |UB true| rather than just
|true|. We also note that the eval function is partial in two
respects: first there is an inexhaustive pattern-matching 
|match ... with UA f -> ...| in the 
line |A (e1,e2)|.
Second, the occurrences of |List.hd| and |List.tl|.
The failure of the latter corresponds to the evaluation of `open'
terms. One can see that if all our terms are closed, |List.hd| and
|List.tl| above will never be applied to the empty list. The partial
pattern match in the |A (e1,e2)| line raises an exception when we try to
evaluate something like |A (B true, B false)|, that is when the first
operand is not a function. 

Suppose our object language were typed too. Then terms like
|A (B true, B false)| would be ill-typed. Thus, if our evaluator
accepted only typed terms, the missing alternatives in the |A|
line would not occur. Alas, what is obvious to us is not obvious to
the meta-language. There is still a pattern match, and we still have
to attach and eliminate the UB and UA tags, even when there is no
longer a need for them. See \cite{WalidICFP02}, Section 1.4 for more details.

In short: It is not satisfactory
to use a Universal type because tagging hurts performance and
necessitates partial pattern matching in the metalanguage, even for a
well-typed object term.  

\subsection{Other solutions}

But how should one design a type system to type
terms like |test1|? GADTs seem necessary. Similarly, how does one design a
typesystem that can keep track of the open/closed status of terms, so that we
can statically insure that only closed terms are passed to |eval| and that
the empty list checking in |List.hd| and |List.tl| can be eliminated.
The latter is an even more interesting problem requiring even
more advanced type systems 
(\cite{WalidPOPLo3,NanevskiICFP02,NanevskiJFP05,DaviesJACM01}
\jacques{I hope that is the right Davies paper}

This is also the main motivating problem (albeit in the context of
a first-order language) for the introduction of GADTs 
into Haskell \cite{PeytonJonesICFP2006}.  This was also a motivating
factor behind the work of Xi on GADTs \cite{XiGADTs}, and some of the
recent work on dependent types \cite{WalidICFP02,WalidConcoqtion}.

See the Related Work section (\ref{related}) for a discussion of the other
solutions to this problem.
\oleg{I feel we need to give some details of the other solutions here, 
to give the reader the feeling of
how advanced solutions were proposed and how rarely they are
implemented.}
\jacques{I have rearranged this section so that it should be clear how
to extend it to explain these other solutions}

\subsection{The ``Final'' Proposal}\label{ourapproach}

We have realized that if we change the encoding of the term from data
constructors to functions, we can encode a tagless interpreter for a
typed object language into a metalanguage with a very simple type system.

Now, let us define our object language (embedded DSL) in a different
way. Instead of using algebraic datatypes to encode the terms of the
language in the meta-language (reminiscent to the \emph{initial algebra} 
approach), we will be using functions as term constructors (the \emph{final}
approach):

\begin{code}
  let b (bv:bool) env  = bv
  let varZ env         = fst env
  let varS vp env      = vp (snd env)
  let app e1 e2 env    = (e1 env) (e2 env)
  let lam e env        = fun x -> e (x,env)
\end{code}

\noindent That is all of the interpreter. Our sample term now reads
\begin{code}
  let testf1 = app (lam varZ) (b true)
\end{code}
which is almost the same as before, only written with lower-case
identifiers. To evaluate a term, we merely need to apply 
the term to the empty environment

\begin{code}
  let testf1r = testf1 ()
  val testf1r : bool = true
\end{code}

The result now has no tags. The above functions |b|,
|varZ|, |app| constitute the evaluator. It is patent that the
interpreter has no tags and no pattern matching. The term |b true| is
evaluated to a boolean and a term |lam varZ| is evaluated to a
function, with no tags attached. The application rule applies that
function without any need for pattern-matching. The evaluator has
another feature: evaluation of open terms leads to a type error rather
than a run-time error. Indeed, 

\begin{code}
  let testf1bad = app (lam (varS varZ)) (b true)
  let testf1badr = testf1bad ()
\end{code}
gives a type error saying 
that the initial environment should be a tuple rather than () --
meaning the term requires a non-empty initial environment: the term is
open.

Thus all the errors that were previously reported at run-time (open terms,
applications of non-functions) are now reported at compile
time. Running of the interpreter will never result in run-time errors
because its code uses no operations that could possibly raise such
an error. In fact, our interpreter is total.

Furthermore, it should be clear that we do not use any
pattern-matching at all, and thus cannot have any pattern-matching
errors.  This is clear not just to us, but also to the compiler.
In other words in traditional approaches one can prove a meta-theorem
that all pattern-matching is total, but this is nevertheless not
apparent to the compiler, which thus still has to generate code for
those cases.  This seriously hurts efficiency~\cite{WalidICFP02}.

In the above code, the interpreter is wired in, directly built into the
functions |b|, |app|, etc.  In the remainder of this paper, 
we explain how to abstract the interpreter,
to make it possible to ``evaluate'' the same term in many different
ways: interpretation, compilation, partial evaluation, size
computation, etc.

\subsection{Contributions}\label{contributions}

   We have realized that if we change the encoding of the term --
from data constructors to functions, we can encode a tagless
interpreter of a typed language in a typed language with no GADTs and
a very simple type system. The interpreter uses no universal type, no
pattern matching and it is patently obvious to the compiler that the
only non-termination can come from interpreting the fixpoint
combinator |fix|. If the source
language is strongly normalizing, our interpreter is total.

The term ``constructor'' functions lam, app, etc. look like free
variables in the encoding of an object term.  Defining these functions
differently gives rise to different interpreters.  Given the same term
representation but varying the interpreters, we can
	- evaluate the term to a value in the metalanguage
	- determine the size or depth of a term
    - compute a \emph{CBN} CPS transformation of a term
	- `compile' the term. This requires staging, as in MetaOCaml.
	- Surprisingly, it is also possible to write a partial evaluator! 

\noindent We have implemented the above in (Meta)OCaml and Haskell.
We view our contributions as:
\begin{enumerate}
\item we interpret/compile/partially-evaluate a typed source language
   from within a typed language.  The requirements of the host type
   system are \jacques{not so onerous?  simple?},
\item a simple, clean implementation,
\item a nicely comparable Haskell/MetaOCaml version,
\item a functorial approach which deals with uniformly with all
evaluators, from interpretation to partial evaluation to CBN CPS transform,
\item clear and easy extensibility to more features,
\item only well-typed and closed terms can be evaluated, and this is 
checked statically,
\item and an approach to self-interpretation which is compatible with the
  above.  Self-interpretation turned out to be considerably harder than
  we at first expected.
\end{enumerate}

We want to reiterate that it was previously thought that insuring
that types are well-typed \emph{or} closed required advanced type
extensions, and we show that this can be done in a simple subset
of both OCaml and Haskell.

\jacques{can we state fairly clearly what type system we need?  I mean,
Haskell98(?) and MetaOCaml with (objects, polymorphic variants, ...).
With our let-polymorphism trick for self-interpretation, what do we really
need?}

The structure of our paper is as follows. Section XXX ...
We finally conclude.

\section{Base language and its tagless interpreters}\label{language}

The language we will embed is a simply-typed lambda-calculus
with fixpoint, integers, booleans and comparison, whose BNF grammar is

\begin{code}
  e ::= Lam hoas_fn | App e e | Fix hoas_fn |
  I int_literal | B bool_literal | Add e1 e2 | 
  Mul e1 e2 | Leq e1 e2 | IF b e-then e-else
\end{code}

This time we use higher-order abstract syntax \cite{DaleMiller}
rather than de Bruijn indices.  The
language is close to the language of \cite{XiPOPL03}, without the polymorphic
lift but with more constants so we can write better examples, enough to
express Fibonacci, factorial, and power.

\subsection{How to make encoding flexible: abstract the interpreter}
\label{encoding}
We have embedded the above language in (Meta)OCaml and Haskell.  In Haskell,
the constructors of DSL terms are functions defined as methods in a type class
Symantics. The name implies that the class interface gives the syntax for
the source language and its instances give the semantics.

\begin{code}
class Symantics repr where
  int :: Int   -> repr Int          -- int literal
  bool :: Bool -> repr Bool         -- bool literal

  lam :: (repr a -> repr b) -> repr (a->b)
  app :: repr (a->b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int  -> repr Int -> repr Int
  mul :: repr Int  -> repr Int -> repr Int
  if_ :: repr Bool -> repr a   -> repr a -> repr a
  leq :: repr Int  -> repr Int -> repr Bool
\end{code}

\noindent |test1| now reads |app (lam (\x -> x)) (bool True)|,
whose inferred type is |Symantics repr => repr Bool|.
For example, we can encode the classical |power| function, as well
as the partial application |\x -> power x 7| as

\begin{code}
testpowfix () = 
  lam (\x -> fix (\self -> lam (\n ->
        if_ (leq n (int 0)) (int 1)
            (mul x (app self (add n (int (-1))))))))
testpowfix7 () = 
  lam (\x -> app (app (testpowfix ()) x) (int 7))
\end{code}
The dummy unit |()| argument above is to avoid the monomorphism
restriction and so that the type of |testpowfix| and |testpowfix7|
will remain polymorphic in |repr|. Instead of supplying this dummy
argument, we could have given the terms explicit polymorphic
signature.  We however prefer if the typechecker of the meta-language
(Haskell) infers the types of the object terms for us. We could also
avoid the |()| argument by switching off the monomophism restriction
with a compiler flag.

Note that the object language is typed. If we make a mistake
(for example, replace |int 7| with |bool True| in |testpowfix7|, the
compiler will highlight that term and complain that it couldn't match
the expected |Int| against the inferred |Bool|. A type error
also results when entering |lam (\x -> app x x)|, with the compiler
complaining about an infinite type. The type-checker of the
meta-language will flag meaningless object level terms --
before any of their evaluation. If we forget, for example, |app| in
some of the terms above, we get an error message with a precise
location. So the type-checker also flags syntactically invalid
object-level terms.\oleg{Should we perhaps move this up, in the
  intro? To justify why typed-metalanguages and the typed DSL are
  good. Or keep the explanation here but refer to it from the Introduction.}

We have also embedded our DSL in (Meta)OCaml, replacing type classes
and instances with signatures and structures. For this and the 
following sections, the following simple signature suffices:

\begin{code}
module type Symantics = sig
  type ('c,'dv) repr
  val int : int  -> ('c,int) repr
  val bool: bool -> ('c,bool) repr
  val add : ('c,int) repr-> ('c,int) repr-> ('c,int) repr
  val mul : ('c,int) repr-> ('c,int) repr-> ('c,int) repr
  val leq : ('c,int) repr-> ('c,int) repr-> ('c,bool) repr
  val if_ : ('c,bool) repr ->
             (unit -> ('c,'da) repr) ->
             (unit -> ('c,'da) repr) -> ('c,'da) repr 
  val lam : (('c,'da) repr -> ('c,'db) repr) 
          -> ('c,'da->'db) repr
  val app : ('c,'da->'db) repr
    -> ('c,'da) repr -> ('c,'db) repr
  val fix : (('c,'da->'db) repr -> ('c,'da->'db) repr) 
            -> ('c,'da->'db) repr
end;;
\end{code}

Some differences are in the additional type parameter |c| (the
environment classifier from \cite{WalidPOPL03}) to be used later when we get to
code generation (this is required by MetaOCaml as part of the type for the
code). MetaOCaml is a call-by-value language, and thus we have to 
$\eta$-expand
the fix-point combinator and encapsulate the branches of the |if_|
into thunks to prevent their evaluation. 

The power example and our sample term |test1| are encoded as
\begin{code}
module EX(S: Symantics) = struct
 open S
 let test1 () = app (lam (fun x -> x)) (bool true)
 let testpowfix () = 
   lam (fun x ->fix (fun self -> lam (fun n ->
     if_ (leq n (int 0)) (fun () -> int 1)
         (fun () -> mul x (app self 
                               (add n (int (-1))))))))
 let testpowfix7 = 
    lam (fun x -> app (app (testpowfix ()) x) (int 7))
end;;
\end{code}
The dummy unit argument in |testpowfix| is an artefact of MetaOCaml
(which we will be using to compile our tests in Sections below),
again related to monorphism. To be
more precise, the environment qualifier (which appears as the |'c|
type parameter in the signature of Symantic functions) must remain
uninstantiated in order for us to `run' the code. That means the type
must be polymorhic. The value restriction then dictates that
the definitions of our object terms must look syntactically like
values. Alternatively to giving the |()| argument, we could have used
the rank-2 record types of OCaml to maintain the necessary polymorphism.

Thus the object expressions (expressions of the embedded DSL) have in
OCaml the type of a functor from |Symantics| to the desired value. This
is essentially the same as the |Symantics repr => | constraint in the case of
the embedding of the DSL in Haskell.

\subsection{Two tagless interpreters}
The first instantiation of the Symantics structure is an interpreter:
evaluating the object (DSL) term to its value in the meta-language. 
The following module essentially interprets (gives one-to-one
correspondence to) each operation of the object language as the
corresponding operation in the meta-language.
\begin{code}
module R  = struct
  type ('c,'dv) repr = 'dv (* absolutely no wrappers *)
  let int  (x:int)  = x
  let bool (b:bool) = b
  let add  e1 e2    = e1 + e2
  let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then (et ()) else (ee ())

  let lam f         = f
  let app e1 e2     = e1 e2
  let fix f         = let rec self n = f self n in self
end;;
\end{code}

This interpreter is patently tagless, using neither Universal type nor
any pattern-matching: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. We can run our
examples by instatiating the |EX| functor with |R|.
\noindent
\begin{code}
  module EXR = EX(R);;
\end{code}

\noindent and so |EXR.test1 ()| evaluates to the untagged boolean value |true|.
In Haskell, we define a\\
|newtype R a = R{unR::a}| and |instance Symantics R|
with a straightforward implementation. We must stress that |R| is
a |newtype|. Although it may look like a tag, it has no run-time
representation (erased during compilation) and pattern-matching
against |R| cannot ever fail and so is operationally an identity and
compiled that way. In both cases, the absense of pattern-match
failures is obvious to the compiler, because there are no
pattern-matching. The failure to yield a value
may only result from interpreting |fix|.

\begin{prop}
If the term |e| encoded in the meta-language has type
|t|, evaluation of that term in the interpreter R will either continue
indefinitely or termninate with a value of the same type |t|.
\end{prop}

We'd like show a different interpreter: it interprets each terms of
the object language to its size (defined as the number of term
constructors). The following is slightly abbreviated code (see the
accompanying source code for the complete definition).
 
\begin{code}
module L = struct
  type ('c,'dv) repr = int
  let int (x:int)  = 1
  let mul e1 e2    = e1 + e2 + 1
  let if_ eb et ee = eb + et () + ee () + 1
  let lam f        = (f 0) + 1
  let app e1 e2    = e1 + e2 + 1
  let fix f        = (f 0) + 1
end;;
\end{code}

\noindent Now the expression
\begin{code}
  let module E = EX(L) in E.test1 ()
\end{code}
evaluates to |3|. This interpreter is not only tagless, it is also
total. It ``evaluates'' even seemingly divergent terms like\\
|app (fix (fun self -> self)) (int 1)|.

\begin{comment}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{comment}

\section{Tagless compiler (aka, staged interpreter)}\label{compiler}

We can also write a ``compiler'' from our lambda calculus directly
into OCaml code, using MetaOCaml's staging facilities. Namely,
future-stage expressions of the type |t| are represented at the
present stage as values of the type |('a,t) code| where |'a| is the
environment classifier \cite{WalidPOPL03,WalidESOP04}. Code values are created
by a \emph{bracket} form: |.<e>.| specifies the expression |e| is to be
evaluated at a future stage. The \emph{escape} |.~e|, which must occur
within a bracket, specifies that the expression |e| must be evaluated
at the current stage; its result, which must be a code value, is
spliced in into the code being built. The \emph{run} form |.! e|
evaluates the code value, which involves run-time compilation and
linking of the future-stage code. The bracket, escape and run are akin to
quasi-quotation, unquotation and |eval| of Lisp.


% this code does not have the 'sv parameter. It shows up later.
\begin{code}
module C = struct
  type ('c,'dv) repr = ('c,'dv) code
  let int (x:int)   = .<x>.
  let bool (b:bool) = .<b>.
  let add e1 e2     = .<.~e1 + .~e2>.
  let mul e1 e2     = .<.~e1 * .~e2>.
  let leq x y       = .< .~x <= .~y >.
  let if_ eb et ee = 
    .<if .~eb then .~(et () ) else .~(ee () )>.
  let lam f         = .<fun x -> .~(f .<x>.)>.
  let app e1 e2     = .<.~e1 .~e2>.
  let fix f = 
     .<let rec self n = .~(f .<self>.) n in self>.
end;;
\end{code}
This is a completely straightforward staging of
|module R|.
This compiler attempts no optimizations at all, it simply produces
residual code. For example, the evaluation of our |test1|
\begin{code}
  let module E = EX(C) in E.test1 ();;
\end{code}
gives a value with inferred type |('a, bool) C.repr| which looks
like |.<((fun x_6 -> x_6) (true))>.|. MetaOCaml can print the code
values, that is, the corresponding future-stage code.

It is readily obvious that this compiler does not suffer
any interpretive overhead whatsoever. Indeed, the
  code produced for a function is simply |fun x_6 -> x_6| and has no 
  embedded calls to the evaluator: cf. the evaluator of DSL functions
  in Section~\ref{tagproblem}, the L-line.
The resulting code obviously contains no tags and no pattern-matching.

We have also implemented this ``compiler'' in Haskell. Since Haskell
has no (convenient, typed) staging facilities, we had to emulate
them. To be precise, we defined a data type |ByteCode| with
constructors |Var|, |Lam|, |App|, |Fix|, |INT| etc. with the obvious
meaning. Unlike our staging language (which is based on higher-order
abstract syntax), our bytecode, to be realistic, uses integer-named
variables. We then define 
\begin{code}
  newtype C t = C (Int -> (ByteCode t, Int)) 
\end{code}
where |Int| is the counter for the creation of fresh variable
names. We define the compiler by making |C| to be an instance of the
class |Symantics|. The implementation\footnote{The implementation uses
GADTs because we also wanted to write a typed interpreter for 
the \textsf{ByteCode} \emph{datatype}.} is quite similar (but slightly more
verbose) than the corresponding MetaOCaml code given above. The
accompanying code gives the full details.

\section{Tagless partial evaluator}\label{PE}

Surprisingly, it is also possible to write a partial evaluator!  The
partial evaluator does have tags: \emph{binding time} or \emph{phase}
tags, i.e. whether a value is static or dynamic.
Pattern-matching on this phase tag is always exhaustive; we still
have no universal type and no tags for object types.  The partial
evaluator uses the previously mentioned compiler to ``interpret'' dynamic
terms, and uses the interpreter to reduce static terms.  In this sense, 
the partial evaluator is a modular extension to both the compiler and
the interpreter. 

\subsection{The trouble with (opaque) functions}
\label{S:PE-problem}
We want to combine the interpreter |R| and the compiler |C| to produce
an (online) partial evaluator.  We defer the discussion of the 
choice of online/offline partial evaluation until after we have presented our
solution in~\ref{S:PE-solution}.
The partial evaluator (PE) is too an interpreter: it
evaluates a term either to an immediate (or, `static' or present-stage)
value or to a `dynamic', future-stage value, or `code' (of the sort
produced by the compiler C). The interpreter attempts to perform all
possible computations (i.e., computations whose operarands are
present-stage values) as the present-stage. Our first attempt, in
Haskell, is straightforward: we introduce
\begin{code}
data P1 t = S1 (R t) (C t) | E1 (C t)
\end{code}
as a `direct sum' of the interpreter |R| and the compiler |C|. The
datatype |P1| has two \emph{phase} (not type!) tags |S1| 
(for static) and |E1| for dynamic. The static value has both the
present-stage and future-stage alternatives. The dynamic value does
not have the present-stage component. We define
\begin{code}
abstr1 :: P1 t -> C t
abstr1 (S1 _ dyn) = dyn
abstr1 (E1 dyn)   = dyn
\end{code}
to extract the future-stage component. We then attempt to define the
interpreter |P1| compositionally -- and it works for the first-order
fragment of our DSL language.

\begin{code}
instance Symantics P1 where
    int  x  = S1 (int x) (int x)
    bool b  = S1 (bool b) (bool b)
    add (S1 n1 _) (S1 n2 _) = int (unR (add n1 n2))
    add e1 e2 = E1 (add (abstr1 e1) (abstr1 e2))
    -- mul and leq are analogous and elided
    if_ (S1 s _) et ef = if unR s then et else ef
    if_ eb et ef = 
       E1 (if_ (abstr1 eb) (abstr1 et) (abstr1 ef))
\end{code}
Integer and boolean literals are obviously immediate, present-stage
values. The result of adding two values is static only if both values
are. If so, we use the evaluator |R| to add the values; the result is
an essentially integer literal, which can be promoted to the future
stage. If at least one of the operands to |add| is dynamic, we promote
both operands to the future-stage and use the compiler |C| to
`compile' the code for adding the operand expressions.  Because the
static value includes as a component its future-stage variant, and
because we build both components together, we are able to get by
without the polymorphic `lift' operation (that is, lift the
computation of any type from the present-stage to the future
stage). We can do such a lifting only on type-by-type basis (for
booleans and for integers). The problem with the polymorphic lift is
that it inhibits true compilation, to assembler or other similar first
order target language with no polymorphic (run-time reified) types.
The representation of integers in the target language most likely
differs from the representation of booleans or functions,
etc. Therefore, without the intentional type analysis, the polymorphic
lift is not compilable.

When we come to the higher-order fragment of our language, however, we
stumble. Let us consider |lam f|. Here, |f| is a function from a
|P1 a| value to a |P1 b| value. The result of |lam f| must be either a
static value or a dynamic value -- but which? Unlike |add| or |if_|, 
|lam| cannot really examine its operand. The arguments of |add| are 
of type |P1 int|, which we can examine by pattern-matching and so 
determine if they are static or dynamic. The argument of |lam| is a 
function, which is not introspectible. One may only 
apply |f| to a particular static or dynamic value, and receive another 
value in return. Alas, as in the case |lam (\x -> mul x dynval)|, 
applying the same function to different static values may give 
either static or dynamic results (in the example above, applying 
the function to (int 0) gives the static value in return). One
solution is to be conservative and always use the |C| compiler to
compile |f| to the future-stage expression, which we can tag with
E1. But that defeats the purpose of partial evaluation as we may miss
many opportunities for static computations and simplifications, such
as multiplication by zero in the example above. A better solution is to
delay the binding-type-analysis (BTA) until the function |f| is
applied to a particular argument (that is, until the result of |lam f|
appears as the argument of |app|). That means, we have to find a way
to incorporate |f| as it is into the P1 data structure. We would like
to write that when |t| is a particular type |a->b|, that data type
should be 
\begin{code}
data P1 (a->b) = S1 (R (P1 a -> P1 b)) (C (a->b)) 
               | E1 (C (a->b))
\end{code}
That is, we need a non-parameteric data type, something like a
typecase (ref? Gibbons?).

The problem we encountered here is quite akin to typed CPS
transform. When transforming abstractions, we come across
non-functorialness. The connection with CPS is not an accident, as we
shall see below (in Section XXX).

\subsection{GADT -- not quite tagless PE}
\label{S:PE-GADT}

Two common ways of providing typecase in Haskell is by using either
generalized algebraic data types (GADT) or type-class functions
(implemented via typeclasses with functional dependencies). These
methods are equivalent; here we show the GADT; please see |incope1.hs|
in the accompanying source code for the latter.

We introduce the following GADT
\begin{code}
data P t where
    VI :: Int  -> P Int
    VB :: Bool -> P Bool
    VF :: (P a -> P b) -> P (a->b)
    E  :: C t -> P t
\end{code}
with four data constructors. The constructor |VF| stands out as its
operand is of the type |P a -> P b| rather than |a -> b| as one may
have expected if |P t| were parametric in |t|. As before, we define a
function to extract the future-stage computation from the |P t| value:
\begin{code}
abstr :: P t -> C t
abstr (VI i) = int i
abstr (VB b) = bool b
abstr (VF f) = lam (abstr . f . E)
abstr (E x)  = x
\end{code}

We use the method |lam| of the |C| intrepreter to compile the argument
of |lam| into the code value. We may now make |P| the instance of
Symantics and thus implement the partial evaluator (the
implementations of |mul|, |leq|, |if_| and |fix| are elided).
\begin{code}
instance Symantics P where
    int x  = VI x
    bool b = VB b

    add e1 e2 = case (e1,e2) of
                 (VI n1,VI n2) -> VI (n1 + n2)
                 _ -> E (add (abstr e1) (abstr e2))
    lam = VF
    app ef ea = case ef of
                        VF f -> f ea
                        E  f -> E (app f (abstr ea))
\end{code}

The implementations of |int|, |bool|, and |add| are similar to the
above. The interpretation of |lam f| merely incorporates the
HOAS function |f| into the result. At any point we can convert it to a
code value, if we must. But delaying such a conversion, we get the
opportunity to apply |f| to a concrete argument. When we evaluating
|app ef ea|, we check to see of the first argument is so-delayed
lambda-expression. If it is, we apply the incorporated |f| to the
concrete argument |ea|, giving us a chance to perform the static
computations (we see the example in the next subsection). If |ef| is a
dynamic value, we residualize.


Although GADT solve our problem, the solution is not quite
satisfactory. First of all, it cannot be ported to MetaOCaml as GADTs
are not supported in OCaml. Second, the tagging problem reappeared, in
the implementation of |app| above. The datatype |P t| has four data
constructors; the pattern-matching in |app| however used only two of
them, |VF| and |E|. It is obvious, one may say, that the constructors
|VI| and |VB| just cannot occur, because the corresponding values
cannot have the arrow type. What is obvious to us is not obvious to
the compiler. The case matching above does look \emph{partial}. The
point may loop tenuous -- and yet it is precisely the main point of
tag elimination. In a typed tagged interpreter there are many
pattern-matching forms that looks partial but are total in
reality. The goal is to make this totality \emph{syntactically}
apparent.

\subsection{The `final' solution}
\label{S:PE-solution}
Let us re-examone the problem in Section~\ref{S:PE-problem}. What we
would ideally like is to write
\begin{code}
data P t = S (repr_pe t) (C t) | E (C t)
\end{code}
where |repr_pe| is a type function, defined inductively
% inductively because P below depends on repr_pe
as 
\begin{code}
repr_pe Int    = Int
repr_pe Bool   = Bool
repr_pe (a->b) = P a -> P b
\end{code}
Although we can use typeclasses to define that type function
in Haskell, that is not portable to MetaOCaml. We notice however that
the type case alternatives above are all associated with the existing
methods of |Symantics| and so the case analysis is already present. 
A simple and more portable solution thus emerges: we bake |repr_pe| 
into the signature |Symantics|. Switching to MetaOCaml for
portability, we recall from Section~\ref{encoding} that |repr| type
had two arguments |type ('c,'dv) repr|. We add an extra argument,
|'sv|, which is the result of applying |repr_pe| function above to the
type |'dv|. The signature |Symantics| now reads
\begin{code}
module type Symantics = sig
  type ('c,'sv,'dv) repr
  val int  : int  -> ('c,int,int) repr
  val bool : bool -> ('c,bool,bool) repr
  val add  : ('c,int,int) repr -> 
      ('c,int,int) repr -> ('c,int,int) repr
  val mul  : ('c,int,int) repr -> 
      ('c,int,int) repr -> ('c,int,int) repr
  val leq  : ('c,int,int) repr -> 
      ('c,int,int) repr -> ('c,bool,bool) repr
  val if_  : ('c,bool,bool) repr ->
             (unit -> ('c,'sa,'da) repr) ->
             (unit -> ('c,'sa,'da) repr) -> 
             ('c,'sa,'da) repr 
  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr)
    -> ('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr),
           'da->'db) repr
  val app : ('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr),
             'da->'db) repr
    -> ('c,'sa,'da) repr -> ('c,'sb,'db) repr
  val fix : 
    (('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr) as 's,
         'da->'db) repr 
    -> ('c,'s,'da->'db) repr)  -> ('c,'s,'da->'db) repr
end;;
\end{code}
The interpreters |R|, |L| and |C| above only relied on the last
argument of the |repr| type, which is unchanged. And so is their
implementation; the newly added argument |'sv| will be phantom.
For example, we would now write the interpreter |C| as
\begin{code}
module C = struct
  type ('c,'sv,'dv) repr = ('c,'dv) code
  (* the rest is the same *)
\end{code}
When writing the partial evaluator however, we make use of the
type argument |'sv|. Here's the corresponding instance of the
Symantics:

\begin{code}
module P = struct
  type ('c,'sv,'dv) repr = 
        {st: 'sv option; dy: ('c,'dv) code}
  let abstr {dy = x} = x
  let pdyn x = {st = None; dy = x}

  let int  (x:int)  = {st = Some (R.int x); dy = C.int x}
  let bool (x:bool) = {st = Some (R.bool x); dy = C.bool x}

  let buildsimp cast e f1 f2 = fun e1 e2 -> 
     match (e1,e2) with
        | ({st = Some e'}, _) when e = e' -> e2
        | (_, {st = Some e'}) when e = e' -> e1
        | ({st=Some n1}, {st=Some n2})-> cast (f1 n1 n2)
        | _ -> pdyn (f2 (abstr e1) (abstr e2))

  let add e1 e2 = buildsimp int 0 R.add C.add e1 e2
  let mul e1 e2 = buildsimp int 1 R.mul C.mul e1 e2
  let leq e1 e2 = 
     match (e1,e2) with
         ({st=Some n1},{st=Some n2})-> bool (R.leq n1 n2)
        | _ -> pdyn (C.leq (abstr e1) (abstr e2))
  let if_ eb et ee = match eb with
           {st = Some b} -> if b then et () else ee ()
         | _ -> pdyn (C.if_ (abstr eb) 
                          (fun () -> abstr (et ()))
                          (fun () -> abstr (ee ())))
  let lam f = {st = Some f; 
               dy = C.lam (fun x -> abstr (f (pdyn x)))}
  let app ef ea = match ef with
        {st = Some f} -> f ea
      | _ -> pdyn (C.app (abstr ef) (abstr ea))

  let fix f = 
    let fdyn = C.fix (fun x -> abstr (f (pdyn x))) in
    let fdynn e = pdyn (C.app fdyn e.dy) in
    {st=Some (function {st = Some _} as e -> 
               let rec self n = 
                (match n.st with Some _->app (f (lam self)) n 
                                | _ -> fdynn n) in 
               app (f (lam self)) e
             | e  -> fdynn e);
    dy = fdyn }
end;;
\end{code}

The type |repr| here literally expresses the type equation at the
beginning of this section. The function |abstr|, as in the previous
subsection, extracts the future-stage computation from the result of
PE. The function |pdyn| does the converse. As in
section~\ref{S:PE-problem}, we build future-stage computations as we
build present-stage ones, if any. Therefore, we avoid the need for the
polymorphic `lift' function. The implementations of
|add| and |mul| are a bit more sophisticated, taking advantage of the
implicit monoid structure. The function |buildsimp| checks to see if
its two arguments |e1| and |e2| are static expressions. If so, the
corresponding static computation (with the help of the intrepreter
|R|) is performed. The function also checks if one argument is static
and is the neutral element of the monoid structure 
\jacques{Comment on the various nice generic things in the implementation,
which are a little silly in this small version, but would really pay off
in something bigger.  See the abstract interpretation hard at work!
See the monoid-driven simplifications!}
\oleg{I'm inclined to ditch buildsimp. We miss an opportunity here:
  when multiplying anything by 0, the result is zero!}

When doing PE on fix, we made a deliberate decision to `go all the
way' -- so long as the computation can proceed at the PE stage
(`static value'), we do that rather than residualizing. In the
accompanying source code, we have also implemented an alternative
where fix is unrolled only once and them we residualize. We prefer the
first approach, even though it (unlike the latter) may cause
non-termination at the PE stage -- but \emph{only} if the computation
we are partially evaluating is non-terminating and enough static
information is available to trigger this non-termination.  Our
treatment of fix may lead to the code bloat in the residual
program. That is an issue investigated elsewhere (cite our PEPM2006
paper).

Given the above Symantics, our running test now evaluates
\begin{code}
  let module E = EX(P) in E.test1 ();;
\end{code}
to
\begin{code}
- : ('a, bool, bool) P.repr = 
   {P.st = Some true; P.dy = .<(true)>.}
\end{code}
It is instructive to compare this result with that of the |C|
evaluator. We observe the beta-redex that was apparent there is now
performed. The result is the static value |true|.
More interesting is the partial evaluation of testpowfix7. The |C|
interpreter evaluates |testpowfix7| to the future-stage code with many
apparent beta-redexes.
\begin{code}
  let module E = EX(C) in E.testpowfix7;;
 - : ('_a, int -> int) C.repr =
.<fun x_1 ->
   (((fun x_2 ->
       let rec self_3 =
        fun n_4 ->
         ((fun x_5 -> if (x_5 <= 0) then 1 
                      else (x_2 * (self_3 (x_5 + (-1)))))
           n_4) in
       self_3) x_1) 7)>.
\end{code}
The result of the |P| interpreter is different:

\begin{code}
  let module E = EX(P) in E.testpowfix7;;
 - : ('_a, ('_a, int, int) P.repr -> ('_a, int, int) P.repr, 
      int -> int) P.repr = 
 {P.st = Some <fun>;
  P.dy = .<fun x_1 -> 
     (x_1 * (x_1 * (x_1 * (x_1 * (x_1 * (x_1 * x_1))))))>.}
\end{code}

which is what one would have expected from the power example.

Unlike the GADT approach in the previous subsection, there is no
longer any seemingly partial pattern-matching. The pattern-match only
occurs in PE (to check if the code is static or dynamic), it is only a
phase rather than a type tag match, and the pattern match is
\emph{patently} exhaustive. That is, totality is assured and it is
assured \emph{syntactically}, in the way that is apparent to the
compiler.

One should note that the partial evaluator has fully reused the code
of the compiler and the interpreter. It is truly the `composition' of the
interpreter and the compiler.  \jacques{Perhaps cite a paper of Thiemann
here, cogen in 6 lines?}

\jacques{Comment on online/offline here}


\section{Variations and discussion}\label{discussion}

state and imperative features

CPS transformation (interpreting/compiling the term to CBN CPS)

It's relation to PE and CPS. Currently, our CPS is CBN. Need to
investigate CBV.
The type-level function |'da -> 'sa| is important because it gives 
the CPS transform. So, the type-function
that enables the PE is exactly the same type function that enables
CPS....



The PE in incope is really a mixture of online and off-line; it looks 
online, but it has a sort of "just in time" binding-time analysis built in.

If we just add a method 'lift' to Symantics with a type
|lift :: a -> repr a| (cf polymorphic lift of Xi03, of MetaML and
MetaOCaml: CSP)
then |Symantics repr| implies |Functor repr|. We can write then
\begin{code}
  instance Semantics repr => Functor repr where
  -- fmap :: (a->b) -> repr a -> repr b
  fmap f ra = app (lift f) ra
\end{code}

\subsection{Translucent types}

We must formulate some propositions: a typed term makes progress in
any interpreter. The reason it is typed in any interpreter:
|forall r. Symantics r => r tau| directly says that in every model the
term is typed. 

The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
|app _add (app _int 1) (app _int 2)| where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is

$$ 
  (\ZZ \rightarrow r \ZZ) \rightarrow
  (\BB \rightarrow r \BB) \rightarrow
  \forall \alpha \beta. (r \alpha \rightarrow r \beta)
      \rightarrow r (\alpha\rightarrow\beta) \rightarrow ... r \tau
$$
% (Int -> r Int) ->
% (Bool -> r Bool) ->
% (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau

or, if we denote the sequence of initial arguments as S r, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.

The attempt to encode self-interpreter brings the |*->*| polymorphism
the the forefront, as we shall now see.

\section{Self-interpretation}\cite{selfinterp}

Taha et al (cite different paper) define a self-interpreter si to be an
object-language term such that, for any object-language term e,

\begin{code}
  si "e"    is observationally equivalent to    e
\end{code}

where |"e"| is an encoding of |e|.  Under the co-approach, we want to encode
each object-language term as a function from a record of cogen methods
to a generated rep.  (Thanks to currying, the object language need not
actually support records.)  In the case of a self-interpreter, that
record is simply si!  So we should define a self-interpreter si to be an
object-language record-term such that, for any object-language term e,

\begin{code}
  "e" si    is observationally equivalent to    e
\end{code}

The self-interpreter then is just like the interpreter
|R| in Section 2, except written at the object rather than meta level.

Note on terms and special forms. In our class Symantics, |add|
is not a term -- because it does not have the type repr t. We could
have introduced |add| to be of the type |repr (Int->Int->Int)| -- but we
didn't. So, |add| is a special form. It is to be applied in
meta-language (using the Haskell application -- white space) rather
than in the object language (app). 

Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).


Self-interpretation in our framework is
a bit tricky due to a need for polymorphism.  An encoded term should
be polymorphic in the interpreter type-constructor "r".  Further, the
functions "lam", "app", etc. above should be polymorphic in the object
types "a" and "b".  Thus the object language seems to require rank-2
polymorphism; and so type annotations will be inevitable.


To neatly bypass the need for rank-2 polymorphism, we use the notion
of let-polymorphism and a `hole', as follows.  A partial evaluator (or
a compiler or interpreter) is an evaluation context that (let-)binds
variables named "app", "lam", etc.  For the object language to
take advantage of let-bound polymorphism in the metalanguage, it
is crucial that we encode object "let" to meta "let".  

We also (unfortunately need to) define a self-interpreter slightly
differently.  A self-interpreter |SI[]| is an object expression with a
hole |[]| waiting to be plugged with the encoding of an object expression.
The self-interpreter binds the free variables |_lam|, |_app|, |_int|, |_bool|,
etc. in the plugged encoding.  We require |SI["E"]| to be equivalent to |E|,
where |"E"| is the object encoding of the object expression |E|.

This strategy of course limits what kinds of processing on encoded
object expressions can be expressed in the object language.  For
example, unlike if we had simply defined |"E"| to be the `identity
encoding' E, we \emph{can} write a size-measurer |SZ[]|, which like |SI[]| is
an object expression with a hole |[]| waiting to be plugged with |"E"|.
However, this notion of size cannot distinguish |let x = E1 in E2[x]|
from |"E2[E1]"|.  Cf. one way to type-check polymorphic let:
\begin{code}
        E1:T    E2[E1]:T'
    ------------------------
    let x = E1 in E2[x] : T'
\end{code}
That analogy shows that the self-interpreter with a hole is a
euphemism for a higher-rank abstraction. That makes it clear where
the difficulty lies, and how to simulate the higher-rank application
(that is, higher-rank arrow elimination) `by hand'.


We also need to duplicate the encoding of an object expression if we
want to process the same object expression in multiple ways (e.g., both
measure its size and interpret it).  But all this is consistent with (in
fact, amplifies) our `coalgebraic/final/cogen' spirit, so our revised
notion of self-interpretation (and, some might say, of size measurement)
may just squeak by the reader's skepticism as we explain our desire for
kind polymorphism.

I guess the let-and-hole argument works out if we don't typecheck the
encoded terms. We can't typecheck the interpreter with the whole
either. We insert the encoded term into the interpreter -- and then
typecheck the whole thing. That'll work. Yet a bit unsatisfactory...


We achieve
self-interpretation in that the following evaluation context in the
object language defines an interpreter for encoded object terms.
\begin{code}
  let lam = \f -> f in
  let app = \f x -> f x in
  let add = \m n -> m + n in
  let int = \i -> i in
  [hole]
\end{code}
The obvious notion of optimality in this setup is easy to achieve.  For
example, given the context above, our partial evaluator turns
\begin{code}
  app (lam (\x -> add x (int 1))) (int 3)
\end{code}
into the number 4, as desired.  We can also encode the self-interpreter
above as the following evaluation context in the object language.
\begin{code}
  let lam = lam (\f -> f) in
  let app = lam (\f -> lam (\x -> app f x)) in
  let add = lam (\m -> lam (\n -> add m n)) in
  let int = lam (\i -> i) in
  [hole]
\end{code}
The "let" and the hole "[hole]" are meta-constructions, which must map to
themselves in any interpretation.  

The 'result' of applying an interpreter to itself, needs to be an 
interpreter too!  In fact, if it is Jones-optimal, then you should be 
able to apply it to itself again and get the same answer.  
Our code does that. First, |twice_inc_3_e| is the
metalanguage encoding of the object term |si["e"]|
where si is the self-interpreter of the object language (actually, only
lam,app,add,int), |""| means the object-to-object encoding, and e is the
object term
\begin{code}
  let twice_ f x = f (f x) in let inc_ n = n + 1 in twice_ inc_ 3
\end{code}

Second, |twice_inc_3_ee| is the metalanguage encoding of the object term
|si["si[e]"]|
which is the same as
|si["si"["e"]]|
The partial evaluator yields 5 on both |twice_inc_3_e| and 
|twice_inc_3_ee|,
as desired.


``But how 
to you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?''
Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.
Again, what is different from the above is that we \emph{can}
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).



\section{Related work}\label{related}
Walid's Jones optimality paper. See his own ICFP02 paper that
described what he achieved then and what he \emph{did not}. He did not
eliminate all the tags. Ken's and mine impression of it:
(i) we can't write a \emph{direct} interpreter of the type language
           -- the only known to them, and \emph{partial} solution, was 
              that by Zhe Yang;
(ii) we have to use indirect interpreter, which needs the universal
           type and hence tagging;
(iii) that means any self-interpreter must have tags
(iv) that means not Jones-optimal.
It seems we should attack the point1 (in which case we don't have to
bother with self-interpreters). We say that we \emph{can} write the direct
interpreter, and without GADT or their equivalents (typeclasses).
Meta-language computes types, and the compiler and interpreter respect
those types and needs no tags.


Walid's ICFP02

Walid's PEPM07 (Concoqtion paper)? At leats at the Concoqtion
presentation at WG2.11, Emir used tagless interpreter as the main and
only example. \jacques{but at the PEPM07 presentation, Walid did not
quite do it that way.  He presented Concoqtion more as an exploration
of (an obvious but difficult) part of the design space for programming
languages.}

Danvy's self-interpreter
Ken has read Danvy et al's paper and said that the paper
doesn't seem to even make the claim that the source language is
typed. The interpreter is typed, but the source language doesn't seem
to be.
In sharp contrast to Danvy's paper, all the evaluators in our case
cannot produce any pattern-match error! Except for the partial
evaluator, there are NO patterns to match. And this is critical for
tag elimination -- pattern matching means there are no tags. And in
PE, the pattern-match deals with phases rather than with types. Also,
in our case the exhaustiveness of the pattern match is apparent to the
compiler -- that's why there are no tags.

Simon Peyton-Jones GADTs (the motivation for GADT is tagless
interpreter, but only for the first-order language)

Xi's POPL03 (He uses a higher-order language, one of the motivations
for his GRDT)

Asai; 
It [dynamic part when dealing with bool, int, lam in PE] is
Asai's trick for reducing the amount of reify/reflect back-and-forth
in a PE for an untyped language that allows you back-and-forth.  Or at
least that's how I read those papers.
Asai's approach is related to our PE (emulating typelevel functions):
Ken: ``In some sense we're just observing that Asai's code type-checks
-- in Hindley-Milner as soon as we deforest the static code
representation.''

Sumii et al.

"Boxes go Bananas" by Washburn and Weirich (intended or otherwise) Ken
wrote about this: ``but a glance reveals no type classes.  Do you mean
then that if we hide the definition |newtype R a = R a| from a client,
and export only the interpreter |instance Symantics R R|, then the
client would see R as an HOAS datatype a la Washburn and Weirich and
only be able to create parametric ("non-exotic") terms?''  Jacques
commented: ``I mean the HOAS parts, and how to deal with it (like in
the instance for |Symantics R R|).  There are other nice tricks for
dealing with HOAS in that paper worth a closer look.  It's all about
plain polymorphism, not type classes, but there are still some clever
ideas there, and they are about issues of tagging (an lack thereof).''

Typing Dep Typing plus two messages on Haskell-Cafe: typechecking 
from untyped to the typed form, ready to be interpreted. Walid's ICFP02
uses dependent types


Regarding our implementation of the type-level function |'sv -> 'dv|.
Gibbons et al paper on
the implementations of typecase may be relevant. I have not read it,
but I have the feeling some of their typecase designs may be
relevant (at least to cite). 
However, the typecase paper relies on having a 
bijection between types -- our |'sv| and |'dv| in this case, which is not 
our situation.  We have a one-sided view of things, where |'sv -> 'dv| is 
easy, and |'dv| "perfectly reflects" an |'sv|, but you still can't get at 
that |'sv|.  \jacques{Could we say that we have a meta-level Galois
injection?  We have one direction completely covered at the object
level (and so easy to lift), but 'dv to 'sv can only be done by
an outside mechanism}


Self-Interpretation and Reflection in a Statically Typed Language 
\url{http://webpages.cs.luc.edu/~laufer/papers/reflection93.pdf}
He uses the SK language and some typeclasses. Their interpreter is a
meta-circular interpreter -- rather than self-interpreter. We can
claim to have implemented a far better interpreter for a far richer
language.

Boxes go Bananas by Weirich and Washburn.

\section{Conclusions}\label{conclusion}

We were dissatisfied with current approaches to DSL embedding: either
the performance was hurt by tagging/untagging and other interpretive
overhead, or the offered solutions required too much expressive power
from the host language's type system.  By shifting from an initial
algebra approach to one which leverages the natural co-algebraic structure
of the $\lambda$-calculus, we show that both of these issues can be 
simultaneously dealt with.

% We show how to do many things without tags, in a simple type system, 
% essentially by avoiding any (inductive) types.  

\jacques{Should we mention that algebraic type tags give us a partly
intensional representation of terms.  This nicely allows pattern matching,
but as sits inside an extensional theory, ensuring that all traces 
of these tags disappear seems quite difficult without a very powerful
type system (enter GADTs).}

Our approach showed better the meaning of GADT and when they can be
avoided.
This shows also how a limited form of GADTs (the ones
used for the sake of type-dependent representations) can be
implemented with just functions. 

separate compilation: separate
typechecking of terms without knowing which particular
hole they will be plugged into. Formulate this as a prop?



\subsection*{Acknowledgments}
We would like to thank Martin Sulzmann and Walid Taha 
for helpful discussions.

\bibliographystyle{mcbride}
\bibliography{tagless}
\end{document}
