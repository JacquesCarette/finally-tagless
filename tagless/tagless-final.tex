\documentclass[preprint]{sigplanconf}
\usepackage{amsmath,amssymb}
\usepackage{natbib1}
\usepackage{comment}
\usepackage{url}

\newcommand{\jacques}[1]{{\it [Jacques says: #1]}}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}
\newcommand{\ccshan}[1]{{\it [Ken says: #1]}}

\usepackage[compact]{fancyvrb1}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=1em,fontsize=\small}

\newtheorem{prop}{Proposition}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\BB}{\mathbb{B}}

\begin{document}

\conferenceinfo{ICFP '07}{Freiburg, Germany} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Staged typed type-preserving tagless ``interpreters'', \emph{finally}}
\subtitle{Subtitle Text, if any}

\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}

\maketitle

\begin{abstract}
We have built a (family of) tagless partial evaluator(s) for a typed object
language \oleg{``,or, embedded DSL''}, placing minimal requirements on
the type system of the meta-language:
we need let-bound polymorphism and staging, but not dependent
types, generalized algebraic data types, nor a tag elimination 
postprocessing step.

The main tool is to take functions seriously: we encode the
higher-order abstract syntax of object terms not by using constructors for
an algebraic data type but rather by invoking cogen functions, which occur as
free variables in the encoded term.  In other words, we eschew initial
algebras for the wilder world of implicit representations of final algebras.
Our family of evaluators contains an interpreter, a compiler, a partial
evaluator and a term-size evaluator \oleg{and a CPS CBN transfromer?}
Our approach highlights the importance of order-2 type constructors
(order-2 polymorphism), which lets us encode DSL terms with
translucent types: enough abstraction is available to interpret the
terms in various ways, and yet enought type information is available 
to typecheck the terms and statically assure their interpretation will
never get stuck.
\oleg{say something about type functions, GADTs and the way to get
  around them?}
\end{abstract}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}\label{intro}

See Section 1 of Walid02. Our subject area is that of a definitional DSL
interpreter.
Define Staging, meta-language and object language. (Walid uses
`subject' language).

We have programmed out interpreters in OCaml/MetaOCaml (cite
www.metaocaml.org) and Haskell. The complete code is available 
\footnote{the URL will appear in the final version of the paper.}
% connot give URL in the ICFP paper due to double-blind reviews
in the supplemental material to the paper. We will use both
(Meta)OCaml and Haskell for our examples throughout the paper.
Each language has its strength and weaknesses; this dual implementation
forced us to clarify our own ideas.  We will continually leverage the
respective strengths of these languages, as certain of our claims are
patently obvious in one implementation, while requiring more argumentation
in the other.

\oleg{should say somewhere in the Introduction that we are interested
  in the typed subject language embedded in the typed
  meta-language. Refer to Sec 2 (see the remark below) why this is
  good. We assume the DSL setting: we enter EDSL terms in the
  meta-language program (e.g., GHCi or (Meta)OCaml top-level). A
  different approach is reading EDSL terms from a file. In that case, we
  need to typecheck them first. We do not deal with that problem here,
  because it has been solved. First we mention Walid02 where they use
  dependent typed. However, these don't seem to be needed: refer to
  `typing dependent typing', and the related work section.}
\jacques{I think it is important to emphasize, probably later in the 
paper, that we could write a camlp4 extension (or simply a parser in
Haskell) that would transform a nice syntax for a lambda calculus into
syntactic terms for what we present here.  Even the self-interpreter
terms could be done this way, which would simplify things tremendously.}
\jacques{I quite agree that the point that needs to be revisited is
  that a \emph{direct} interpreter for a (typed) language "cannot" be
  done. see discussion in related work about the direct interpreter,
  with regards to Walid's paper. Perhaps bring some of it here?}
\jacques{I have looked at a fair bit of the literature, and I have not
  seen this done, especially not in the typed 'online' setting.  The
  one question that seems to be open is, how accurate can we make the
  PE without a tough BTA?}
Should we mention some of these points in our contributions subsection
below?

        Taha et al and Xi et al (both POPL2003) argued that writing an
interpreter of a typed language in a typed language is an interesting
problem. The known solutions include the use of the Universal type, or
GADTs. The former is not quite satisfactory for performance reasons;
and also because of the presence of partial pattern matching in the
interpreter; the inexhaustiveness of the pattern matching should never
be triggered when executing a well-typed term. Alas, this fact cannot
be made obvious to the compiler. That also exlains unoptimality.


\subsection{The Tag Problem}\label{tagproblem}

\oleg{see tagless\_interp1.ml, module Tagfull for the complete code.
  If you change the code in here, please adjust the .ml file
  accordingly. Let the paper and the accompanying code be in sync.}


Consider the sample language of abstraction, application and boolean
literals with de Bruin indices as variables. This is the language used
in the body of Walid02, from which we borrow the first part of 
this example.

The language can be described by the following grammar (using OCaml
syntax)


\begin{code}
  type var = VZ | VS of var
  type exp = V of var | B of bool | A of exp * exp | L of exp
\end{code}

and a sample term is
\begin{code}
  let test1 = A ((L (V VZ)),(B true))
\end{code}

The first attempt at the interpreter, the function eval. It takes the
object language term such as |test1| and gives us its value.
The first argument to |eval| is the environment, initially empty,
which is the sequence of values associated with free variables in the
interpreted code.
\begin{code}
  let rec eval0 env = function 
  | V VZ -> List.hd env
  | V (VS v) -> eval0 (List.tl env) (V v)
  | B b -> b 
  | L e -> fun x -> eval0 (x::env) e
  | A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{code}

Suppose our meta-language (the language in which we wrote test1
and that interpreter) were untyped. The above code would be acceptable.
The |L e| line exhibits interpretative overhead (see descr in Walid02
paper): |eval0 env' e| will be executed every time the result of
evaluating |L e| is applied. Staging can be used to remove the
interpretative overhead. Cite some paper -- see Walid02, Sec 1.1 and
1.2. They say it well.

If we use OCaml or some other typed language as the meta-language, 
the function |eval0| is ill-typed. The line |B b|
tells that the function returns a boolean, whereas the next line says
the result of |eval0| is a function. Clearly, the result of the
function can't be both of these types. 
A related problem is the type of the env: since variables
in the language may be either of boolean or function types, a regular
OCaml list cannot be used for holding the corresponding values. 

The solution is to introduce a Universal type (see Walid02 Section
1.3) which contains both of these:

\begin{code}
  type u = UB of bool | UA of (u -> u)
\end{code}

We can then write the interpreter, which is accepted by the typecheker
\begin{code}
  let rec eval env = function
  | V VZ -> List.hd env
  | V (VS v) -> eval (List.tl env) (V v)
  | B b -> UB b
  | L e -> UA (fun x -> eval (x::env) e)
  | A (e1,e2) -> match eval env e1 with UA f -> f (eval env e2)
\end{code}
and whose inferred type is |u list -> exp -> u|. Now we can evaluate
our sample |test1| term:

\begin{code}
  let test1r = eval [] test1
  val test1r : u = UB true 
\end{code}

Unfortunately, the result is tagged: |UB true| rather than just
|true|. We also note that the eval function is partial in two
respects: first there is an inexhaustive pattern-matching in the 
line |A (e1,e2)|.
Second, the occurrences of |List.hd| and |List.tl|.
The failure of the latter corresponds to the evaluation of `open'
terms. One can see that if all our terms are closed, |List.hd| and
|List.tl| above will never be applied to the empty list. The partial
pattern match in the |A (e1,e2)| line raises an exception when we try to
evaluate something like |A (B true, B false)|, that is, the first
operand is not a function. 

Suppose our object language were typed, too. Then terms like
|A (B true, B false)| would be ill-typed. Thus, if our evaluator
accepted only typed terms, then the missing alternatives in the |A|
line would not occur. Alas, what is obvious to us is not obvious to
the meta-language. There is still a pattern match, and we still have
to attach and eliminate the UB and UA tags, even when there is no
longer a need for them. See Walid02, Sec 1.4 for more detail.

Of course there is also the issue how to design a type system to type
terms like |test1| (GADT seem necessary). Also, how to design the
typesystem to keep track of open/closed status of terms, so we can
statically assure that only closed terms are passed to eval and that
the empty list checking in |List.hd| and |List.tl| can be eliminated
as well. The latter is even more interesting problem requiring even
more advanced type systems (cite env classifiers, Nanevski, Rowan
Davies, Pitts).

The problem is also outlined in Simon Peyton-Jones GADT paper, but
only in the context of a first-order language

In short: \oleg{ (put into really introduction or the conclusions?) }
It is not satisfactory
to use the Universal type because tagging hurts performance and
necessitates partial pattern matching in the metalanguage, even for a
well-typed object term.  


% \subsection{Other solutions}
% The tag problem motivated GADTs (SPJ, Xi) and dependent types
% (WalidICFP02, Concoqtion). Or, perhaps just refer to related work?
% But we need to say something here, to give the reader the feeling of
% how advanced solutions were proposed and how rarely they are implemented.

See the Related Work section (\ref{related}) for a discussion of the other
solutions to this problem.
\oleg{I feel we need to give some details of the other solutions here, 
to give the reader the feeling of
how advanced solutions were proposed and how rarely they are
implemented.}


\subsection{The `Final' Proposal}\label{ourapproach}

We have realized that if we change the encoding of the term from data
constructors to functions, we can encode a tagless interpreter for a
typed object language into a metalanguage with a very simple type system.

\oleg{see tagless\_interp1.ml, module Tagless for the complete code.
If you change the code in here, please adjust the .ml file
accordingly. Let the paper and the accompanying code be in sync.}

\jacques{do we want to start using a lot of little files and
use includes to insure the code stays in synch?}
\oleg{Because the fragements are interrelated, we can't. We have to
  use sort of a `partial include', and I don't think that is easy. I
  have checked all the fragments so far.}

Now, let us define our object language (embedded DSL) in a different
way. Instead of using algebraic datatypes to encode the terms of the
language in the meta-language (reminiscent to the initial approach),
we will be using functions as term constructors (the final approach):

\begin{code}
  let b (bv:bool) env = bv
  let varZ (vc,_) = vc
  let varS vp (_,envr) = vp envr
  let app e1 e2 env = (e1 env) (e2 env)
  let lam e env = fun x -> e (x,env)
\end{code}

That is all of the interpreter. Our sample term now reads
\begin{code}
  let testf1 = app (lam varZ) (b true)
\end{code}
which is almost the same as before, only written with lower-case
identifiers. To evaluate the term, we merely need to apply 
the term to the empty environment

\begin{code}
  let testf1r = testf1 ()
  val testf1r : bool = true
\end{code}

The result now has no tags. The above functions |b|,
|varZ|, |app| constitute the evaluator. It is patent that the
interpreter has no tags and no pattern matching. The term |b true| is
evaluated to a boolean and a term |lam varZ| is evaluated to a
function, with no tags attached. The application rule applies that
function without any need for pattern-matching. The evaluator has
another feature: evaluation of open terms leads to a type error rather
than a run-time error. Indeed, 

\begin{code}
  let testf1bad = app (lam (varS varZ)) (b true)
  let testf1badr = testf1bad ()
\end{code}
gives a type error saying 
that the initial environment should be a tuple rather than () --
meaning the term requires a non-empty initial environment: the term is
open.

Thus all the errors that were previously reported at run-time (open terms,
applications of non-functions) are now reported at compile
time. Running of the interpreter will never result in run-time errors
because its code uses no operations that could possibly raise such
an error. In fact, our interpreter is total.


\oleg{
We should emphasize that not only our code can't have pattern-match
errors, that fact is apparent to the compiler itself. It is one thing
to write a code with pattern-matching and claim (and prove) that
missing cases cannot occur. However true that may be, the fact the
missing cases cannot occur is not evident to the compiler, which still
has to generate code to report missing cases or compile in the error
generation. That's where efficiency is lost, and Walid's papers on tag
elimination have numbers showing that performance is lost indeed. 
In our code, there is simply no pattern-mismatch opportunity: the
proof of no bad match is expressed in the code itself. }

In the above code, the interpreter is wired in, directly built into the
functions |b|, |app|, etc.  In the remainder of this paper, 
we explain how to abstract the interpreter,
to make it possible to `evaluate' the same term in many different
ways: interpretation, compilation, compute the size (or depth), etc.

\subsection{Contributions}\label{contributions}

        We have realized that if we change the encoding of the term --
from data constructors to functions, we can encode a tagless
interpreter of a typed language in a typed language with no GADTs and
a very simple type system. The interpreter uses no universal type, no
pattern matching and it is patently obvious to the compiler that the
only non-termination can come from interpreting |fix|. If the source
language is strongly normalizing, our interpreter is total.

The term ``constructor'' functions lam, app, etc. look like free
variables in the encoding of an object term.  Defining these functions
differently gives rise to different interpreters.  Given the same term
representation but varying the interpreters, we can
	- evaluate the term to a value in the metalanguage
	- determine the size or depth of a term
    - compute a \emph{CBN} CPS transformation of a term
	- `compile' the term. This requires staging, as in MetaOCaml.
	- Surprisingly, it is also possible to write a partial evaluator! 

We have implemented the above in (Meta)OCaml and Haskell.

What are our contributions:
\begin{enumerate}
\item we interpret/compile/partially-evaluate a typed source language
   from within a typed language.  The requirements of the host type
   system are \jacques{not so onerous?  simple?},
\item a simple, clean implementation,
\item a nicely comparable Haskell/MetaOCaml version,
\item a functorial Symantics that deals with uniformly across all
evaluators, from interpretation to partial evaluation,
\item clear and easy extensibility to more features,
\item only closed terms can be evaluated.
\item an approach to self-interpretation which is compatible with the
  above.
\end{enumerate}

Making sure that we interpret terms that are well-typed and closed --
and we can do that in far simpler type system (e.g., Haskell or
OCaml). It was thought that doing either requires quite advanced
extensions (GADT, Nanevski, Concoqtion).
\jacques{can we state fairly clearly what type system we need?  I mean,
Haskell98(?) and MetaOCaml with (objects, polymorphic variants, ...).
With our let-polymorphism trick for self-interpretation, what do we really
need?}

The structure of our paper is as follows. Section XXX ...
We finally conclude.

\section{Our base language: Tagless interpreter}\label{language}

Introduce the language, The language is simply-typed lambda-calculus
with fixpoint, integers, booleans and comparison, whose BNF grammar is

\begin{code}
  e ::= Lam hoas_fn | App e e | Fix hoas_fn |
  I int_literal | B bool_literal | Add e1 e2 | 
  Mul e1 e2 | Leq e1 e2 | IF b e-then e-else
\end{code}

This time we use higher-order abstract syntax (cite: but not
Elliott. Cite Dale Miller!) rather than de Bruijn indices.  The
language is close to the language of Xi03, without the polymorphic
lift but with more constants so we can write better examples, enough to
express Fibonacci, factorial, and power.

\subsection{How to make encoding flexible: abstract the interpreter}
\label{encoding}
We have implemented the above in (Meta)OCaml and Haskell.  In Haskell,
the term `constructor' functions are defined as methods in a type class
Symantics (the name means that the class interface gives the syntax for
the source language and its instances give the semantics).

\begin{code}
class Symantics repr where
  int :: Int -> repr Int            -- int literal
  bool :: Bool -> repr Bool         -- bool literal

  lam :: (repr a -> repr b) -> repr (a->b)
  app :: repr (a->b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int -> repr Int -> repr Int
  mul :: repr Int -> repr Int -> repr Int
  if_ :: repr Bool -> repr a -> repr a -> repr a
  leq :: repr Int -> repr Int -> repr Bool
\end{code}

|test1| now reads |app (lam (\x -> x)) (bool True)|,
whose inferred type is |Symantics repr => repr Bool|.
For example, we can encode the classical |power| function, as well
as the partial application |\x -> power x 7| as

\begin{code}
testpowfix () = 
  lam (\x -> fix (\self -> lam (\n ->
        if_ (leq n (int 0)) (int 1)
            (mul x (app self (add n (int (-1))))))))
testpowfix7 () = 
  lam (\x -> app (app (testpowfix ()) x) (int 7))
\end{code}
The dummy unit |()| argument above is to avoid the monomorphism
restriction and so that the type of |testpowfix| and |testpowfix7|
will remain polymorphic in |repr|. Instead of supplying the dummy
argument, we could have given the terms explicit polymorphic
signature.  We however prefer if the typechecker of the meta-language
(Haskell) infers the types of the object terms for us. We could also
avoid the |()| argument by switching off the monomophism restriction
with a compiler flag.

Note that the subject language (DSL) is typed. If we make a mistake
(for example, replace |int 7| with |bool True| in |testpowfix7|, the
compiler will highlight that term and complain that it couldn't match
the expected |Int| against the inferred |Bool|). A type error
also results when entering |lam (\x -> app x x)|, with the compiler
complaining about an infinite type. The type-checker of the
meta-language will flag meaningless DSL (subject level) terms --
before any of their evaluation. If we forget, for example, |app| in
some of the terms above, we get an error message with a precise
location. So the type-checker also flags syntactically invalid
subject-level terms.\oleg{Should we perhaps move this up, in the
  intro? To justify why typed-metalanguages and the typed DSL are
  good. Or keep the explanation here but refer to it from the Introduction.}

We can do the above in (Meta)OCaml as well, replacing type classes with
functors, via a module type.  For this section, the following simple
signature will be sufficient:

\begin{code}
module type Symantics = sig
  type ('c,'dv) repr
  val int : int  -> ('c,int) repr
  val bool: bool -> ('c,bool) repr
  val add : ('c,int) repr-> ('c,int) repr-> ('c,int) repr
  val mul : ('c,int) repr-> ('c,int) repr-> ('c,int) repr
  val leq : ('c,int) repr-> ('c,int) repr-> ('c,bool) repr
  val if_ : ('c,bool) repr ->
             (unit -> ('c,'da) repr) ->
             (unit -> ('c,'da) repr) -> ('c,'da) repr 
  val lam : (('c,'da) repr -> ('c,'db) repr) 
          -> ('c,'da->'db) repr
  val app : ('c,'da->'db) repr
    -> ('c,'da) repr -> ('c,'db) repr
  val fix : (('c,'da->'db) repr -> ('c,'da->'db) repr) 
            -> ('c,'da->'db) repr
end;;
\end{code}

Some differences are in the additional type parameter |c| (the
environment classifier, Walid03) to be used later when we get to code
generation (required by MetaOCaml, part of the type of the
code). MetaOCaml is a call-by-value language, and thus we have to 
$\eta$-expand
the fix-point combinator and encapsulate the branches of the |if_|
into thunks to prevent their evaluation. 

The power example and our sample term |test1| are encoded as
\begin{code}
module EX(S: Symantics) = struct
 open S
 let test1 () = app (lam (fun x -> x)) (bool true)
 let testpowfix () = 
   lam (fun x ->fix (fun self -> lam (fun n ->
     if_ (leq n (int 0)) (fun () -> int 1)
         (fun () -> mul x (app self 
                               (add n (int (-1))))))))
 let testpowfix7 = 
    lam (fun x -> app (app (testpowfix ()) x) (int 7))
end;;
\end{code}
The dummy unit argument in |testpowfix| is an artefact of MetaOCaml
(which we will be using to compile our tests in Sections below),
again related to monorphism. To be
more precise, the environment qualifier (which appears as the |'c|
type parameter in the signature of Symantic functions) must remain
uninstantiated in order for us to `run' the code. That means, the type
must be polymorhic. The value restriction then dictates 
the definitions of our object terms must look syntactically like
values. Alternatively to giving an |()| argument, we could have used
rank-2 record types of OCaml to maintain the necessary polymorphism.

Thus the object expressions (expressions of the embedded DSL) have in
OCaml the type of a functor from |Symantics| to the desired value. This
is essentially the same as the |Symantics repr => | constraint in the case of
the embedding of the DSL in Haskell.

The first instantiation of the Symantics structure is an interpreter:
evaluating the object (DSL) term to its value in the meta-language. 
The following module essentially interprets (gives one-to-one
correspondence to) each operation of the object language as the
corresponding operation in the meta-language.
\begin{code}
module R  = struct
  type ('c,'dv) repr = 'dv (* absolutely no wrappers *)
  let int  (x:int)  = x
  let bool (b:bool) = b
  let add  e1 e2    = e1 + e2
  let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then (et ()) else (ee ())

  let lam f         = f
  let app e1 e2     = e1 e2
  let fix f         = let rec self n = f self n in self
end;;
\end{code}

This interpreter is patently tagless: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. We can run our
examples by instatiating the |EX| functor with |R|.
\oleg{Jacques: you mentioned "Boxes go Bananas" by Washburn and
  Weirich. Do you want to comment more about that paper here?}

\noindent
\begin{code}
  module EXR = EX(R);;
\end{code}

\noindent and so |EXR.test1 ()| evaluates to the untagged boolean value |true|.
In Haskell, we define a\\
|newtype R a = R a| and |instance Symantics R|
with the straightforward implementation. We must stress that |R| is
a |newtype|. Although it may look like a tag, it has no run-time
representation (erased during compilation) and pattern-matching
against |R| cannot ever fail and so is operationally an identity and
compiled that way.

Also, note that the interpreter uses no Universal type nor pattern
matching, and it is obvious to the compiler that non-termination can
only result from interpreting |fix|.

\begin{prop}
If the term |e| in the meta-language has type
|t|, evaluation of that term in the interpreter R will either continue
indefinitely or termninate with a value of the same type |t|.
\end{prop}

We'd like show a different interpreter: it interprets each terms of
the object language to its size (defined as the number of term
constructors). The following is slightly abbreviated code (see the
accompanying source code for the complete definition).
 
\begin{code}
module L = struct
  type ('c,'dv) repr = int
  let int (x:int) = 1
  let mul e1 e2 = e1 + e2 + 1
  let if_ eb et ee = eb + et () + ee () + 1

  let lam f = (f 0) + 1
  let app e1 e2 = e1 + e2 + 1
  let fix f = (f 0) + 1
end;;
\end{code}

\noindent Now the expression
\begin{code}
  let module E = EX(L) in E.test1 ()
\end{code}
evaluates to |3|. This interpreter is not only tagless, it is also
total. It ``evaluates'' even seemingly divergent terms like\\
|app (fix (fun self -> self)) (int 1)|.

\begin{comment}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{comment}

\section{Tagless compiler (aka, staged interpreter)}\label{compiler}

We can also write a ``compiler'' from our lambda calculus directly
into OCaml code, using MetaOCaml's staging facilities. Namely,
future-stage expressions of the type |t| are represented at the
present stage as values of the type |('a,t) code| where |'a| is the
environment classifier (Walid03, ESOP04). Code values are created by
a \emph{bracket} form: |.<e>.| specifies the expression |e| is to be
evaluated at the future stage. The \emph{escape} |.~e|, which must occur
within a bracket, specifies that the expression |e| must be evaluated
at the present stage; its result, which must be a code value, is
spliced in into the code being built. The \emph{run} form |.! e|
evaluates the code value, which involves run-time compilation and
linking of the future-stage code. The bracket, escape and run are akin to
quasi-quotation, unquotation and |eval| of Lisp.


% this code does not have the 'sv parameter. It shows up later.
\begin{code}
module C = struct
  type ('c,'dv) repr = ('c,'dv) code
  let int (x:int) = .<x>.
  let bool (b:bool) = .<b>.
  let add e1 e2 = .<.~e1 + .~e2>.
  let mul e1 e2 = .<.~e1 * .~e2>.
  let leq x y = .< .~x <= .~y >.
  let if_ eb et ee = 
    .<if .~eb then .~(et () ) else .~(ee () )>.

  let lam f = .<fun x -> .~(f .<x>.)>.
  let app e1 e2 = .<.~e1 .~e2>.
  let fix f = 
     .<let rec self n = .~(f .<self>.) n in self>.
end;;
\end{code}
This is a completely straightforward staging of
|module R|.
This compiler attempts no optimizations at all, it simply produces
residual code. For example, the evaluation of our |test1|
\begin{code}
  let module E = EX(C) in E.test1 ();;
\end{code}
gives the value of the inferred type |('a, bool) C.repr| which looks
like |.<((fun x_6 -> x_6) (true))>.|. MetaOCaml can print the code
values, that is, the corresponding future-stage code.

\begin{comment}
\oleg{one may wish to give the folloiwng example too; the code
  expression can be somewhat abbreviated...}
\begin{code}
  let module E = EX(C) in E.testpowfix7;;
# - : ('_a, int -> int) C.repr =
.<fun x_1 ->
   (((fun x_2 ->
       let rec self_3 =
        fun n_4 ->
         ((fun x_5 -> if (x_5 <= 0) then 1 else (x_2 * (self_3 (x_5 + (-1)))))
           n_4) in
       self_3) x_1) 7)>.
\end{code}
\end{comment}

\noindent It is readily obvious that this compiler does not suffer
any interpretive overhead whatsoever. Indeed, the
  code produced for a function is simply |fun x_6 -> x_6| and has no 
  embedded calls to the evaluator: cf. the evaluator of DSL functions
  in Section~\ref{tagproblem}, the L-line.
The resulting code obviously contains no tags and no pattern-matching.

We have also implemented the ``compiler'' in Haskell. Since Haskell
has no (convenient, and typed) staging facilities, we had to emulate
them. To be precise, we defined a data type |ByteCode| with the
constructors |Var|, |Lam|, |App|, |Fix|, |INT| etc. with the obvious
meaning. Unlike our staging language (which is based on higher-order
abstract syntax), our bytecode, to be realistic, uses integer-named
variables. We then define 
\begin{code}
  newtype C t = C (Int -> (ByteCode t, Int)) 
\end{code}
where |Int| is the counter for the creation of fresh variable
names. We define the compiler by making |C| to be the instance of the
class |Symantics|. The implementation is quite similar (but more
verbose) than the corresponding MetaOCaml code given above. The
accompanying complete code gives more details.

\oleg{I wanted to avoid saying anything about GADTs here. There
  doesn't seem to be much point in getting into too long
  explanations...}



\section{Tagless partial evaluator}\label{PE}

Surprisingly, it is also possible to write a partial evaluator!  The
partial evaluator does have tags: whether a value is static or dynamic.
The pattern-matching on this \emph{phase} tag is always exhaustive; we still
have no universal type and no tags for object types.  The partial
evaluator uses the previously mentioned compiler to ``interpret'' dynamic
terms, and it uses the interpreter to reduce static terms.  In this sense, 
the partial evaluator is a modular extension to both the compiler and
the interpreter. 

\subsection{Non-functorialness of functions problem}
the sv type problem: the inductive type function 'dv -> 'sv
its GADT solution. 
its type-level function solution: incope1.hs 
\begin{code}
data P t where
    VI :: Int -> P Int
    VB :: Bool -> P Bool
    VF :: (P a -> P b) -> P (a->b)
    E  :: C t -> P t

abstr :: P t -> C t
abstr (VI i) = int i
abstr (VB b) = bool b
abstr (VF f) = lam (abstr . f . E)
abstr (E x) = x

instance Symantics P where
    int x  = VI x
    bool b = VB b

    -- lam :: (repr a -> repr b) -> repr (a->b)
    lam = VF
    -- app :: repr (a->b) -> repr a -> repr b
    app ef ea = case ef of
                        VF f -> f ea
                        E  f -> E (app f (abstr ea))
    add e1 e2 = case (e1,e2) of
                 (VI n1,VI n2) -> VI nr where nr = n1 + n2
                 _ -> E (add (abstr e1) (abstr e2))
\end{code}

GADT: |E|, Purely dynamic; the other alternatives: |static| code.

We need GADT -- or, to be more precise,
representation that varies with the type. We need a value of the type
|P Int| to have a different representation than the value of the type 
|P (Int->Int)|. GADT give you that (although we don't use the GADT in
their intended meaning for that). 

Drawbacks of the GADT solution. GADTs, although work, are still not
quite satisfactory: let us look at the following lines in incope.hs
\begin{code}
    app ef ea = case ef of
                        VF f -> f ea
                        E  f -> E (app f (abstr ea))
\end{code}
The datatype has four constructors, |VI|, |VB|, |VF| and |E|. How come we used
only two of them in the above expression. It is obvious, one may say,
that the constructors |VI| and |VB| just can't occur, because of the
typing consideration. This is obvious to us -- but it is not obvious
to the compiler. The case matching above does look \emph{partial}. That
seems like a tenuous point -- but it is precisely the main point of
the whole tag elimination approach. In a typed tagged interpreter
there are lots of pattern-matching that looks partial but in reality
total. The ultimate goal is to make this totality syntactically
apparent.

How can we do without GADT
Now show the Symantics signature in full, with the 'sv types

In the ideal world, the MetaOCaml signature (or Haskell type class)
Symantics would start with
\begin{code}
  kind tp = int | bool | (->) tp tp
  type ('c, 'dv:tp) repr : tp
  val int : int -> ('c, int) repr
  val bool : bool -> ('c, bool) repr
  val lam : (('c, 'da) repr -> ('c, 'db) repr) -> 
            ('c, 'da -> 'db) repr
  val add : ('c, int) repr -> ('c, int) repr -> 
            ('c, int) repr
\end{code}

Here "repr" is a tp-to-tp function, defined inductively in the case of
partial evaluation:

\begin{code}
  ('c, int   ) repr_pe = VI of int
                       | E of ('c, int) code
  ('c, bool  ) repr_pe = VB of bool
                       | E of ('c, bool) code
  ('c, 'a->'b) repr_pe = VF of ('c,'a) repr -> ('c,'b) repr
                       | E of ('c, 'a->'b) code
\end{code}

Unfortunately, we can't define type-level functions inductively.
Instead we bake |repr_pe| above into the signature Symantics, so repr
takes three type arguments |('c,'sv,'dv)|, where the extra argument 
|'sv| is equal to |('c,'dv) repr_pe.|
It seems Ken has just showed how to define type-level functions
inductively... This shows also how a limited form of GADTs (the ones
used for the sake of type-dependent representations) can be
implemented with just functions. 


So we have to augment the full signature of Symantics (from that of
sec.~\ref{encoding}) to
\begin{code}
module type Symantics = sig
  type ('c,'sv,'dv) repr
  val int  : int  -> ('c,int,int) repr
  val bool : bool -> ('c,bool,bool) repr
  val add  : ('c,int,int) repr -> 
      ('c,int,int) repr -> ('c,int,int) repr
  val mul  : ('c,int,int) repr -> 
      ('c,int,int) repr -> ('c,int,int) repr
  val leq  : ('c,int,int) repr -> 
      ('c,int,int) repr -> ('c,bool,bool) repr
  val if_  : ('c,bool,bool) repr ->
             (unit -> ('c,'sa,'da) repr) ->
             (unit -> ('c,'sa,'da) repr) -> 
             ('c,'sa,'da) repr 
  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr)
    -> ('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr),
           'da->'db) repr
  val app : ('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr),
             'da->'db) repr
    -> ('c,'sa,'da) repr -> ('c,'sb,'db) repr
  val fix : 
    (('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr) as 's,
         'da->'db) repr 
    -> ('c,'s,'da->'db) repr)  -> ('c,'s,'da->'db) repr
end
\end{code}



\subsection{MetaOCaml version}

%%  let dyn_id (x:('c,'sv,'dv) repr):('c,'sv1,'dv) repr = x

%% We add one item to the language, |dyn_id|, as it
%% is needed in the next section, where we will explain its purpose.

Here is the full code for the MetaOCaml version.
\begin{code}
module P = struct
  type ('c,'sv,'dv) repr = 
        {st: 'sv option; dy: ('c,'dv) code}
  let abstr {dy = x} = x
  let pdyn x = {st = None; dy = x}

  let int  (x:int)  = {st = Some (R.int x); dy = C.int x}
  let bool (x:bool) = {st = Some (R.bool x); dy = C.bool x}

  (* generic build - takes a repr constructor, an 
     interpreter function and a compiler function (all 
     binary) and builds a PE version *)
  let build cast f1 f2 = fun e1 -> fun e2 -> 
     match (e1,e2) with
         ({st = Some n1}, {st = Some n2}) -> cast (f1 n1 n2)
        | _ -> pdyn (f2 (abstr e1) (abstr e2))
  (* same as 'build' but takes care of the neutral element 
     (e) simplification allowed via a monoid structure which is 
     implicitly present *)
  let buildsimp cast e f1 f2 = fun e1 e2 -> 
     match (e1,e2) with
        | ({st = Some e'}, _) when e = e' -> e2
        | (_, {st = Some e'}) when e = e' -> e1
        | ({st=Some n1}, {st=Some n2})-> cast (f1 n1 n2)
        | _ -> pdyn (f2 (abstr e1) (abstr e2))

  let add e1 e2 = buildsimp int 0 R.add C.add e1 e2
  let mul e1 e2 = buildsimp int 1 R.mul C.mul e1 e2
  let leq e1 e2 = 
     match (e1,e2) with
         ({st=Some n1},{st=Some n2})-> bool (R.leq n1 n2)
        | _ -> pdyn (C.leq (abstr e1) (abstr e2))
  let if_ eb et ee = match eb with
           {st = Some b} -> if b then et () else ee ()
         | _ -> pdyn (C.if_ (abstr eb) 
                          (fun () -> abstr (et ()))
                          (fun () -> abstr (ee ())))
  let lam f = {st = Some f; 
               dy = C.lam (fun x -> abstr (f (pdyn x)))}
  let app ef ea = match ef with
        {st = Some f} -> f ea
      | _ -> pdyn (C.app (C.dyn_id (abstr ef)) (abstr ea))

  let fix f = 
    let fdyn = C.fix (fun x -> abstr (f (pdyn x))) in
    let fdynn e = pdyn (C.app fdyn e.dy) in
    {st=Some (function {st = Some _} as e -> 
               let rec self n = 
                 (match n.st with Some _-> app (f (lam self)) n 
                                 | _ -> fdynn n) in 
               app (f (lam self)) e
             | e  -> fdynn e);
    dy = fdyn }
end;;
\end{code}

\oleg{Compare the output of some of the ctest with the corresponding
ptest. Use power?}

One should note that the partial evaluator has fully reused the code
of the compiler and the interpreter. It is truly the `composition' of the
interpreter and the compiler.  \jacques{Perhaps cite a paper of Thiemann
here, cogen in 6 lines?}

\jacques{Comment on the various nice generic things in the implementation,
which are a little silly in this small version, but would really pay off
in something bigger.  See the abstract interpretation hard at work!
See the monoid-driven simplifications!}
\jacques{Also need to note that the type of the above is not quite
the original Symantics (although the code has the right operational
semantics).  We need to make some adjustments, as per below.}



Also, unlike the GADT
solution, there is no longer any seemingly partial
pattern-matching. The pattern-match only occurs in PE (to check if the
code is static or dynamic), it is only a phase rather than a type tag
match, and the pattern match is \emph{patently} exhaustive. That is,
totality is assured and it is assured \emph{syntactically}, in the way that
is apparent to the compiler.


It's relation to PE and CPS. Currently, our CPS is CBN. Need to
investigate CBV.
The type-level function |'da -> 'sa| is important because it gives 
the CPS transform. So, the type-function
that enables the PE is exactly the same type function that enables
CPS....

Say that when doing PE on fix, we made a deliberate decision to `go
all the way' -- so long as the computation can proceed at the PE stage
(`static value'), we do that rather than residualizing. We have also
an alternative where fix is unrolled only once and them we
residualize. We have used the first approach, even it (unlike the
latter) may cause non-termination at the PE stage -- but \emph{only} if the
computation we are partially evaluating is non-terminating and enough
static information is available to trigger this non-termination.
Say that our treating of fix may lead to the code bloat in the
residual program. That is an issue investigated elsewhere (cite our
PEPM2006 paper). 

\section{Variations and discussion}\label{discussion}

state and imperative features

CPS transformation (interpreting/compiling the term to CBN CPS)

We must formulate some propositions: a typed term makes progress in
any interpreter. The reason it is typed in any interpreter:
|forall r. Symantics r => r tau| directly says that in every model the
term is typed. 

The PE in incope is really a mixture of online and off-line; it looks 
online, but it has a sort of "just in time" binding-time analysis built in.

If we just add a method 'lift' to Symantics with a type
|lift :: a -> repr a| (cf polymorphic lift of Xi03, of MetaML and
MetaOCaml: CSP)
then |Symantics repr| implies |Functor repr|. We can write then
\begin{code}
  instance Semantics repr => Functor repr where
  -- fmap :: (a->b) -> repr a -> repr b
  fmap f ra = app (lift f) ra
\end{code}

\subsection{Translucent types}
The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
|app _add (app _int 1) (app _int 2)| where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is

$$ 
  (\ZZ \rightarrow r \ZZ) \rightarrow
  (\BB \rightarrow r \BB) \rightarrow
  \forall \alpha \beta. (r \alpha \rightarrow r \beta)
      \rightarrow r (\alpha\rightarrow\beta) \rightarrow ... r \tau
$$
% (Int -> r Int) ->
% (Bool -> r Bool) ->
% (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau

or, if we denote the sequence of initial arguments as S r, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.


\section{Self-interpretation}\cite{selfinterp}

Taha et al (cite different paper) define a self-interpreter si to be an
object-language term such that, for any object-language term e,

\begin{code}
  si "e"    is observationally equivalent to    e
\end{code}

where |"e"| is an encoding of |e|.  Under the co-approach, we want to encode
each object-language term as a function from a record of cogen methods
to a generated rep.  (Thanks to currying, the object language need not
actually support records.)  In the case of a self-interpreter, that
record is simply si!  So we should define a self-interpreter si to be an
object-language record-term such that, for any object-language term e,

\begin{code}
  "e" si    is observationally equivalent to    e
\end{code}

The self-interpreter then is just like the interpreter
|R| in Section 2, except written at the object rather than meta level.

Note on terms and special forms. In our class Symantics, |add|
is not a term -- because it does not have the type repr t. We could
have introduced |add| to be of the type |repr (Int->Int->Int)| -- but we
didn't. So, |add| is a special form. It is to be applied in
meta-language (using the Haskell application -- white space) rather
than in the object language (app). 

Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).


Self-interpretation in our framework is
a bit tricky due to a need for polymorphism.  An encoded term should
be polymorphic in the interpreter type-constructor "r".  Further, the
functions "lam", "app", etc. above should be polymorphic in the object
types "a" and "b".  Thus the object language seems to require rank-2
polymorphism; and so type annotations will be inevitable.


To neatly bypass the need for rank-2 polymorphism, we use the notion
of let-polymorphism and a `hole', as follows.  A partial evaluator (or
a compiler or interpreter) is an evaluation context that (let-)binds
variables named "app", "lam", etc.  For the object language to
take advantage of let-bound polymorphism in the metalanguage, it
is crucial that we encode object "let" to meta "let".  

We also (unfortunately need to) define a self-interpreter slightly
differently.  A self-interpreter |SI[]| is an object expression with a
hole |[]| waiting to be plugged with the encoding of an object expression.
The self-interpreter binds the free variables |_lam|, |_app|, |_int|, |_bool|,
etc. in the plugged encoding.  We require |SI["E"]| to be equivalent to |E|,
where |"E"| is the object encoding of the object expression |E|.

This strategy of course limits what kinds of processing on encoded
object expressions can be expressed in the object language.  For
example, unlike if we had simply defined |"E"| to be the `identity
encoding' E, we \emph{can} write a size-measurer |SZ[]|, which like |SI[]| is
an object expression with a hole |[]| waiting to be plugged with |"E"|.
However, this notion of size cannot distinguish |let x = E1 in E2[x]|
from |"E2[E1]"|.  Cf. one way to type-check polymorphic let:
\begin{code}
        E1:T    E2[E1]:T'
    ------------------------
    let x = E1 in E2[x] : T'
\end{code}
That analogy shows that the self-interpreter with a hole is a
euphemism for a higher-rank abstraction. That makes it clear where
the difficulty lies, and how to simulate the higher-rank application
(that is, higher-rank arrow elimination) `by hand'.


We also need to duplicate the encoding of an object expression if we
want to process the same object expression in multiple ways (e.g., both
measure its size and interpret it).  But all this is consistent with (in
fact, amplifies) our `coalgebraic/final/cogen' spirit, so our revised
notion of self-interpretation (and, some might say, of size measurement)
may just squeak by the reader's skepticism as we explain our desire for
kind polymorphism.

I guess the let-and-hole argument works out if we don't typecheck the
encoded terms. We can't typecheck the interpreter with the whole
either. We insert the encoded term into the interpreter -- and then
typecheck the whole thing. That'll work. Yet a bit unsatisfactory...


We achieve
self-interpretation in that the following evaluation context in the
object language defines an interpreter for encoded object terms.
\begin{code}
  let lam = \f -> f in
  let app = \f x -> f x in
  let add = \m n -> m + n in
  let int = \i -> i in
  [hole]
\end{code}
The obvious notion of optimality in this setup is easy to achieve.  For
example, given the context above, our partial evaluator turns
\begin{code}
  app (lam (\x -> add x (int 1))) (int 3)
\end{code}
into the number 4, as desired.  We can also encode the self-interpreter
above as the following evaluation context in the object language.
\begin{code}
  let lam = lam (\f -> f) in
  let app = lam (\f -> lam (\x -> app f x)) in
  let add = lam (\m -> lam (\n -> add m n)) in
  let int = lam (\i -> i) in
  [hole]
\end{code}
The "let" and the hole "[hole]" are meta-constructions, which must map to
themselves in any interpretation.  

The 'result' of applying an interpreter to itself, needs to be an 
interpreter too!  In fact, if it is Jones-optimal, then you should be 
able to apply it to itself again and get the same answer.  
Our code does that. First, |twice_inc_3_e| is the
metalanguage encoding of the object term |si["e"]|
where si is the self-interpreter of the object language (actually, only
lam,app,add,int), |""| means the object-to-object encoding, and e is the
object term
\begin{code}
  let twice_ f x = f (f x) in let inc_ n = n + 1 in twice_ inc_ 3
\end{code}

Second, |twice_inc_3_ee| is the metalanguage encoding of the object term
|si["si[e]"]|
which is the same as
|si["si"["e"]]|
The partial evaluator yields 5 on both |twice_inc_3_e| and 
|twice_inc_3_ee|,
as desired.


``But how 
to you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?''
Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.
Again, what is different from the above is that we \emph{can}
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).



\section{Related work}\label{related}
Walid's Jones optimality paper. See his own ICFP02 paper that
described what he achieved then and what he \emph{did not}. He did not
eliminate all the tags. Ken's and mine impression of it:
(i) we can't write a \emph{direct} interpreter of the type language
           -- the only known to them, and \emph{partial} solution, was 
              that by Zhe Yang;
(ii) we have to use indirect interpreter, which needs the universal
           type and hence tagging;
(iii) that means any self-interpreter must have tags
(iv) that means not Jones-optimal.
It seems we should attack the point1 (in which case we don't have to
bother with self-interpreters). We say that we \emph{can} write the direct
interpreter, and without GADT or their equivalents (typeclasses).
Meta-language computes types, and the compiler and interpreter respect
those types and needs no tags.


Walid's ICFP02

Walid's PEPM07 (Concoqtion paper)? At leats at the Concoqtion
presentation at WG2.11, Emir used tagless interpreter as the main and
only example. \jacques{but at the PEPM07 presentation, Walid did not
quite do it that way.  He presented Concoqtion more as an exploration
of (an obvious but difficult) part of the design space for programming
languages.}

Danvy's self-interpreter
Ken has read Danvy et al's paper and said that the paper
doesn't seem to even make the claim that the source language is
typed. The interpreter is typed, but the source language doesn't seem
to be.
In sharp contrast to Danvy's paper, all the evaluators in our case
cannot produce any pattern-match error! Except for the partial
evaluator, there are NO patterns to match. And this is critical for
tag elimination -- pattern matching means there are no tags. And in
PE, the pattern-match deals with phases rather than with types. Also,
in our case the exhaustiveness of the pattern match is apparent to the
compiler -- that's why there are no tags.

Simon Peyton-Jones GADTs (the motivation for GADT is tagless
interpreter, but only for the first-order language)

Xi's POPL03 (He uses a higher-order language, one of the motivations
for his GRDT)

Asai; 
It [dynamic part when dealing with bool, int, lam in PE] is
Asai's trick for reducing the amount of reify/reflect back-and-forth
in a PE for an untyped language that allows you back-and-forth.  Or at
least that's how I read those papers.
Asai's approach is related to our PE (emulating typelevel functions):
Ken: ``In some sense we're just observing that Asai's code type-checks
-- in Hindley-Milner as soon as we deforest the static code
representation.''

Sumii et al.

"Boxes go Bananas" by Washburn and Weirich (intended or otherwise) Ken
wrote about this: ``but a glance reveals no type classes.  Do you mean
then that if we hide the definition |newtype R a = R a| from a client,
and export only the interpreter |instance Symantics R R|, then the
client would see R as an HOAS datatype a la Washburn and Weirich and
only be able to create parametric ("non-exotic") terms?''  Jacques
commented: ``I mean the HOAS parts, and how to deal with it (like in
the instance for |Symantics R R|).  There are other nice tricks for
dealing with HOAS in that paper worth a closer look.  It's all about
plain polymorphism, not type classes, but there are still some clever
ideas there, and they are about issues of tagging (an lack thereof).''

Typing Dep Typing plus two messages on Haskell-Cafe: typechecking 
from untyped to the typed form, ready to be interpreted. Walid's ICFP02
uses dependent types


Regarding our implementation of the type-level function |'sv -> 'dv|.
Gibbons et al paper on
the implementations of typecase may be relevant. I have not read it,
but I have the feeling some of their typecase designs may be
relevant (at least to cite). 
However, the typecase paper relies on having a 
bijection between types -- our |'sv| and |'dv| in this case, which is not 
our situation.  We have a one-sided view of things, where |'sv -> 'dv| is 
easy, and |'dv| "perfectly reflects" an |'sv|, but you still can't get at 
that |'sv|.  \jacques{Could we say that we have a meta-level Galois
injection?  We have one direction completely covered at the object
level (and so easy to lift), but 'dv to 'sv can only be done by
an outside mechanism}


Self-Interpretation and Reflection in a Statically Typed Language 
\url{http://webpages.cs.luc.edu/~laufer/papers/reflection93.pdf}
He uses the SK language and some typeclasses. Their interpreter is a
meta-circular interpreter -- rather than self-interpreter. We can
claim to have implemented a far better interpreter for a far richer
language.

\section{Conclusions}\label{conclusion}

We show how to do many things without tags, in a simple type system, 
essentially by avoiding any (inductive) types.  

\jacques{Should we mention that algebraic type tags give us a partly
intensional representation of terms.  This nicely allows pattern matching,
but as sits inside an extensional theory, ensuring that all traces 
of these tags disappear seems quite difficult without a very powerful
type system (enter GADTs).}

Our approach showed better the meaning of GADT and when they can be
avoided.

separate compilation: separate
typechecking of terms without knowing which particular
hole they will be plugged into. Formulate this as a prop?


\subsection*{Acknowledgments}
We would like to thank Martin Sulzmann and Walid Taha 
for helpful discussions.

\end{document}
