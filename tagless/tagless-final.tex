\hyphenation{meta-language meta-languages Meta-OCaml meta-cir-cu-lar type-case}
\documentclass[preprint]{sigplanconf}
\usepackage{amsmath,amssymb}
\usepackage{mathptmx}
%\usepackage{txfonts} \renewcommand{\ttdefault}{cmtt}
\usepackage[T1]{fontenc}
\usepackage[override]{cmtt}
\usepackage{comment}
\usepackage{hyphenat}
\usepackage{url}
\usepackage{prooftree1}
\usepackage{mdwlist}

\newcommand{\jacques}[1]{{\it [Jacques says: #1]}}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}
\newcommand{\ccshan}[1]{{\it [Ken says: #1]}}

\usepackage{natbib1}
\let\cite=\citep

\usepackage[compact]{fancyvrb1}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=\parindent}

\newtheorem{prop}{Proposition}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\BB}{\mathbb{B}}

\newcommand{\fun}[1]{\mathopen{\lambda\mathord{#1}.\,}}
\newcommand{\fix}[1]{\mathopen{\mathrm{fix\,}\mathord{#1}.\,}}
\newcommand{\Forall}[1]{\mathopen{\forall\mathord{#1}.\,}}
\newcommand{\Exists}[1]{\mathopen{\exists\mathord{#1}.\,}}
\newcommand{\cond}[3]{\mathrm{if\ }#1\mathrm{\ then\ }#2\mathrm{\ else\ }#3}
\newcommand{\True}{\mathinner{\mathrm{true}}}
\newcommand{\False}{\mathinner{\mathrm{false}}}

% Make space around section headings no less than space around theorems
\makeatletter
\@tempdima\topsep \advance\@tempdima\parsep
\ifdim\@sectionbelowskip   <\topsep \@sectionbelowskip   \topsep \fi
\ifdim\@subsectionbelowskip<\topsep \@subsectionbelowskip\topsep \fi
\makeatother

% Match the rule inserted by \@makecaption in sigplanconf.cls
\setproofrulebreadth .33pt
\newenvironment{floatrule}
    {\hrule width \hsize height .33pt \vspace{.5pc}}
    {\par\addvspace{1ex}}

% Make some Rel symbols into Bin symbols instead
\DeclareMathSymbol{\to}{\mathbin}{symbols}{"21}
\DeclareMathSymbol{:}{\mathbin}{operators}{"3A}

\begin{document}

\conferenceinfo{ICFP '07}{Freiburg, Germany} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Staged typed type-preserving tagless ``interpreters'', \emph{finally}}
\subtitle{Subtitle Text, if any}

\begin{comment}
\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}
\end{comment}
\authorinfo{}{}{}

\maketitle

\begin{abstract}
We have built a family of tagless interpretations for a
higher-order typed object
language in a typed metalanguage (Haskell and ML), 
requiring neither dependent
types, nor generalized algebraic data types, nor a tag elimination 
postprocessing step. It has been considered an open (impossible)
problem to do so.
Our family of interpreters contains 
an evaluator, a compiler, a partial
evaluator, a term-size evaluator and a CPS call-by-name transformer.
We take advantage of staging when available.

The main tool is to take functions seriously: we encode the
de Bruijn or higher-order abstract syntax  of object terms 
not by using constructors for
an algebraic data type but rather by invoking cogen functions, which occur as
free variables in the encoded term.  In other words, we eschew initial
algebras for the wilder world of final algebras.
(Or: we expose the co-algebraic structure of the object language).
Our approach highlights the importance of order-2 type constructors
(rank-2 polymorphism), available with ML functors and
Haskell98 constructor classes. This lets us encode object terms with
translucent types: enough abstraction is available to interpret the
terms in various ways, and yet enought type information is available 
to typecheck the terms and statically assure their interpretation will
never get stuck.

\oleg{If we talk about self-interpreter, mention let-bound polymorphism}
\end{abstract}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}\label{intro}

A popular way to define and implement a programming language is to embed it in
another \citep{reynolds-definitional}.  Embedding means to represent
terms and values of the \emph{object language} as terms and values in the
\emph{metalanguage}.  Embedding is especially appropriate for domain\hyp
specific object languages because it supports rapid prototyping and integration
with the host environment \citep{hudak-building} \oleg{perhaps say a
  couple more words}.  If the metalanguage supports \emph{staging}, then
the embedding can compile object programs to the metalanguage and avoid the
overhead of interpreting them on the fly \citep{WalidICFP02}.  A staged
definitional interpreter is thus a promising way to build a domain\hyp specific
language.

\begin{figure}
    \begin{floatrule}
    \begin{proofrules}
        \[ \[ [x:t_1] \proofoverdots e:t_2 \] \justifies \fun{x}e:t_1\to t_2 \]
        \[ \[ [f:t_1\to t_2] \proofoverdots e:t_1\to t_2 \] \justifies \fix{f}e:t_1\to t_2 \]
        \[ e_1:t_1\to t_2 \quad e_2:t_1 \justifies e_1 e_2: t_2 \]
        \[ \text{$n$ is an integer} \justifies n:\ZZ \]
        \[ \text{$b$ is a boolean} \justifies b:\BB \]
        \[ e:\BB \quad e_1:t \quad e_2:t \justifies \cond{e}{e_1}{e_2}:t \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1+e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \times e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \le e_2:\BB \]
    \end{proofrules}
    \end{floatrule}
    \caption{Our typed object language}
    \label{fig:object}
\end{figure}

We focus on embedding a typed object language into a typed metalanguage.
Types are useful as usual because they document the programmer's
intentions and can be checked early (see~\S\ref{encoding}).
To be concrete, we use the typed object language in
Figure~\ref{fig:object} throughout this paper.  We aim not just for
evaluation of object programs but also for
compilation, partial evaluation, and other processing.

\Citet{WalidICFP02} and \citet{xi-guarded} motivated interpreting
a typed object language in a typed metalanguage as an interesting
problem.  The known solutions to this problem store object terms and
values in the metalanguage in a universal type, a generalized algebraic
data type (GADT), or a dependent type.  In the remainder of this section,
we discuss these solutions, identify their drawbacks, then summarize our
proposal and contributions.  No matter how we represent the object language in the
metalanguage, the representation can be performed by hand or more
conveniently by a parser or preprocessor such as camlp4.  We leave aside
the issue of whether to program a parser or preprocessor using dependent
types \citep{WalidICFP02} or not \citep{baars-typing}.

\subsection{The Tag Problem}\label{tagproblem}

%\oleg{see tagless\_interp1.ml, module Tagfull for the complete code.
%  If you change the code in here, please adjust the .ml file
%  accordingly. Let the paper and the accompanying code be in sync.}

It is straightforward to create an algebraic data type, say in OCaml, to
represent object terms such as those in Figure~\ref{fig:object}.
For brevity, we elide treating integers, conditionals, and fixpoint in
this section.
\begin{code}
type var = VZ | VS of var
type exp = V of var | B of bool
         | L of exp | A of exp * exp
\end{code}
We represent each variable using a unary de Bruijn index.
For example, we represent the object term $(\fun{x}x)\True$ as
\begin{code}
let test1 = A (L (V VZ), B true)
\end{code}

Following \citet{WalidICFP02},
we try to implement an interpreter function |eval0|. It takes
an object term such as |test1| above and gives us its value.
The first argument to |eval0| is the environment, initially empty,
which is the list of values bound to free variables in the
interpreted code.
\begin{code}
let rec lookup (x::env) = function
| VZ   -> x
| VS v -> lookup env v

let rec eval0 env = function
| V v       -> lookup env v
| B b       -> b 
| L e       -> fun x -> eval0 (x::env) e
| A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{code}

If our OCaml-like metalanguage were untyped, the code above would be acceptable.
The |L e| line exhibits interpretive overhead:
|eval0| traverses the function body~|e| every time (the result of
evaluating) |L e| is applied. Staging can be used to remove this
interpretive overhead \citep[\S1.1--2]{WalidICFP02}.

However, the function |eval0| is ill-typed
if we use OCaml or some other typed language as the metalanguage.
The line |B b|
says that |eval0| returns a boolean, whereas the next line |L e| says
the result is a function, but all branches of a pattern-match form must
yield values of the same type. 
A related problem is the type of the environment |env|: a regular
OCaml list cannot hold both boolean and function values. 

The usual solution is to introduce a universal type \citep[\S1.3]
{WalidICFP02} containing both booleans and functions.
\begin{code}
type u = UB of bool | UA of (u -> u)
\end{code}
We can then write a typed interpreter
\begin{code}
let rec eval env = function
| V v       -> lookup env v
| B b       -> UB b
| L e       -> UA (fun x -> eval (x::env) e)
| A (e1,e2) -> match eval env e1 with UA f ->
               f (eval env e2)
\end{code}
whose inferred type is |u list -> exp -> u|. Now we can evaluate
our sample |test1| term.
\begin{code}
let test1r = eval [] test1
val test1r : u = UB true 
\end{code}

The unfortunate tag |UB| in the result reflects that |eval| is a partial
function, in two ways.  First, the pattern match |with UA f| in the line
|A (e1,e2)| is not exhaustive, so |eval| can fail if we apply a boolean,
as in the ill-typed term |A (B true, B false)|.
\begin{code}
let test2 = A (B true, B false)
let test2r = eval [] test2
Exception: Match_failure in eval
\end{code}
Second, the |lookup|
function assumes a nonempty environment, so |eval| can fail if we
evaluate an open term.
\begin{code}
let test3 = A (L (V (VS VZ)), B true)
let test3r = eval [] test3
Exception: Match_failure in lookup
\end{code}
After all, the type |exp| represents object
terms both well-typed and ill-typed, both open and closed.

If we evaluate only closed terms that have been type-checked, then
|eval| would never fail. Alas, this soundness is not obvious to the
metalanguage, whose type system we must still appease with the
nonexhaustive pattern matching in |lookup| and |eval| and the tags |UB|
and |UA| \citep[\S1.4]{WalidICFP02}.  In other words, the algebraic data
types above fail to express in the metalanguage that the object program
is well-typed.  This failure necessitates tagging and nonexhaustive
pattern\hyp matching operations that incur a performance penalty in
interpretation \citep{WalidICFP02} and impair optimality in partial evaluation
\citep{taha-tag}.  In short, the universal\hyp type solution is
unsatisfactory because it does not preserve typing.

\subsection{Solutions Using Fancier Types}

It is commonly thought that to interpret a typed object language in
a typed metalanguage while preserving types is difficult and requires
GADTs or dependent types \citep{taha-tag}.  In fact, this problem
motivated much work on GADTs \citep{xi-guarded,peyton-jones-simple} and
on dependent types \citep{WalidICFP02,fogarty-concoqtion}.

For a metalanguage's type system to allow the well-typed object term
|test1| but disallow the ill-typed object term |test2|, fancier types
such as GADTs or dependent types seem necessary.  Yet other type systems
have been proposed to distinguish closed terms like |test1| from open
terms like |test3|
\citep{WalidPOPL03,NanevskiICFP02,NanevskiJFP05,DaviesJACM01,nanevski-contextual},
so that |lookup| never receives an empty environment.  We discuss these
proposals further in \S\ref{related}; here we just note that many
advanced type systems have been devised to ensure statically that an
object term is well-typed and closed.

\subsection{Our Final Proposal}\label{ourapproach}

We represent object programs using ordinary functions rather than
data constructors.  The interpreter below provides these functions.
\jacques{but now this smaller language does not match anymore what
we wrote earlier.  I think it is better to use a small language here,
but i don't quite know how to say so}
\begin{code}
let varZ env        = fst env
let varS vp env     = vp (snd env)
let b (bv:bool) env = bv
let lam e env       = fun x -> e (x,env)
let app e1 e2 env   = (e1 env) (e2 env)
\end{code}
These functions comprise the entire interpreter.
We now represent our sample term $(\fun{x}x)\True$ as
\begin{code}
let testf1 = app (lam varZ) (b true)
\end{code}
This representation is almost the same as in \S\ref{tagproblem}, only
written with lowercase identifiers. To evaluate an object term is to
apply its representation to the empty environment.
\begin{code}
let testf1r = testf1 ()
val testf1r : bool = true
\end{code}
The result has no tags---the interpreter patently uses no tags and no
pattern matching. The term |b true| evaluates to a boolean and the term
|lam varZ| evaluates to a function, both untagged. The |app| function
applies |lam varZ| without pattern matching. What is more, evaluating an
open term such as |testf3| below gives a type error rather than
a run-time error.
\begin{code}
let testf3 = app (lam (varS varZ)) (b true)
let testf3r = testf3 ()
\end{code}
The type error correctly complains
that the initial environment should be a tuple rather than |()|.
In other words, the term is open.

In sum, by Church\hyp encoding terms using ordinary functions,
we achieve a tagless evaluator
for a typed object language in a metalanguage with a simple type
system \citep{hindley-principal,milner-theory}.  In this \emph{final}
rather than \emph{initial} approach, both kinds of run-time errors
in \S\ref{tagproblem} (applying a nonfunction and evaluating an open
term) are reported at compile time. Because the new interpreter uses no
universal type or
pattern matching, it never results in a run-time error and is in fact
total.  Because this totality is obvious not just to us but also to the
metalanguage implementation, we avoid the serious performance penalty
\citep{WalidICFP02} of error checking.  \Citet{Gluck-jones-optimality}
explains deeper technical reasons that inevitably lead to these performance
penalties.

The evaluator above is wired directly into the
functions |b|, |lam|, |app|, and so on.  In the rest of this paper, 
we explain how to abstract the interpreter so as
to process the same term in many other
ways: compilation, partial evaluation, size
computation, and so forth.

\subsection{Contributions}\label{contributions}

The term ``constructor'' functions |b|, |lam|, |app|, and so on appear
free in the encoding of an object term such as |testf1| above.  Defining
these functions differently gives rise to different interpreters, that
is, different folds on object programs.  Given the same term
representation but varying the interpreter, we can
\begin{itemize*}
    \item evaluate the term to a value in the metalanguage;
    \item measure the size or depth of the term;
    \item compile the term, with staging support such as in MetaOCaml;
    \item even partially evaluate the term, online; and
    \item transform the term to continuation\hyp passing style (CPS),
        including call-by-name (CBN).
\end{itemize*}
We have programmed our interpreters in OCaml (and, for staging,
\citet{metaocaml}) and standard Haskell. The complete code is available%
\footnote{The URL will appear in the final version of the paper.}
% connot give URL in the ICFP paper due to double-blind reviews
to supplement the paper. Our examples below switch between (Meta)OCaml
and Haskell even though we have implemented each example equivalently in
both metalanguages, because some of our claims are more obvious in one
metalanguage than the other.  For example, MetaOCaml provides
convenient, typed staging facilities.

Our code ensures statically that an object term is well-typed and
closed, yet is surprisingly simple and obvious in hindsight.  We stress
that until now our achievement has been thought impossible: to
interpret a typed object language in a typed metalanguage without
tagging or type\hyp system extensions.  For example, \citet{taha-tag}
states that ``expressing such an interpreter in a statically typed
programming language is a rather subtle matter. In fact, it is only
recently that some work on programming type-indexed values in ML
\citep{yang-encoding} has given a hint of how such a function can be
expressed.''  We further discuss related work in~\S\ref{related}.

Our contributions are
\begin{enumerate}
\item to evaluate, compile, and partially evaluate a typed object language
   in a typed metalanguage with a simple type system;
\item to check statically for well-typed and closed object programs;
\item clean, comparable implementations in MetaOCaml and Haskell;
\item a functor signature that encompasses all our interpreters, from
    evaluation and compilation to partial evaluation and CBN CPS
    transformation;
\item a clear way to extend the object language with more features;
\item and an approach to self\hyp interpretation compatible with the
  above.  Self\hyp interpretation turned out to be harder than expected.
\end{enumerate}

\jacques{we should probably be more explicit about the contributions of
the discussion section}

The structure of our paper is as follows. Section XXX ...
We finally conclude.
\jacques{TODO}

\section{The object language and its tagless interpreters}\label{language}

Figure~\ref{fig:object} shows our object language, a simply-typed
$\lambda$-calculus with fixpoint, integers, booleans, and comparison.
The language is close to \citets{xi-guarded}, without their polymorphic
lift but with more constants so as to express Fibonacci, factorial, and
power.

In the sequel, we encode binding in object programs using higher-order
abstract syntax (HOAS) \citep{miller-manipulating,pfenning-higher-order}
rather than de Bruijn indices.

\subsection{How to make encoding flexible: abstract the interpreter}
\label{encoding}

We embed this language in (Meta)OCaml and Haskell.  In Haskell,
the functions that construct object terms are methods in a type class
|Symantics|. The class is so named because its interface gives the syntax for
the object language and its instances give the semantics.
The parameter |repr| to the class has kind |* -> *|.
\begin{code}
class Symantics repr where

  int  :: Int  -> repr Int
  bool :: Bool -> repr Bool

  lam :: (repr a -> repr b) -> repr (a -> b)
  app :: repr (a -> b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int -> repr Int -> repr Int
  mul :: repr Int -> repr Int -> repr Int
  leq :: repr Int -> repr Int -> repr Bool
  if_ :: repr Bool -> repr a -> repr a -> repr a
\end{code}
For example, we encode the term |test1|, or $(\fun{x}x)\True$, from
\S\ref{tagproblem} above as |app (lam (\x -> x)) (bool True)|,
whose inferred type is |Symantics repr => repr Bool|.
For another example, the classical $\mathit{power}$ function is
\begin{code}
testpowfix () = 
  lam (\x -> fix (\self -> lam (\n ->
  if_ (leq n (int 0)) (int 1)
      (mul x (app self (add n (int (-1))))))))
\end{code}
and the partial application $\fun{x} \mathit{power}\;x\;7$ is
\begin{code}
testpowfix7 () = 
  lam (\x -> app (app (testpowfix ()) x)
                 (int 7))
\end{code}
The dummy argument |()| above is to avoid the monomorphism
restriction, to keep the type of |testpowfix| and |testpowfix7|
polymorphic in |repr|. Instead of supplying this dummy
argument, we could have given the terms explicit polymorphic
signatures.  We however prefer for
Haskell to infer the object types for us. We could also
avoid the dummy argument by switching off the monomophism restriction
with a compiler flag.

Because the types in the |Symantics| class reflect the object language's type
system, this representation admits only well-typed object terms for evaluation.
If we make a mistake somewhere, say replace |int 7| with |bool True| in
|testpowfix7|, Haskell will complain there that the expected type |Int| does not
match the inferred |Bool|.  Similarly, the object term $\fun{x}xx$ and its
encoding |lam (\x -> app x x)| both fail occurs-checks in type checking.
Haskell's type checker also flags syntactically invalid object terms, such as if
we forget |app| somewhere above.

\begin{figure*}
\begin{floatrule}
\begin{code}
module type Symantics = sig

  type ('c, 'dv) repr

  val int : int  -> ('c, int) repr
  val bool: bool -> ('c, bool) repr

  val lam : (('c, 'da) repr -> ('c, 'db) repr) -> ('c, 'da -> 'db) repr
  val app : ('c, 'da -> 'db) repr -> ('c, 'da) repr -> ('c, 'db) repr
  val fix : (('c, 'da -> 'db) repr -> ('c, 'da -> 'db) repr) -> ('c, 'da -> 'db) repr

  val add : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val mul : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val leq : ('c, int) repr -> ('c, int) repr -> ('c, bool) repr
  val if_ : ('c, bool) repr -> (unit -> ('c, 'da) repr) -> (unit -> ('c, 'da) repr) -> ('c, 'da) repr

end
\end{code}
\end{floatrule}
\caption{A simple (Meta)OCaml embedding of our object language}
\label{fig:ocaml-simple}
\end{figure*}

To embed the same object language in (Meta)OCaml, we replace the type
class |Symantics| and its instances by a module signature |Symantics|
and its implementations.  Figure~\ref{fig:ocaml-simple} shows a simple
signature that suffices for this and the following sections.
Two differences between this signature and the |Symantics| class in
Haskell are: the additional type parameter |c|, an
environment classifier \citep{WalidPOPL03} required by MetaOCaml for
code generation in~\S\ref{compiler}; and the $\eta$-expanded type for
|fix| and thunk types in |if_| due to OCaml's being a call-by-value
language.

The functor below encodes our examples |test1| and $\mathit{power}$.
\begin{code}
module EX(S: Symantics) = struct
  open S
  let test1 () =
    app (lam (fun x -> x)) (bool true)
  let testpowfix () = 
    lam (fun x -> fix (fun self -> lam (fun n ->
    if_ (leq n (int 0)) (fun () -> int 1)
        (fun () -> mul x
          (app self (add n (int (-1))))))))
  let testpowfix7 = 
     lam (fun x -> app (app (testpowfix ()) x)
                       (int 7))
end;;
\end{code}
The dummy argument to |test1| and |testpowfix| is an artefact of
MetaOCaml, again related to monomorphism: in order for us to run a
piece of generated code, it must be polymorphic in its environment
classifier (the type variable |'c| in Figure~\ref{fig:ocaml-simple}).
The value restriction then dictates that
the definitions of our object terms must look syntactically like
values. Alternatively to giving the |()| argument, we could have used
the rank-2 record types of OCaml to maintain the necessary polymorphism.

Thus we represent an object expression in
OCaml as a functor from |Symantics| to an appropriate semantic domain. This
is essentially the same as the constraint |Symantics repr =>| in the
Haskell embedding.

\subsection{Two tagless interpreters}
\label{S:interpreter-RL}

Having abstracted our term representation over the interpreter, we are
now ready to present a series of interpreters.  Each interpreter is an
instance of the |Symantics| class in Haskell and a module implementing
the |Symantics| signature in MetaOCaml.

The first interpreter evaluates an object term to its value in the
metalanguage.  The module below interprets each object\hyp language
operation as the corresponding metalanguage operation.
\begin{code}
module R = struct
  (* absolutely no wrappers *)
  type ('c,'dv) repr = 'dv

  let int  (x:int)  = x
  let bool (b:bool) = b

  let lam  f        = f
  let app  e1 e2    = e1 e2
  let fix  f        =
    let rec self n = f self n in self

  let add  e1 e2    = e1 + e2
  let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then et () else ee ()

end
\end{code}

As in~\S\ref{ourapproach},
this interpreter is patently tagless, using neither a universal type nor
any pattern matching: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. To run our
examples, we instantiate the |EX| functor from~\S\ref{encoding} with this |R|
module.
\begin{code}
module EXR = EX(R)
\end{code}
Thus |EXR.test1 ()| evaluates to the untagged boolean value |true|.
In Haskell, we define
\begin{code}
newtype R a = R {unR::a}
instance Symantics R where ...
\end{code}
Although |R| looks like a tag, it is only
a |newtype|.  The types |a| and |R a| are represented differently
only at compile time, not at run time.  Pattern matching against~|R|
cannot ever fail and is compiled away.
In both OCaml and Haskell, it is obvious to the compiler that
pattern matching cannot fail, because there is no
pattern matching. Evaluation can only fail to yield a value
due to interpreting |fix|.
That is, the proposition below follows immediately from the soundness of
the metalanguage's type system.
\begin{prop}
If the term |e| encoded in the metalanguage has type~|t|,
then evaluating~|e| in the interpreter~|R| either continues
indefinitely or terminates with a value of the same type~|t|.
\end{prop}

For variety, we show another interpreter, which measures the \emph{size}
of each object term, defined as the number of term
constructors. The following is slightly abbreviated code (see the
accompanying source code for the complete definition).
\begin{code}
module L = struct
  type ('c,'dv) repr = int
  let int (x:int)  = 1
  let lam f        = f 0 + 1
  let app e1 e2    = e1 + e2 + 1
  let fix f        = f 0 + 1
  let mul e1 e2    = e1 + e2 + 1
  let if_ eb et ee = eb + et () + ee () + 1
end;;
\end{code}
Now the expression
\begin{code}
let module E = EX(L) in E.test1 ()
\end{code}
evaluates to |3|. This interpreter is not only tagless but also
total. It ``evaluates'' even seemingly divergent terms like
\begin{code}
app (fix (fun self -> self)) (int 1)
\end{code}

\begin{comment}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{comment}

\section{A tagless compiler (or, a staged interpreter)}\label{compiler}

Besides immediate evaluation, we can compile our object language
into OCaml code using MetaOCaml's staging facilities. MetaOCaml
represents future-stage expressions of type |t| at the
present stage as values of type |('c,t) code| where |'c| is the
environment classifier \citep{WalidPOPL03,calcagno-ml-like}. Code values are created
by a \emph{bracket} form |.<e>.|, which specifies that the expression |e| is to be
evaluated at a future stage. The \emph{escape} |.~e| must occur
within a bracket and specifies that the expression |e| must be evaluated
at the current stage; its result, which must be a code value, is
spliced into the code being built by the enclosing bracket. The \emph{run} form |.!e| evaluates
the future-stage code value~|e| by compiling and linking it at run-time.
The bracket, escape, and run are akin to
quasi-quotation, unquotation, and |eval| of Lisp.

Inserting brackets and escapes appropriately into the
evaluator~|R| above yields the compiler~|C| below.
% this code does not have the 'sv parameter. It shows up later.
\begin{code}
module C = struct

  type ('c,'dv) repr = ('c,'dv) code

  let int (x:int)   = .<x>.
  let bool (b:bool) = .<b>.

  let lam f         = .<fun x -> .~(f .<x>.)>.
  let app e1 e2     = .<.~e1 .~e2>.
  let fix f = 
    .<let rec self n = .~(f .<self>.) n in self>.

  let add e1 e2     = .<.~e1 + .~e2>.
  let mul e1 e2     = .<.~e1 * .~e2>.
  let leq x y       = .<.~x <= .~y>.
  let if_ eb et ee = 
    .<if .~eb then .~(et ()) else .~(ee ())>.

end
\end{code}
This is a completely straightforward staging of
|module R|.
This compiler simply produces
residual code with no optimization. For example, interpreting our |test1|
\begin{code}
let module E = EX(C) in E.test1 ()
\end{code}
gives the code value |.<(fun x_6 -> x_6) (true)>.|
of inferred type |('c, bool) C.repr|.

This compiler does not incur
any interpretive overhead: the
code produced for $\fun{x}x$ is simply |fun x_6 -> x_6| and does not
  call the interpreter, unlike the recursive calls to |eval0| and
  |eval| in the |L e| lines in \S\ref{tagproblem}.
The resulting code obviously contains no tags and no pattern matching.

We have also implemented this compiler in Haskell. Since Haskell
has no (convenient, typed) staging facilities, we had to emulate
them. To be precise, we defined a data type |ByteCode| with
constructors such as |Var|, |Lam|, |App|, |Fix|, and |INT|.
Whereas our representation of object terms uses HOAS,
our bytecode uses integer-named
variables to be realistic. We then define 
\begin{code}
newtype C t = C (Int -> (ByteCode t, Int)) 
\end{code}
where |Int| is the counter for creating fresh variable
names. We define the compiler by making |C| an instance of the
class |Symantics|. The implementation\footnote{The implementation uses
GADTs because we also wanted to write a typed interpreter for 
the \textsf{ByteCode} \emph{data type}.} is quite similar (but slightly more
verbose) than the corresponding MetaOCaml code given above. The
accompanying code gives the full details.

\section{A tagless partial evaluator}\label{PE}

Surprisingly, we can write a partial evaluator using the idea above,
namely to build object terms using ordinary functions rather than data
constructors.  We present this partial evaluator in a sequence of four
attempts, the last of which finally uses no universal type and no tags
for object types.  We then discuss residualization and binding-time
analysis.  Our partial evaluator is a modular extension to the evaluator
in~\S\ref{S:interpreter-RL} and the compiler in~\S\ref{compiler}, in
that it uses the former to reduce static terms and the latter to build
dynamic terms.

\ccshan{Move later:} The
partial evaluator does have tags: \emph{binding time} or \emph{phase}
tags, i.e. whether a value is static or dynamic.
Pattern-matching on this phase tag is always exhaustive.

\subsection{Avoiding polymorphic lift}
\label{S:PE-lift}

Roughly speaking, a partial evaluator interprets each object term either
to a static (present-stage) term, as produced by the evaluator~|R|, or
to a dynamic (future-stage) term, as produced by the compiler~|C|.  The
interpreter computes as much as possible statically.  Thus one might try
to define partial evaluation as the Haskell data type
\begin{code}
data P0 t = S0 (R t) | E0 (C t)
\end{code}
To extract a dynamic term from this type, we create the functions
\begin{code}
abstrInt :: P0 Int -> C Int
abstrInt (S0 r) = int (unR r)
abstrInt (E0 c) = c

abstrBool :: P0 Bool -> C Bool
abstrBool (S0 r) = bool (unR r)
abstrBool (E0 c) = c
\end{code}
We then start to define |P0| as an instance of the |Symantics| class.
\begin{code}
instance Symantics P0 where
  int  x = S0 (int x) (int x)
  bool x = S0 (bool x) (bool x)
  add (S0 e1) (S0 e2) = S0 (add e1 e2)
  add e1 e2 = E0 (add (abstrInt e1)
                      (abstrInt e2))
\end{code}
Integer and boolean literals are obviously immediate, present-stage
values. Addition yields a static term if and only if both operands
are static. If so, we use the evaluator~|R| to add the operands.
If not, we extract dynamic terms from the operands and add them
using the compiler~|C|.

Whereas |mul| and |leq| are as easy to define as |add|, we encounter
a problem with |if_|.  If the first argument to |if_| is a dynamic term
(type |C Bool|) and the second and third arguments are a static term
(type |R a|) and a dynamic term (type |C a|), then we need to convert
the static term to dynamic, but there is no polymorphic ``lift''
function |a -> C a| to send a value to the future stage.

Our |Symantics| class includes only separate lifting methods |bool| and
|int|, not a parametrically polymorphic lifting method, for good reason:
When compiling to a first-order target language such as machine code,
booleans, integers, and functions may well be represented differently.
Thus polymorphic lift cannot be compiled without intensional type
analysis.  To avoid the need for polymorphic lift, we turn to
\citearound{'s technique}\citet[see also \citealp{sumii-hybrid}]{asai-binding-time}:
build a dynamic term
alongside every static term.

\subsection{Delaying binding-time analysis}
\label{S:PE-problem}

We switch to the Haskell data type
\begin{code}
data P1 t = P1 (Maybe (R t)) (C t)
\end{code}
so that a partially evaluated term |P1 t| always contains a dynamic
component and sometimes contains a static component.  The two
alternative constructors of a |Maybe| value, |Just| and |Nothing|,
tag each partially evaluated term with a phase: either present or
future.  This tag is not an object type tag: all pattern matching below
is exhaustive.  Because the future-stage component is always present, we
can now define the polymorphic function
\begin{code}
abstr1 :: P1 t -> C t
abstr1 (P1 _ dyn) = dyn
\end{code}
to extract it without requiring polymorphic lift into~|C|.  We then try
to define the interpreter |P1|---and get as far as the first-order
constructs of our object language, including |if_|.
\begin{code}
instance Symantics P1 where
  int  x = P1 (Just (int x)) (int x)
  bool b = P1 (Just (bool b)) (bool b)
  add (P1 (Just n1) _) (P1 (Just n2) _)
    = int (unR (add n1 n2))
  add e1 e2 = P1 Nothing
    (add (abstr1 e1) (abstr1 e2))
  -- mul and leq are analogous and elided
  if_ (P1 (Just s) _) et ef
    = if unR s then et else ef
  if_ eb et ef = P1 Nothing
    (if_ (abstr1 eb) (abstr1 et) (abstr1 ef))
\end{code}

When we come to functions, however, we stumble.  According to our
definition of~|P1|, a partially evaluated object function, such as the
identity $\fun{x}x$ embedded in Haskell as |lam (\x -> x)|\texttt{ ::
}|P1 (a -> a)|, consists of a dynamic part (type |C1 (a -> a)|) and
maybe a static part (type |R1 (a -> a)|).  The dynamic part is useful
when this function is passed to another function that is only
dynamically known, as in $\fun{k}k(\fun{x}x)$.  The static part is
useful when this function is applied to a static argument, as in
$(\fun{x}x)\True$.  Neither part, however, lets us \emph{partially}
evaluate the function, that is, compute as much as possible statically
when it is applied to a mix of static and dynamic inputs.  For example,
the partial evaluator should turn $\fun{n}(\fun{x}x)n$ into $\fun{n}n$
by substituting $n$ for~$x$ in the body of $\fun{x}x$ even though $n$ is
not statically known.  Even the same static function, applied to
different static arguments, can give both static and dynamic results: we
want to simplify $(\fun{y}x\times y)0$ to~$0$ but $(\fun{y}x\times y)1$
to~$x$.

To enable these simplifications, we delay binding-time analysis (BTA)
for a static function until it is applied, that is, until |lam f|
appears as the argument of |app|.  To do so, we have to incorporate |f|
as it is into the |P1| data structure: applying the type constructor
|P1| to a function type |a -> b| should yield one of
\begin{code}
data P1 (a -> b) = S1 (P1 a -> P1 b)
                 | E1 (C (a -> b))

data P1 (a -> b) = P1 (Maybe (P1 a -> P1 b))
                      (C (a -> b))
\end{code}
That is, we need a nonparametric data type, something akin to
type-indexed functions and type-indexed types, which
\citet{oliveira-typecase} dub the \emph{typecase} design pattern.

Thus typed partial evaluation, like typed CPS transformation,
inductively defines a map from source types to target types that
performs case distinction on the source type.  The connection with CPS
is not an accident, as we shall see in~\S\ref{S:CPS}.

\subsection{Eliminating tags from typecase}
\label{S:PE-GADT}

Two common ways to provide typecase in Haskell are
GADTs and type-class functional dependencies
\citep{oliveira-typecase}.  These
methods are equivalent, and here we show the GADT way; |incope1.hs|
in the accompanying source code shows the latter.

We introduce a GADT with four data constructors.
\begin{code}
data P t where
  VI :: Int  -> P Int
  VB :: Bool -> P Bool
  VF :: (P a -> P b) -> P (a -> b)
  E  :: C t -> P t
\end{code}
The constructors |VI|, |VB|, and |VF| build static terms (like |S0|
in~\S\ref{S:PE-lift}), and |E| builds dynamic terms (like |E0|).  However,
the type |P t| is no longer parametric in~|t|: the constructor |VF| takes an
operand of type |P a -> P b| rather than |a -> b|. As before, we define a
function to extract a future-stage computation from a value of type |P t|.
\begin{code}
abstr :: P t -> C t
abstr (VI i) = int i
abstr (VB b) = bool b
abstr (VF f) = lam (abstr . f . E)
abstr (E x)  = x
\end{code}
The cases of this function |abstr| are type-indexed.  In particular, the |VF f|
case uses the method |lam| of the |C| interpreter to compile~|f|.

We may now make |P| an instance of
|Symantics| and implement the partial evaluator as follows. We elide
|mul|, |leq|, |if_|, and |fix|.
\begin{code}
instance Symantics P where
  int x  = VI x
  bool b = VB b
  add (VI n1) (VI n2) = VI (n1 + n2)
  add e1 e2 = E (add (abstr e1) (abstr e2))
  lam = VF
  app (VF f) ea = f ea
  app (E f)  ea = E (app f (abstr ea))
\end{code}
The implementations of |int|, |bool|, and |add| are like
in~\S\ref{S:PE-problem}.
The interpretation of |lam f| just wraps the
HOAS function |f| into the result. We can always compile |f| to a code value,
but we delay it to apply |f| to concrete arguments. The interpretation of
|app ef ea| checks to see if |ef| is such a delayed
HOAS function |VF f|. If it is, we apply |f| to the
concrete argument |ea|, giving us a chance to perform static
computations (see example |testpowfix7| in~\S\ref{S:PE-solution}). If |ef| is a
dynamic value |E f|, we residualize.

This solution using GADTs works but is not quite satisfactory. First, it
cannot be ported to MetaOCaml as GADTs are unavailable there.  Second,
the problem of nonexhaustive pattern\hyp matching reappears in |app|
above: the type |P t| has four constructors, of which pattern\hyp
matching in |app| uses only |VF| and~|E|. One may say that the
constructors |VI| and |VB| obviously cannot occur because they do not
construct values of type |P (a -> b)| as required by the type of |app|.
Indeed the compiler could reason thus, but it may not; GHC for one
issues warnings.  Although this point may seem minor, it is the heart of
the tagging problem and the purpose of tag elimination. A typed tagged
interpreter contains many pattern\hyp matching forms that look partial
but are total in reality. The goal is to make this totality
\emph{syntactically} apparent.

\subsection{The ``final'' solution}
\label{S:PE-solution}
Let us re-examine the problem in~\S\ref{S:PE-problem}. What we
would ideally like is to write
\begin{code}
data P t = P (Maybe (repr_pe t)) (C t)
\end{code}
where |repr_pe| is the type function defined
% inductively because P below depends on repr_pe
by
\begin{code}
repr_pe Int      = Int
repr_pe Bool     = Bool
repr_pe (a -> b) = P a -> P b
\end{code}
Although we can use type classes to define this type function
in Haskell, that is not portable to MetaOCaml. However,
these three typecase alternatives are already present in existing
methods of |Symantics|.
A simple and more portable solution thus emerges: we bake |repr_pe| 
into the signature |Symantics|. Switching to MetaOCaml to demonstrate this
portability, we recall from Figure~\ref{fig:ocaml-simple} in~\S\ref{encoding} that the |repr| type
constructor took two arguments |'c| and~|'dv|. We add an argument
|'sv| for the result of applying |repr_pe| to~|'dv|.
Figure~\ref{fig:ocaml} shows the new |Symantics| signature.
\begin{figure*}
\begin{floatrule}
\begin{code}
module type Symantics = sig

  type ('c,'sv,'dv) repr

  val int : int  -> ('c,int,int) repr
  val bool: bool -> ('c,bool,bool) repr

  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x) -> ('c,'x,'da->'db) repr
  val app : ('c,'x,'da->'db) repr -> (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x)
  val fix : ('x -> 'x) -> (('c, ('c,'sa,'da) repr -> ('c,'sb,'db) repr, 'da->'db) repr as 'x)

  val add : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val mul : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val leq : ('c,int,int) repr -> ('c,int,int) repr -> ('c,bool,bool) repr
  val if_ : ('c,bool,bool) repr -> (unit -> 'x) -> (unit -> 'x) -> (('c,'sa,'da) repr as 'x)

end
\end{code}
\end{floatrule}
\caption{A (Meta)OCaml embedding of our object language that supports partial evaluation}
\label{fig:ocaml}
\end{figure*}

\begin{figure}
\begin{floatrule}
\begin{code}
module P = struct

  type ('c,'sv,'dv) repr = {st: 'sv option;
                            dy: ('c,'dv) code}

  let abstr {dy = x} = x
  let pdyn x = {st = None; dy = x}

  let int  (x:int)  = {st = Some (R.int x);
                       dy = C.int x}
  let bool (x:bool) = {st = Some (R.bool x);
                       dy = C.bool x}

  let add e1 e2 = match e1, e2 with
  | {st = Some 0}, e | e, {st = Some 0} -> e
  | {st = Some m}, {st = Some n} -> int (R.add m n)
  | _ -> pdyn (C.add (abstr e1) (abstr e2))

  let if_ eb et ee = match eb with
  | {st = Some b} -> if b then et () else ee ()
  | _ -> pdyn (C.if_ (abstr eb) 
                     (fun () -> abstr (et ()))
                     (fun () -> abstr (ee ())))

  let lam f =
  {st = Some f; 
   dy = C.lam (fun x -> abstr (f (pdyn x)))}

  let app ef ea = match ef with
  | {st = Some f} -> f ea
  | _ -> pdyn (C.app (abstr ef) (abstr ea))

  let fix f = 
    let fdyn = C.fix (fun x -> abstr (f (pdyn x)))
    in let rec self = function
       | {st = Some _} as e -> app (f (lam self)) e
       | e -> pdyn (C.app fdyn (abstr e))
       in {st = Some self; dy = fdyn}

end
\end{code}
\end{floatrule}
\caption{Our partial evaluator (eliding \texttt{mul} and \texttt{leq})}
\label{fig:pe}
\end{figure}

The interpreters |R|, |L| and~|C| above only use the old
type arguments |'c| and~|'dv|, which are treated by the new signature
in the same way.  Hence all that needs to change in these interpreters
to match the new signature is to add a phantom type
argument~|'sv| to~|repr|.
For example, the compiler |C| now begins
\begin{code}
module C = struct
  type ('c,'sv,'dv) repr = ('c,'dv) code
  (* the rest is the same *)
\end{code}
In contrast, the partial evaluator~|P| relies on the type argument |'sv|.

Figure~\ref{fig:pe} shows the partial evaluator~|P|.
Its type |repr| literally expresses the type equation for |repr_pe| above.
The function |abstr|, as in~\S\ref{S:PE-GADT},
extracts a future-stage code value from a result of
partial evaluation.  Conversely, the function |pdyn| coerces a
code value into a partial\hyp evaluation result. As
in~\S\ref{S:PE-problem}, we build dynamic terms alongside
static ones, if any, to avoid
polymorphic lift. The implementations of
|add| and |mul| are a bit more sophisticated, taking advantage 
respectively of the implicit monoid and ring structure of the 
underlying domain.
The function |build| checks to see if
its two arguments |e1| and |e2| are static expressions. If so, the
corresponding static computation (with the help of the intrepreter
|R|) is performed. The function |buildsim_monoid| also checks if one argument
is static and is the neutral element of the monoid structure.
|ring| also worries about the full ring structure rather than
just the monoid structure.  These are put here to illustrate how we 
would go about writing general algebraic simplification routines that
would mimic various compiler optimizations, and these would be quite
beneficial in a large language with more base types.

When doing partial evaluation on |fix|, we made a deliberate decision to `go
all the way' -- so long as the computation can proceed at the PE stage,
we do that rather than residualizing. In the
accompanying source code, we have also implemented an alternative
where |fix| is unrolled only once and then we residualize. We prefer the
first approach, even though it may cause
non-termination at the PE stage -- but \emph{only} if the computation
we are partially evaluating is non-terminating, and enough static
information is available to trigger this non-termination.  Our
treatment of |fix| may lead to code bloat in the residual
program. The techniques of~\citet{SwadiTahaKiselyovPasalic2006} can
be used to deal with this issue.

Given the above Symantics, our running test now evaluates
\begin{code}
let module E = EX(P) in E.test1 ();;
\end{code}
to
\begin{code}
- : ('a, bool, bool) P.repr = 
   {P.st = Some true; P.dy = .<(true)>.}
\end{code}
It is instructive to compare this result with that of the |C|
evaluator. We observe that the beta-redex has now been 
performed. The result is the static value |true|.
More interesting is the partial evaluation of testpowfix7. The |C|
interpreter evaluates |testpowfix7| to the future-stage code with many
apparent beta-redexes.
\begin{code}
let module E = EX(C) in E.testpowfix7;;
 - : ('_a, int -> int) C.repr =
.<fun x_1 -> (((fun x_2 ->
  let rec self_3 =
   fun n_4 ->
    ((fun x_5 -> if (x_5 <= 0) then 1 
                 else (x_2 * (self_3 (x_5 + (-1)))))
      n_4) in
  self_3) x_1) 7)>.
\end{code}
The result of the |P| interpreter is different:

\begin{code}
let module E = EX(P) in E.testpowfix7;;
 - : ('_a, ('_a, int, int) P.repr -> ('_a, int, int) P.repr, 
      int -> int) P.repr = 
 {P.st = Some <fun>;
  P.dy = .<fun x_1 -> 
     (x_1 * (x_1 * (x_1 * (x_1 * (x_1 * (x_1 * x_1))))))>.}
\end{code}
as expected from the power example.

Unlike the GADT approach in the previous subsection, there is no
longer any seemingly partial pattern-matching. All pattern-matching
occurs in the partial evaluator only (to check if the current value 
is static or dynamic), and is 
phase rather than a type tag match. Furthermore the matching
\emph{patently} exhaustive. That is, totality is assured and it is
assured \emph{syntactically}, in a way that is apparent to the
compiler.

One should note that the partial evaluator has fully reused the code
of the compiler and the interpreter. It is truly the `composition' of the
interpreter and the compiler.   This is a simpler situation than
in \citet{SperberThiemann:TwoForOne} where a partial evaluator and
a compiler are `composed', but the general ideas are similar.

Lastly, we should comment on the approach taken in the partial evaluator.
It is an online, typed, polyvariant partial evaluator.  It has no
termination analysis, and as written may diverge (but only on divergent
programs) as we have decided to aggressively unroll loops.

\section{Variations and discussion}\label{discussion}

We show that our |Symantics| and generally our approach accomodates
a number of variants without any (new) difficulties.  In particular,
we show how a a call-by-name CPS interpreter, a call-by-value CPS
program transformation, as well as how to deal with imperative features.
Finally, we comment on the requirements on the type system to support 
our approach.

\subsection{Call-by-name CPS interpreters}\label{S:CPS}

The object language generally inherits the evaluation strategy from
the metalanguage. Evaluating an application |e1 e2| in OCaml requires
the evaluation of |e2| before performing the application. Therefore,
when evaluating the encoding of the object language application 
|app e1 e2|, |e2| will always have to be evaluated. One may wonder if it is
possible to represent a call-by-name (CBN) object language in a
call-by-value (CBV) metalanguage. The answer, given already by 
\citet{reynolds-definitional,reynolds-relation} and \citet{PlotkinCBN}, is continuation-passing style (CPS), which they
specifically introduced 
to make the evaluation strategy of a definitional interpreter
independent of the strategy of the metalanguage. We demonstrate the
same independence in the typed setting \oleg{mention the quote from
Reynolds that typed setting would be nice?}\jacques{I think so, but from
which paper? The one ReynoldsPlotkin paper is typed}\ccshan{I think Oleg
meant ``It should also be possible to define languages with a highly
 refined syntactic type structure. Ideally, such a treatment should be
 metacircular, in
 the sense that the type structure used in the defined language should be adequate for the
 defining language'' \citep{reynolds-definitional}, but perhaps Jacques
 meant another paper by ``ReynoldsPlotkin''?}. Specifically, we
demonstrate CBN evaluation for our object language in OCaml, which is
call-by-value. We do so by building a CBN CPS interpreter for the object
language.

The CBN CPS interpretation was introduced by \citet{PlotkinCBN}. As in
the more familiar CBV CPS, the interpretation of a value is a function
accepting a continuation argument |k| and yielding the answer
returned by that continuation.
\begin{code}
let int (x:int) = fun k -> k x
let add e1 e2 = 
  fun k -> e1 (fun v1 -> e2 (fun v2 -> k (v1+v2)))
\end{code}
In both cases, the returned value has type 
|(int -> 'w)->'w| where |'w| is the (polymorphic) answer type.

The interpretation of abstraction and application in CBN CPS is
different from CBV CPS and is as follows:
\begin{code}
let lam f = fun k -> k f
let app e1 e2 = fun k -> e1 (fun f -> (f e2) k)
\end{code}
In the tell-tale sign of CBN, in interpreting the application
we do not `evaluate' the argument e2, that is, we do apply it to the
continuation. Rather, we pass the unevaluated |e2| to the abstraction.
The result of |lam (fun x -> add x (int 1))| has the type
|((((int->'w1)->'w1) -> ((int->'w1)->'w1)) -> 'w2) -> 'w2|.
We want to collect the above interpretation functions into a module
with the |Symantics| signature, so as to fit the CBN CPS interpreter within our
general framework. Alas, we encounter the same problem we had in
Section~\ref{S:PE-problem}, in writing a type equation for the |repr|
type that can represent both the type of |int i| and the type of 
|lam f|. We cannot say that the result of interpreting an object
expression of type |t| is |(t->'w)->'w| because if |t| happens to
be an arrow type (such as |int->int| in the above example), the result
type obviously does not fit the desired pattern. We need a type
function with a typecase distinction. In Section~\ref{S:PE-solution} we have
shown how to encode such a function, using the extra type
argument |'sv| of the |repr| type. Furthermore, we notice that the type 
function |repr_pe| needed for the partial evaluator 
(in~\S\ref{S:PE-solution}) is precisely the same type function we
need for CBN CPS. This gives us for the following CBN CPS interpreter,
with the |Symantics| signature:

\begin{code}
module RCN = struct
  type ('c,'sv,'dv) repr = 
     {ko: 'w. ('sv -> 'w) -> 'w}
  let int (x:int) = {ko = fun k -> k x}
  let add e1 e2 = 
    {ko = fun k -> e1.ko (fun v1 -> 
                   e2.ko (fun v2 -> k (v1+v2)))}
  let if_ eb et ee = 
    {ko = fun k -> eb.ko 
         (fun vb -> if vb then (et ()).ko k 
                    else (ee ()).ko k)}
  ...
  let lam f = {ko = fun k -> k f}
  let app e1 e2 = 
     {ko = fun k -> e1.ko (fun f -> (f e2).ko k)}
  let fix f = 
     let rec fx f n = app (f (lam (fx f))) n in 
     lam(fx f)

  let get_res x = x.ko (fun v -> v)
end
\end{code}

We have made our interpreter fully polymorphic over the answer type.
We could have just as easily made |RCN| a functor, parameterized over
the answer type -- as is common when writing CPS in SML. In the
present case, we chose to use the higher-rank polymorphism of offered by
OCaml record types.

Because |RCN| has the signature |Symantics|, we can instantiate our previous
examples with it, and all works as expected.  More interesting
is the following example.
\begin{code}
module EXS(S: Symantics) = struct
 open S
 let diverg () = 
   app (lam (fun x -> int 1)) 
       (app (fix (fun self -> self)) (int 2))
end
\end{code}

If we interpret |EXS| with the |R| interpreter of
\S\ref{S:interpreter-RL} realized in OCaml
\begin{code}
let module M = EXS(R) in M.diverg ();;
\end{code}
we never get any result because the above diverges. In contrast, if we use
the CBN interpreter
\begin{code}
let module M = EXS(RCN) in RCN.get_res (M.diverg ())
\end{code}
we get the result |1|.

\oleg{mention some formal results? Like indifference theorems by
  Plotkin? And say we generalized their result to the type setting?
  Mention this in the conclusions?}
\jacques{you have strayed outside what I know -- what Plotkin results?
  I always consider mentionning formal results a good thing}

\subsection{CBV CPS transformers}

We can easily change our CBN CPS interpreter into a CBV CPS
interpreter, by replacing only one definition:
\begin{code}
module RCV = struct
  include RCN
  let lam f = {ko = 
    fun k -> k (fun e -> e.ko 
         (fun v -> f {ko = fun k -> k v}))}
end;;
\end{code}
Now when we apply an abstraction to an argument |e|, we always force
the evaluation of that argument before proceeding. This approach is in
line with \citet{reynolds-relation} (albeit in a typed setting). The interpreter RCV
is useful for achieving CBV evaluation of the object language
no matter whether the metalanguage is CBV or CBN.

In this section however we show a more general approach to CBV CPS:
a CPS transformer. We take any implementation of |Symantics| (interpreter,
compiler, partial evaluator) and make it evaluate the CPS language
instead. We can view interpreter transformation as a
transformation on the object language, from direct to
continuation-passing style.

The transformation is canonical, as the following (abbreviated) code
shows
\begin{code}
module CPST(S: Symantics) = struct
  let int i = S.lam (fun k -> S.app k (S.int i))
  let add e1 e2 = S.lam (fun k ->
    S.app e1 (S.lam (fun v1 ->
    S.app e2 (S.lam (fun v2 -> S.app k (S.add v1 v2))))))
  let if_ ec et ef = S.lam (fun k ->
    S.app ec (S.lam (fun vc ->
    S.if_ vc (fun () -> S.app (et ()) k) (fun () -> S.app (ef ())
    k))))
  ...
  let lam f = S.lam (fun k -> S.app k (S.lam (fun x ->
    f (S.lam (fun k -> S.app k x)))))
  let app e1 e2 = S.lam (fun k -> 
    S.app e1 (S.lam (fun f ->
    S.app e2 (S.lam (fun v -> S.app (S.app f v) k)))))
  let fix f  = S.fix (fun self -> (f self))
end
\end{code}
The code literally establishes a mapping from CPS interpretations to
(direct) interpretations, the latter performed by 
the base interpreter |S|.

A reader may notice that the above module does not define the type
equation for |repr| and so it does not have the signature Symantics.
The reason for that is again the result type of |lam f|. Whereas
|int i| or |add e1 e2| have the (abbreviated) 
type |('c,..., (int -> 'w) -> 'w) S.repr|,
the result of |lam (fun x -> add x (int 1))| has type
|('c,...((int -> (int -> 'w1) -> 'w1) -> 'w2) -> 'w2) S.repr|. 
Hence, to write the type equation for |CPST.repr| we need a 
type-discriminating
function similar to the type function |repr_pe| of
\S\ref{S:PE-solution}. Alas, the function we need here is
similar but not identical to |repr_pe|. Thus we need to add 
another type variable to the |repr| type and modify the Symantics
signature accodringly. Again, the terms in previous Symantics module
stay unchanged, but the |repr| type equations in those structures have to
account for a new (phantom) type variable.

To save space, we show a simplified and less principled approach that
lets us use the module |CPST| as it is. Because the module does not
have the signature Symantics, we cannot apply the |EX| functor to it.
But we can write the tests nevertheless:
\begin{code}
module T = struct
 module M = CPST(C)
 open M
 let test1 () = app (lam (fun x -> x)) (bool true)
 let testpowfix () = ... (* the same as before *)
 let testpowfix7 = (* the same as before *)
    lam (fun x -> app (app (testpowfix ()) x) (int 7))
end;;
\end{code}
That is, we instantiate |CPST| with the desired base intrepreter (the
compiler |C| in the case above) so that we can use the result |M| to
interpret object level terms. The sample terms are \emph{exactly}
as we used before. We just copied them over. That copying step is the
price we pay for the simplified treatment. We shall also encounter this
copying when discussing the self-interpreter in the next section. We
will see then that the copying step is not frivolous, but represents
a profound instance of 
``plugging the hole'', which is one of the many faces of polymorphism.

It is instructive to see the result of evaluating a few examples. With 
|CPST| instantiated with a straightforward compiler |C| as above, our
running test |T.test1 ()| gives
\begin{code}
.<fun x_5 -> ((fun x_2 -> 
        (x_2 (fun x_3 -> fun x_4 -> (x_4 x_3))))
        (fun x_6 -> ((fun x_1 -> (x_1 (true))) 
        (fun x_7 -> ((x_6 x_7) x_5)))))>.
\end{code}

which shows the naive CPS transform of the original code. The result is
quite unoptimal with several apparent $\beta$-redexes. 
By instantiating |CPST| with the |P| evaluator instead, our
running test gives the expected
\begin{code}
  P.repr = {P.st = Some <fun>; P.dy = .<fun x_5 -> (x_5 (true))>.}
\end{code}

\subsection{State and imperative features}

We can easily modify the CBV CPS transform to pass a piece of state
along with the continuation. This lets us support mutable state. We
can also use reference variables of the metalanguage to emulate
reference variables in the object language.
\oleg{Should we have an example in the code?}
\jacques{In the code, yes.  Probably not in the paper, it is getting 
long as it is}

\subsection{Lifting and Functor}

It is perhaps worthwhile noting that 
if we were to add a method 'lift' to |Symantics| with type
|lift :: a -> repr a| (the polymorphic lift of \citet{xi-guarded}, 
of MetaML and
MetaOCaml: CSP)
then |Symantics repr| implies |Functor repr|.  This can be seen from
\begin{code}
instance Semantics repr => Functor repr where
  -- fmap :: (a->b) -> repr a -> repr b
  fmap f ra = app (lift f) ra
\end{code}
Of course, as we have mentioned earlier in~\S\ref{S:PE-lift}, 
addin such a polymorphic lift would appear to introduce more
problems than it solves.

\subsection{Translucent types}

We must formulate some propositions: a typed term makes progress in
any interpreter. The reason it is typed in any interpreter:
|forall r. Symantics r => r tau| directly says that in every model the
term is typed. 

The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
|app _add (app _int 1) (app _int 2)| where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is

$$ 
  (\ZZ \rightarrow r \ZZ) \rightarrow
  (\BB \rightarrow r \BB) \rightarrow
  \forall \alpha \beta. (r \alpha \rightarrow r \beta)
      \rightarrow r (\alpha\rightarrow\beta) \rightarrow ... r \tau
$$
% (Int -> r Int) ->
% (Bool -> r Bool) ->
% (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau

or, if we denote the sequence of initial arguments as S r, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.

The attempt to encode self-interpreter brings the |*->*| polymorphism
the the forefront, as we shall now see.

\section{Self-interpretation}\label{selfinterp}

A self-interpreter |si| is an
object-language term such that, for any object-language term e,

\begin{code}
  si "e"    is observationally equivalent to    e
\end{code}

where |"e"| is an encoding of |e|.  In our approach, we want to encode
each object-language term as a function from a record%
\footnote{Thanks to currying, the object language need not actually support
records.}  of interpreter methods to a generated representation.  
In the case of a self-interpreter, that
record is simply |si|!  So we should define a self-interpreter |si| to be an
object-language term such that, for any object-language term |e|,

\begin{code}
  "e" si    is observationally equivalent to    e
\end{code}

A self-interpreter is then just like the interpreter
|R| in Section 2, except written at the object rather than the meta level.

It is wortwhile noting that, in our class |Symantics|, |add|
is not a term since it does not have type |repr t| for any |t|. We could
have introduced |add| to be of type |repr (Int->Int->Int)| -- but we
didn't. Thus |add| is a special form, which is to be applied in the
meta-language using the meta-language application syntax (i.e. white
space) rather than in the object language (i.e. via |app|). 

\oleg{Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).}

Self-interpretation in our framework is somewhat
tricky due to the need for more polymorphism than is easily available
in the meta-language.  An encoded term should
be polymorphic in the interpreter type-constructor "r".  Furthermore, the
functions "lam", "app", etc. should be polymorphic in the object
types "a" and "b".  Thus the object language seems to require rank-2
polymorphism; and so type annotations will be inevitable.

To neatly bypass the need for rank-2 polymorphism, we use the notion
of let-polymorphism and a `hole', as follows.  A partial evaluator (or
a compiler or interpreter) is an evaluation context that (let-)binds
variables named "app", "lam", etc.  For the object language to
take advantage of let-bound polymorphism in the metalanguage, it
is crucial that we encode the object-level "let" using the meta "let".  

We also (unfortunately need to) define a self-interpreter slightly
differently.  A self-interpreter |SI[]| is an object expression with a
hole |[]| waiting to be plugged with the encoding of an object expression.
The self-interpreter binds the free variables |_lam|, |_app|, |_int|, |_bool|,
etc. in the plugged encoding.  We require |SI["E"]| to be equivalent to |E|,
where |"E"| is the object encoding of the object expression |E|.

This strategy of course limits what kinds of processing on encoded
object expressions can be expressed in the object language.  For
example, unlike if we had simply defined |"E"| to be the \emph{identity
encoding} E, we \emph{can} write a size-measure interpreter |SZ[]|, 
which like |SI[]| is
an object expression with a hole |[]| waiting to be plugged with 
an |"E"|.
However, this notion of size cannot distinguish |let x = E1 in E2[x]|
from |"E2[E1]"|.  Recall that one way to type-check polymorphic let is:
\begin{code}
        E1:T    E2[E1]:T'
    ------------------------
    let x = E1 in E2[x] : T'
\end{code}
That analogy shows that the self-interpreter with a hole is a
euphemism for a higher-rank abstraction. That makes it clear where
the difficulty lies, and how to simulate the higher-rank application
(that is, higher-rank arrow elimination) `by hand'.

We also need to duplicate the encoding of an object expression if we
want to process the same object expression in multiple ways (e.g., both
measure its size and interpret it).  But all this is consistent with (in
fact, amplifies) our `coalgebraic/final/cogen' spirit, so our revised
notion of self-interpretation (and, some might say, of size measurement)
may just squeak by the reader's skepticism as we explain our desire for
kind polymorphism.

\oleg{I guess the let-and-hole argument works out if we don't typecheck the
encoded terms. We can't typecheck the interpreter with the hole
either. We insert the encoded term into the interpreter -- and then
typecheck the whole thing. That'll work. Yet a bit unsatisfactory...}

We achieve
self-interpretation in that the following evaluation context in the
object language defines an interpreter for encoded object terms.
\begin{code}
  let lam = \f -> f in
  let app = \f x -> f x in
  let add = \m n -> m + n in
  let int = \i -> i in
  [hole]
\end{code}
The obvious notion of optimality in this setup is easy to achieve.  For
example, given the context above, our partial evaluator turns
\begin{code}
  app (lam (\x -> add x (int 1))) (int 3)
\end{code}
into the number 4, as desired.  We can also encode the self-interpreter
as the following evaluation context in the object language.
\begin{code}
  let lam = lam (\f -> f) in
  let app = lam (\f -> lam (\x -> app f x)) in
  let add = lam (\m -> lam (\n -> add m n)) in
  let int = lam (\i -> i) in
  [hole]
\end{code}
The "let" and the hole "[hole]" are meta-constructions, which must map to
themselves in any interpretation.  

The 'result' of applying an interpreter to itself needs to be an 
interpreter too!  In fact, if it is Jones-optimal, then you should be 
able to apply it to itself again and get the same answer.  
Our code does that. First, let |twice_inc_3_e| be the
metalanguage encoding of the object term |si["e"]|,
where si is the self-interpreter for the needed components of the object
language (only |lam|, |app|, |add|, and |int| in this case), |""| means the
object-to-object encoding, and |e| is the object term
\begin{code}
  let twice_ f x = f (f x) in let inc_ n = n + 1 in twice_ inc_ 3
\end{code}
Second, |twice_inc_3_ee| is the metalanguage encoding of the object term
|si["si[e]"]|
which is the same as
|si["si"["e"]]|.
The partial evaluator yields 5 on both |twice_inc_3_e| and 
|twice_inc_3_ee|,
as desired.

\jacques{But how 
to you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?}
\oleg{Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.}
Again, what is different from the above is that we can
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).

\section{Related work}\label{related}
Walid's Jones optimality paper. See his own ICFP02 paper that
described what he achieved then and what he \emph{did not}. He did not
eliminate all the tags. Ken's and mine impression of it:
(i) we can't write a \emph{direct} interpreter of the type language
           -- the only known to them, and \emph{partial} solution, was 
              that by Zhe Yang;
(ii) we have to use indirect interpreter, which needs the universal
           type and hence tagging;
(iii) that means any self-interpreter must have tags
(iv) that means not Jones-optimal.
It seems we should attack the point1 (in which case we don't have to
bother with self-interpreters). We say that we \emph{can} write the direct
interpreter, and without GADT or their equivalents (typeclasses).
Meta-language computes types, and the compiler and interpreter respect
those types and needs no tags.


Walid's ICFP02

Walid's PEPM07 (Concoqtion paper)? At leats at the Concoqtion
presentation at WG2.11, Emir used tagless interpreter as the main and
only example. \jacques{but at the PEPM07 presentation, Walid did not
quite do it that way.  He presented Concoqtion more as an exploration
of (an obvious but difficult) part of the design space for programming
languages.}

Danvy's self-interpreter
Ken has read Danvy et al's paper and said that the paper
doesn't seem to even make the claim that the source language is
typed. The interpreter is typed, but the source language doesn't seem
to be.
In sharp contrast to Danvy's paper, all the evaluators in our case
cannot produce any pattern-match error! Except for the partial
evaluator, there are NO patterns to match. And this is critical for
tag elimination -- pattern matching means there are no tags. And in
PE, the pattern-match deals with phases rather than with types. Also,
in our case the exhaustiveness of the pattern match is apparent to the
compiler -- that's why there are no tags.

Simon Peyton-Jones GADTs (the motivation for GADT is tagless
interpreter, but only for the first-order language)

Xi's POPL03 (He uses a higher-order language, one of the motivations
for his GRDT)

Asai; 
It [dynamic part when dealing with bool, int, lam in PE] is
Asai's trick for reducing the amount of reify/reflect back-and-forth
in a PE for an untyped language that allows you back-and-forth.  Or at
least that's how I read those papers.
Asai's approach is related to our PE (emulating typelevel functions):
Ken: ``In some sense we're just observing that Asai's code type-checks
-- in Hindley-Milner as soon as we deforest the static code
representation.''

Sumii et al.

"Boxes go Bananas" by Washburn and Weirich (intended or otherwise) Ken
wrote about this: ``but a glance reveals no type classes.  Do you mean
then that if we hide the definition |newtype R a = R a| from a client,
and export only the interpreter |instance Symantics R R|, then the
client would see R as an HOAS datatype a la Washburn and Weirich and
only be able to create parametric ("non-exotic") terms?''  Jacques
commented: ``I mean the HOAS parts, and how to deal with it (like in
the instance for |Symantics R R|).  There are other nice tricks for
dealing with HOAS in that paper worth a closer look.  It's all about
plain polymorphism, not type classes, but there are still some clever
ideas there, and they are about issues of tagging (an lack thereof).''

Typing Dep Typing plus two messages on Haskell-Cafe: typechecking 
from untyped to the typed form, ready to be interpreted. Walid's ICFP02
uses dependent types

Regarding our implementation of the type-level function |'sv -> 'dv|.
Gibbons et al paper on
the implementations of typecase may be relevant. I have not read it,
but I have the feeling some of their typecase designs may be
relevant (at least to cite). 
However, the typecase paper relies on having a 
bijection between types -- our |'sv| and |'dv| in this case, which is not 
our situation.  We have a one-sided view of things, where |'sv -> 'dv| is 
easy, and |'dv| "perfectly reflects" an |'sv|, but you still can't get at 
that |'sv|.  \jacques{Could we say that we have a meta-level Galois
injection?  We have one direction completely covered at the object
level (and so easy to lift), but 'dv to 'sv can only be done by
an outside mechanism}


Self-Interpretation and Reflection in a Statically Typed Language 
\url{http://webpages.cs.luc.edu/~laufer/papers/reflection93.pdf}
He uses the SK language and some typeclasses. Their interpreter is a
meta-circular interpreter -- rather than self-interpreter. We can
claim to have implemented a far better interpreter for a far richer
language.

Thiemann, cogen in six lines (for the interpreter/compiler work)?

\section{Conclusions}\label{conclusion}

We were dissatisfied with current approaches to DSL embedding: either
the performance was hurt by tagging/untagging and other interpretive
overhead, or the offered solutions required too much expressive power
from the host language's type system.  By shifting from an initial
algebra approach to one which leverages the natural co-algebraic structure
of the $\lambda$-calculus, we show that both of these issues can be 
simultaneously dealt with.

% We show how to do many things without tags, in a simple type system, 
% essentially by avoiding any (inductive) types.  

\jacques{Should we mention that algebraic type tags give us a partly
intensional representation of terms.  This nicely allows pattern matching,
but as sits inside an extensional theory, ensuring that all traces 
of these tags disappear seems quite difficult without a very powerful
type system (enter GADTs).}

Our approach showed better the meaning of GADT and when they can be
avoided.
This shows also how a limited form of GADTs (the ones
used for the sake of type-dependent representations) can be
implemented with just functions. 

separate compilation: separate
typechecking of terms without knowing which particular
hole they will be plugged into. Formulate this as a prop?



\subsection*{Acknowledgments}
We would like to thank Martin Sulzmann and Walid Taha 
for helpful discussions.

\bibliographystyle{mcbride}
\bibsep=0pt
\bibliography{tagless}
\end{document}
