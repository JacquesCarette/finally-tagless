%% TODO: add a ref 
%%  Lars Birkedal's master thesis
%%   http://www.itu.dk/people/birkedal/papers/paresm.ps.gz
%% and thank Eijiro Sumii

\begin{comment}
\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}
\end{comment}

\begin{abstract}
\ifdim\parindent=0pt \parindent=1em \fi
We have built the first family of tagless interpretations for a
higher-order typed object
language in a typed metalanguage (Haskell or ML) that
require no dependent
types, generalized algebraic data types, or
postprocessing to eliminate tags.
The statically type-preserving interpretations include
an evaluator, a compiler (or staged evaluator), a partial
evaluator, and call-by-name and
call-by-value CPS transformers.

Our main idea is to encode
\ifshort HOAS \else de Bruijn or higher-order abstract syntax \fi
using compiler-generator (cogen) functions rather than data constructors.
In other words, we represent object terms not in an initial algebra
but using the coalgebraic structure of the $\lambda$-calculus.
Our representation also simulates inductive maps from types to
types, which are required for typed partial evaluation and CPS transformations.

Our encoding of an object term abstracts over the various ways to
interpret it, yet statically assures that the interpreters never get
stuck.  To achieve self\hyp interpretation and show Jones\hyp
optimality, we relate this exemplar of higher-rank and higher-kind
polymorphism \ifshort\else (provided by ML functors and Haskell~98 constructor
classes) \fi to plugging a term into a context of let\hyp polymorphic
bindings.
\end{abstract}

\begin{quote}
\small
    It should also be possible to define languages with a highly
    refined syntactic type structure. Ideally, such a treatment should
    be metacircular, in the sense that the type structure \linebreak[1] used in the
    defined language should be adequate for the defining language.
    \rm \ifshort \hfill John Reynolds~\else\\\fi\citep{reynolds-definitional}
\end{quote}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}\label{intro}

A popular way to define and implement a language is to embed it in
another \citep{reynolds-definitional}.  Embedding means to represent
terms and values of the \emph{object language} as terms and values in the
\emph{metalanguage}.  Embedding is especially appropriate for domain\hyp
specific object languages because it supports rapid prototyping and integration
with the host environment \citep{hudak-building}.
If the metalanguage supports \emph{staging}, then
the embedding can compile object programs to the metalanguage and avoid the
overhead of interpreting them on the fly \citep{WalidICFP02}.  A staged
definitional interpreter is thus a promising way to build a domain\hyp specific
language (DSL)\@.

\begin{figure}
    \begin{floatrule}
    \begin{proofrules}
        \[ \[ [x:t_1] \proofoverdots e:t_2 \] \justifies \fun{x}e:t_1\to t_2 \]
        \[ \[ [f:t_1\to t_2] \proofoverdots e:t_1\to t_2 \] \justifies \fix{f}e:t_1\to t_2 \]
        \[ e_1:t_1\to t_2 \quad e_2:t_1 \justifies e_1 e_2: t_2 \]
        \[ \text{$n$ is an integer} \justifies n:\ZZ \]
        \[ \text{$b$ is a boolean} \justifies b:\BB \]
        \[ e:\BB \quad e_1:t \quad e_2:t \justifies \cond{e}{e_1}{e_2}:t \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1+e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \times e_2:\ZZ \]
        \[ e_1:\ZZ \quad e_2:\ZZ \justifies e_1 \le e_2:\BB \]
    \end{proofrules}
    \end{floatrule}
    \caption{Our typed object language}
    \label{fig:object}
\end{figure}

We focus on embedding a \emph{typed} object language into a 
\emph{typed} metalanguage.
The benefit of types in this setting is to rule out meaningless object terms,
thus enabling faster interpretation and assuring that our interpreters
do not get stuck.
To be concrete, we use the typed object language in
Figure~\ref{fig:object} throughout this paper.  We aim not just for
evaluation of object programs but also for
compilation, partial evaluation, and other processing.

\Citet{WalidICFP02} and \citet{xi-guarded} motivated interpreting
a typed object language in a typed metalanguage as an interesting
problem.  The common solutions to this problem store object terms and
values in the metalanguage in a universal type, a generalized algebraic
data type (GADT), or a dependent type.  In the remainder of this section,
we discuss these solutions, identify their drawbacks, then summarize our
proposal and contributions.  
\ifshort
We leave aside the solved problem of writing a parser\slash type\hyp checker,
for embedding object language objects into the metalanguage
(whether using dependent types \citep{WalidICFP02} or not \citep{baars-typing}),
and just enter them by hand.
\else
No matter how we represent the object language in the
metalanguage, the representation can be created either by hand (for example, by
entering object terms at a metalanguage interpreter's prompt) or
by a parser\slash type\hyp checker reading from a text string.
We leave aside the solved problem of writing such a 
parser\slash type\hyp checker,
whether using dependent types \citep{WalidICFP02} or not \citep{baars-typing}.
\oleg{Should we really leave it aside, given that we do have the code:
  IncopeTypecheck, and given Sumii-san's advice to emphasize the fact
  that we do have the code. Please see my comment in the related work
  section in the paragraph that deals with typing dynamic typing.}
\fi

\subsection{The tag problem}\label{tagproblem}

\begin{SaveVerbatim}{2a}
type var = VZ | VS of var
type exp = V of var | B of bool | L of exp | A of exp * exp
\end{SaveVerbatim}
\begin{SaveVerbatim}[commandchars=\@\{\}]{2b}
let rec lookup (x::env) = function VZ -> x | VS v -> lookup env v
let rec eval0 env = function
| V v       -> lookup env v
| B b       -> b 
| L e       -> fun x -> eval0 (x::env) e
| A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{SaveVerbatim}
\begin{SaveVerbatim}{2c}
type u = UB of bool | UA of (u -> u)
\end{SaveVerbatim}
\begin{SaveVerbatim}{2d}
let rec eval env = function
| V v       -> lookup env v
| B b       -> UB b
| L e       -> UA (fun x -> eval (x::env) e)
| A (e1,e2) -> match eval env e1 with UA f -> f (eval env e2)
\end{SaveVerbatim}
\begin{SaveVerbatim}{test1}
let test1 = A (L (V VZ), B true)
\end{SaveVerbatim}

% see tagless\_interp1.ml, module Tagfull for the complete code.
% If you change the code in here, please adjust the .ml file
% accordingly. Let the paper and the accompanying code be in sync.
It is straightforward to create an algebraic data type, say in OCaml%
\ifshort, Fig.~\ref{fig:tag-problem}(a)\fi, to
represent object terms such as those in Figure~\ref{fig:object}.
For brevity, we elide treating integers, conditionals, and fixpoint in
this section.
\ifshort\else\UseVerbatim{2a}\fi
We represent each variable using a unary de Bruijn index.%
\footnote{We use de Bruijn indices to simplify the comparison with
\citearound{'s work}\citet{WalidICFP02}.}
For example, we represent the object term $(\fun{x}x)\True$ as
\ifshort \BUseVerbatim{test1}.\else \UseVerbatim{test1}\fi

\ifshort
\begin{figure}
%
(a) \BUseVerbatim{2a}

\smallskip
(b) \BUseVerbatim{2b}

\smallskip
(c) \BUseVerbatim{2c}

\smallskip
(d) \BUseVerbatim{2d}

\medskip
\caption{OCaml code illustrating the tag problem}
\label{fig:tag-problem}
\end{figure}
\fi

\noindent Following \citep{WalidICFP02},
we try to implement an interpreter function |eval0|\ifshort,
Fig.~\ref{fig:tag-problem}(b)\fi. It takes
an object term such as |test1| above and gives us its value.
The first argument to |eval0| is the environment, initially empty,
which is the list of values bound to free variables in the
interpreted code.
\ifshort\else\UseVerbatim{2b}\fi
If our OCaml-like metalanguage were untyped, the code above would be 
acceptable.
The |L e| line exhibits interpretive overhead:
|eval0| traverses the function body~|e| every time (the result of
evaluating) |L e| is applied. Staging can be used to remove this
interpretive overhead \citep[\S1.1--2]{WalidICFP02}.

However, the function |eval0| is ill-typed
if we use OCaml or some other typed language as the metalanguage.
The line |B b|
says that |eval0| returns a boolean, whereas the next line |L e| says
the result is a function, but all branches of a pattern-match form must
yield values of the same type. 
A related problem is the type of the environment |env|: a regular
OCaml list cannot hold both boolean and function values. 

The usual solution is to introduce a universal type \citep[\S1.3]
{WalidICFP02} containing both booleans and functions\ifshort,
Fig.~\ref{fig:tag-problem}(c)\fi.
\ifshort\else\UseVerbatim{2c}\fi
We can then write a typed interpreter\ifshort,
Fig.~\ref{fig:tag-problem}(d), \else\UseVerbatim{2d}\fi
whose inferred type is |u list -> exp -> u|. Now we can evaluate
\ifshort
\texttt{eval [] test1} obtaining |UB true|.
\else
\begin{code}
let test1r = eval [] test1
val test1r : u = UB true 
\end{code}
\fi
The unfortunate tag |UB| in the result reflects that |eval| is a partial
function.  
First, the pattern match |with UA f| in the line
|A (e1,e2)| is not exhaustive, so |eval| can fail if we apply a boolean,
as in the ill-typed term |A (B true, B false)|.
\ifshort\else
\begin{code}
let test2 = A (B true, B false)
let test2r = eval [] test2
Exception: Match_failure in eval
\end{code}
\fi
Second, the |lookup|
function assumes a nonempty environment, so |eval| can fail if we
evaluate an open term
\ifshort
|A (L (V (VS VZ)), B true)|.
\else
\begin{code}
let test3 = A (L (V (VS VZ)), B true)
let test3r = eval [] test3
Exception: Match_failure in lookup
\end{code}
\fi
After all, the type |exp| represents object
terms both well-typed and ill-typed, both open and closed.

If we evaluate only closed terms that have been type-checked, then
|eval| would never fail. Alas, this soundness is not obvious to the
metalanguage, whose type system we must still appease with the
nonexhaustive pattern matching in |lookup| and |eval| and the tags |UB|
and |UA| \citep[\S1.4]{WalidICFP02}.  In other words, the algebraic data
types above fail to express in the metalanguage that the object program
is well-typed.  This failure necessitates tagging and nonexhaustive
pattern\hyp matching operations that incur a performance penalty in
interpretation \citep{WalidICFP02} and impair optimality in partial evaluation
\citep{taha-tag}.  In short, the universal\hyp type solution is
unsatisfactory because it does not preserve typing.

\ifshort\else\subsection{Solutions using fancier types}\fi

It is commonly thought that to interpret a typed object language in
a typed metalanguage while preserving types is difficult and requires
GADTs or dependent types \citep{taha-tag}.  In fact, this problem
motivated much work on GADTs \citep{xi-guarded,peyton-jones-simple} and
on dependent types \citep{WalidICFP02,fogarty-concoqtion}.
\ifshort\else
For a metalanguage's type system to allow the well-typed object term
|test1| but disallow the ill-typed object term |test2|, fancier types
such as GADTs or dependent types seem necessary.  
\fi
Yet other type systems
have been proposed to distinguish closed terms like |test1| from open
terms 
\ifshort\citep{WalidPOPL03,NanevskiJFP05,DaviesJACM01}\else
like |test3|
\citep{WalidPOPL03,NanevskiICFP02,NanevskiJFP05,DaviesJACM01,nanevski-contextual}\fi,
so that |lookup| never receives an empty environment.  We discuss these
proposals further in \S\ref{related}\ifshort\else;
here we just note that many
advanced type systems have been devised to ensure statically that an
object term is well-typed and closed\fi.

\subsection{Our final proposal}\label{ourapproach}

We represent object programs using ordinary functions rather than
data constructors.  These functions comprise the entire interpreter, shown
below.
\ifshort
\begin{code3}
let varZ env    = fst env         let b (bv:bool) env = bv
let varS vp env = vp (snd env)    let lam e env   = fun x -> e (x,env)
let app e1 e2 env   = (e1 env) (e2 env)
\end{code3}
\else
\begin{code}
let varZ env        = fst env
let varS vp env     = vp (snd env)
let b (bv:bool) env = bv
let lam e env       = fun x -> e (x,env)
let app e1 e2 env   = (e1 env) (e2 env)
\end{code}
\fi
We now represent our sample term $(\fun{x}x)\True$ as
\ifshort
\texttt{let testf1 = app (lam varZ) (b true)}.
\else
\begin{code}
let testf1 = app (lam varZ) (b true)
\end{code}
\fi
This representation is almost the same as in \S\ref{tagproblem}, only
written with lowercase identifiers. To evaluate an object term is to
apply its representation to the empty environment\ifshort
, |testf1 ()|, obtaining |true|\fi.
\ifshort\else
\begin{code}
let testf1r = testf1 ()
val testf1r : bool = true
\end{code}
\fi
The result has no tags: the interpreter patently uses no tags and no
pattern matching. The term |b true| evaluates to a boolean and the term
|lam varZ| evaluates to a function, both untagged. The |app| function
applies |lam varZ| without pattern matching. What is more, evaluating an
open term such as
\ifshort
\texttt{app (lam (varS varZ)) (b true)}
\else
|testf3| below
\fi
gives a type error rather than a run-time error.
\ifshort\else
\begin{code}[commandchars=\\\{\}]
let testf3 = app (lam (varS varZ)) (b true)
let testf3r = testf3 \underline{()}
This expression has type unit but is here used with type 'a * 'b
\end{code}
\fi
The type error correctly complains
that the initial environment should be a tuple rather than~|()|.
In other words, the term is open.

In sum, by Church\hyp encoding terms using ordinary functions, we
achieve a tagless evaluator for a typed object language in a
metalanguage with a simple 
\ifshort
Hindley-Milner \fi
type system\ifshort\else\ \citep{hindley-principal,milner-theory}\fi
.  In this \emph{final} rather
than \emph{initial} approach, both kinds of run-time errors in
\S\ref{tagproblem} (applying a nonfunction and evaluating an open
term) are reported at compile time. Because the new interpreter
uses no universal type or pattern matching, it never results in a
run-time error, and is in fact total.  Because this safety is obvious
not just to us but also to the metalanguage implementation, we avoid
the serious performance penalty \citep{WalidICFP02} of error checking.
\Citet{Gluck-jones-optimality} explains deeper technical reasons that
inevitably lead to these performance penalties.

Unlike \citearound{'s HOAS-based term representation}\citet{JFP-Mogensen},
our solution does \emph{not} involve Church-encoding the
universal type. The Church encoding of the type~|u| in \S\ref{tagproblem}
requires two continuations; the function |app| in the interpreter above would
have to provide both to the encoding of~|e1|. The continuation
corresponding to the |UB| case of~|u| must either raise an error or
loop. For a well-typed object term, that error continuation is never
invoked, yet it must be supplied. In contrast, our interpreter has no error
continuation at all.

The evaluator above is wired directly into the
functions |b|, |lam|, |app|, and so on.  \ifshort We \else In the rest of this paper, we \fi
explain how to abstract the interpreter so as
to process the \emph{same} term in many other
ways: compilation, partial evaluation, CPS
conversion, and so forth.

\subsection{Contributions}\label{contributions}

The term ``constructor'' functions |b|, |lam|, |app|, and so on appear
free in the encoding of an object term such as |testf1| above.  Defining
these functions differently gives rise to different interpreters, that
is, different folds on object programs.  Given the same term
representation but varying the interpreter, we can
\begin{itemize}
    \item evaluate the term to a value in the metalanguage;
    \item measure the size or depth of the term;
    \item compile the term, with staging support such as in MetaOCaml;
    \item partially evaluate the term, online; and
    \item transform the term to continuation\hyp passing style (CPS),
        even call-by-name (CBN) CPS, so as to isolate the evaluation
        order of the object language from that of the metalanguage.\ifshort
\footnote{Due to serious lack of space, 
we refer the reader to the accompanying code for this.}\fi
\end{itemize}
We have programmed our interpreters in OCaml (and, for staging,
\citet{metaocaml}) and standard Haskell. The complete code is
available at \url{http://okmij.org/ftp/packages/tagless-final.tar.gz}
to supplement the paper. 
\ifshort For simplicity, main examples in the paper will be in MetaOCaml;
all examples have also been implemented in Haskell.
\else Our examples below switch between (Meta)OCaml
and Haskell even though we have implemented each example equivalently in
both metalanguages, because some of our claims are more obvious in one
metalanguage than the other.  For example, MetaOCaml provides
convenient, typed staging facilities.\fi

We attack the problem of tagless (staged) type-preserving
interpretation exactly as it was posed
by \citet{WalidICFP02} and \citet{xi-guarded}.
We use their running examples and achieve the
result they call desirable.  Our contributions are as follows.

\begin{enumerate}
\item We build interpreters that evaluate (\S\ref{language}),
   compile (or evaluate with staging) (\S\ref{S:compiler}), and 
   partially evaluate (\S\ref{PE}) a typed higher-order object language
   in a typed metalanguage, in direct and continuation\hyp passing
   styles\ifshort\else\ (\S\ref{variations})\fi.
\item All these interpreters use no type tags, patently never get stuck,
    and need no advanced type-system features such as GADTs, dependent types,
    or intentional type analysis.
\item The partial evaluator avoids polymorphic lift and delays binding-time
    analysis.  It bakes a type-to-type map into the interpreter
    interface to eliminate the need for GADTs and thus remain portable
    across Haskell 98 and ML.
\ifshort\else
\item We show the first typed, tagless, and statically type-preserving CPS
    transformation with simple types (\S\ref{variations}), which is available for mainstream
    functional languages (ML and Haskell). Type preservation is assured by
    the soundness of our embedding.
\fi
\item We use the type system of the metalanguage
    to check statically that an object program is well-typed and closed.
\item We show clean, comparable implementations in MetaOCaml and Haskell.
\item We specify a functor signature that encompasses all our
  interpreters, from evaluation and compilation (\S\ref{language}) 
   to partial evaluation \ifshort\else and CPS transformation \fi(\S\ref{PE}).
\item We point a clear way to extend the object language with more features
    such as state\ifshort\else~(\S\ref{state})\fi.\ifshort\footnote{Again, please see our code.}\fi
    \ Our term encoding is contravariant in the object language, so
    extending the language does not invalidate terms already encoded.
\item We describe an approach to self\hyp interpretation compatible with the
  above\ifshort\else~(\S\ref{selfinterp})\fi.  Self\hyp interpretation turned
  out to be harder than expected.\ifshort\footnotemark[\value{footnote}]\fi
\end{enumerate}
Our code is surprisingly simple and obvious in hindsight, but
it has been cited as a difficult problem (\cite{sumii-hybrid}
notwithstanding) to interpret a typed object language in a typed metalanguage
without tagging or type\hyp system extensions.  For example, \citet{taha-tag}
say that ``expressing such an interpreter in a statically typed
programming language is a rather subtle matter. In fact, it is only
recently that some work on programming type-indexed values in ML
\citep{yang-encoding} has given a hint of how such a function can be
expressed.''  We discuss related work in~\S\ref{related}.

To reiterate, we do \emph{not} propose any new language
feature or \ifshort new \else even any new programming \fi technique.
\ifshort
We
\else
Rather, we
\fi
solve a problem that was stated in the published record as open and
likely unsolvable in ML or Haskell 98 without extensions, by a novel
combination of simple types and techniques already described in the
literature that use features present in mainstream functional languages.
In particular, we follow \citearound{'s encoding of type-indexed
values}\citet{yang-encoding}, \citearound{'s construction of dynamic
terms alongside static terms}\citet{asai-binding-time}, and
\citearound{'s deforestation of syntax
constructors}\citet{sumii-hybrid}.  These techniques require just
a Hindley-Milner type system with either a module system that supports
functors or constructor classes, as realized in all variants of ML and
Haskell.
The simplicity of our solution and its
use of only mainstream features \ifshort\else are virtues that \fi make it more practical to build typed,
embedded DSLs.
\begin{comment}
We may claim some contribution about type-preserving CPS. Check
related work section in \citep{Guillemette-Monier-PLPV}, especially
check the work of Shao on type-preserving CPS in Flint. The PLPV paper
in related work shows other tasks, including closure conversion, which
we may tackle in our approach. We may be able to write a
type-preserving, assured compiler, whose properties and assured by HM.
\end{comment}


\section{The object language and its tagless interpreters}\label{language}

Figure~\ref{fig:object} shows our object language, a simply-typed
$\lambda$-calculus with fixpoint, integers, booleans, and comparison.
The language is close to \citets{xi-guarded}, without their polymorphic
lift but with more constants so as to more conveniently express Fibonacci,
factorial, and power.
\oleg{There is now IncopeDB.hs that defines at least R, L and C
 interpreters (no PE at present though) using DeBruijn indices. So we
 can say that (some of) our development is available in DeBruijn
 style. We won't talk about it in the paper though.}
In contrast to \S\ref{intro}, we encode binding using higher-order
abstract syntax (HOAS) \citep{miller-manipulating,pfenning-higher-order}
rather than de Bruijn indices. This makes the encoding convenient and
ensures that our object programs are closed.

\subsection{How to make encoding flexible: abstract the interpreter}
\label{encoding}

\ifshort
\begin{SaveVerbatim}{3a}
class Symantics repr where
  int :: Int  -> repr Int;       bool :: Bool -> repr Bool
  lam :: (repr a -> repr b) -> repr (a -> b)
  app :: repr (a -> b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int -> repr Int -> repr Int
  mul :: repr Int -> repr Int -> repr Int
  leq :: repr Int -> repr Int -> repr Bool
  if_ :: repr Bool -> repr a -> repr a -> repr a
\end{SaveVerbatim}
\else
\begin{SaveVerbatim}{3a}
class Symantics repr where
  int  :: Int  -> repr Int
  bool :: Bool -> repr Bool

  lam :: (repr a -> repr b) -> repr (a -> b)
  app :: repr (a -> b) -> repr a -> repr b
  fix :: (repr a -> repr a) -> repr a

  add :: repr Int -> repr Int -> repr Int
  mul :: repr Int -> repr Int -> repr Int
  leq :: repr Int -> repr Int -> repr Bool
  if_ :: repr Bool -> repr a -> repr a -> repr a
\end{SaveVerbatim}
\fi
\begin{SaveVerbatim}{3b}
testpowfix () = lam (\x -> fix (\self -> lam (\n ->
                 if_ (leq n (int 0)) (int 1)
                     (mul x (app self (add n (int (-1))))))))
\end{SaveVerbatim}
\begin{SaveVerbatim}{3c}
testpowfix7 () = lam (\x -> app (app (testpowfix ()) x) (int 7))
\end{SaveVerbatim}

We embed our language in (Meta)OCaml and Haskell.  In Haskell,
the functions that construct object terms are methods in a type class
|Symantics| (with a parameter |repr| of kind |* -> *|)\ifshort,
Fig.~\ref{fig:symantics-haskell}(a)\fi. The class is so named
because its interface gives the syntax of the object language and its
instances give the semantics.
\ifshort\else\UseVerbatim{3a}\fi
For example, we encode the term |test1|, or $(\fun{x}x)\True$, from
\S\ref{tagproblem} above as \texttt{app (lam (\textbackslash x -> x)) (bool True)},
whose inferred type is \texttt{Symantics repr => repr Bool}.
For another example, the classical $\mathit{power}$ function is
\ifshort in Fig.~\ref{fig:symantics-haskell}(b)
\else\UseVerbatim{3b}\fi
and the partial application $\fun{x} \mathit{power}\;x\;7$ is
\ifshort in Fig.~\ref{fig:symantics-haskell}(c).
\else\UseVerbatim{3c}\fi
The dummy argument |()| above is to avoid the monomorphism
restriction, to keep the type of |testpowfix| and |testpowfix7|
polymorphic in |repr|. \ifshort\else Instead of supplying this dummy
argument, we could have given the terms explicit polymorphic
signatures.  We however prefer for
Haskell to infer the object types for us. We could also
avoid the dummy argument by switching off the monomorphism restriction
with a compiler flag. \fi
The methods |add|, |mul|, and |leq| are quite similar, and so are
|int| and |bool|. Therefore, we often show
only one method of each group and elide the rest. The
accompanying code has the complete implementations.

\ifshort
\begin{figure}
(a) \BUseVerbatim{3a}

\smallskip
(b) \BUseVerbatim{3b}

\smallskip
(c) \BUseVerbatim{3c}

\medskip
\caption{Symantics in Haskell}
\label{fig:symantics-haskell}
\end{figure}
\fi

Comparing |Symantics| with Fig.~\ref{fig:object}
shows how to represent \emph{every} typed, closed object term in the
metalanguage. Moreover, the representation preserves types.
\begin{proposition}
If an object term has the object type~$t$, then its
representation in the metalanguage has the type 
|forall repr.| |Symantics repr => repr |$t$.
\end{proposition}
Conversely, the type system of the metalanguage statically checks that the
represented object term is well-typed and closed.
If we err, say replace |int 7| with |bool True| in
|testpowfix7|, Haskell will complain there that the expected type |Int| does not
match the inferred |Bool|.  Similarly, the object term $\fun{x}xx$ and its
encoding |lam (\x -> app x x)| both fail occurs-checks in type checking.
Haskell's type checker also flags syntactically invalid object terms, 
such as if we forget |app| somewhere above.

\begin{SaveVerbatim}{ocaml-simple}
module type Symantics = sig type ('c, 'dv) repr
  val int : int  -> ('c, int) repr
  val bool: bool -> ('c, bool) repr

  val lam : (('c, 'da) repr -> ('c, 'db) repr) -> ('c, 'da -> 'db) repr
  val app : ('c, 'da -> 'db) repr -> ('c, 'da) repr -> ('c, 'db) repr
  val fix : ('x -> 'x) -> (('c, 'da -> 'db) repr as 'x)

  val add : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val mul : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
  val leq : ('c, int) repr -> ('c, int) repr -> ('c, bool) repr
  val if_ : ('c, bool) repr
            -> (unit -> 'x) -> (unit -> 'x) -> (('c, 'da) repr as 'x)
end
\end{SaveVerbatim}
\begin{SaveVerbatim}{ocaml-examples}
module EX(S: Symantics) = struct open S
  let test1 () = app (lam (fun x -> x)) (bool true)
  let testpowfix () =
       lam (fun x -> fix (fun self -> lam (fun n ->
        if_ (leq n (int 0)) (fun () -> int 1)
            (fun () -> mul x (app self (add n (int (-1))))))))
  let testpowfix7 = lam (fun x -> app (app (testpowfix ()) x) (int 7))
end
\end{SaveVerbatim}

\ifshort
\begin{figure}[t]
\begin{tabular}{@{}l@{}}
\ifx\relax\normalbaselineskip\else\baselineskip\normalbaselineskip\fi
\BUseVerbatim[baseline=b]{ocaml-simple}\\[\smallskipamount]
\ifx\relax\normalbaselineskip\else\baselineskip\normalbaselineskip\fi
\BUseVerbatim[baseline=t]{ocaml-examples}
\end{tabular}

\medskip
\caption{A simple (Meta)OCaml embedding of our object language, 
    and examples}
\label{fig:ocaml-simple}
\label{fig:ocaml-examples}
\end{figure}
\else
\begin{figure}[t]
\begin{floatrule}
\BUseVerbatim{ocaml-simple}
\end{floatrule}
\caption{A simple (Meta)OCaml embedding of our object language}
\label{fig:ocaml-simple}
\end{figure}
\begin{figure}[t]
\begin{floatrule}
\BUseVerbatim{ocaml-examples}
\end{floatrule}
\caption{Examples using the embedding in Figure~\ref{fig:ocaml-simple} of our object language}
\label{fig:ocaml-examples}
\end{figure}
\fi

To embed the same object language in (Meta)OCaml, we replace the \ifshort type
class \fi |Symantics| \ifshort\else type class \fi and its instances by a module signature |Symantics|
and its implementations.  Figure~\ref{fig:ocaml-simple} shows a simple
signature that suffices until~\S\ref{PE}.  The two differences are:
the additional type parameter |'c|, an
\emph{environment classifier} \citep{WalidPOPL03} required by MetaOCaml for
code generation in~\S\ref{S:compiler}; and the $\eta$-expanded type for
|fix| and thunk types in |if_| since OCaml is a call-by-value
language.
We shorten some of the types using OCaml's |as| syntax.

The functor |EX| in Fig.~\ref{fig:ocaml-examples} encodes 
our running examples |test1| and the $\mathit{power}$ function
(|testpowfix|).
The dummy argument to |test1| and |testpowfix| is an artifact of
MetaOCaml, related to monomorphism: in order for us to run a
piece of generated code, it must be polymorphic in its environment
classifier (the type variable |'c| in Figure~\ref{fig:ocaml-simple}).
The value restriction dictates that
the definitions of our object terms must look syntactically like
values. (Alternatively, we could have used
the rank-2 record types of OCaml to maintain the necessary polymorphism.)
\ifshort\else\par\fi
Thus, we represent an object expression in
OCaml as a functor from |Symantics| to an appropriate semantic domain. This
is essentially the same as the constraint \texttt{Symantics repr =>} in the
Haskell embedding.

\subsection{Two tagless interpreters}
\label{S:interpreter-RL}

Having abstracted our term representation over the interpreter, we are
now ready to present a series of interpreters.  Each interpreter is an
instance of the |Symantics| class in Haskell and a module implementing
the |Symantics| signature in MetaOCaml.

The first interpreter evaluates an object term to its value in the
metalanguage.  The module below interprets each object\hyp language
operation as the corresponding metalanguage operation.
\ifshort
\begin{code3}
module R = struct type ('c,'dv) repr = 'dv (* no wrappers *)
  let int  (x:int)  = x         let bool (b:bool) = b
  let lam  f        = f         let app  e1 e2    = e1 e2
  let fix  f        = let rec self n = f self n in self
  let add  e1 e2    = e1 + e2   let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then et () else ee () end
\end{code3}
\else
\begin{code}
module R = struct
  type ('c,'dv) repr = 'dv (* no wrappers *)

  let int  (x:int)  = x
  let bool (b:bool) = b
  let lam  f        = f
  let app  e1 e2    = e1 e2
  let fix  f        = let rec self n = f self n in self
  let add  e1 e2    = e1 + e2
  let mul  e1 e2    = e1 * e2
  let leq  x y      = x <= y
  let if_  eb et ee = if eb then et () else ee ()
end
\end{code}
\fi
%
As in~\S\ref{ourapproach},
this interpreter is patently tagless, using neither a universal type nor
any pattern matching: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. To run our
examples, we instantiate the |EX| functor from~\S\ref{encoding}
with~|R|\ifshort: \texttt{module EXR = EX(R)}\fi.
\ifshort\else
\begin{code}
module EXR = EX(R)
\end{code}
\fi
Thus, |EXR.test1 ()| evaluates to the untagged boolean value |true|.
\ifshort It \else
In Haskell, we define
\begin{code}
newtype R a = R {unR::a}
instance Symantics R where ...
\end{code}
Although |R| looks like a tag, it is only
a |newtype|.  The types |a| and |R a| are represented differently
only at compile time, not at run time.  Pattern matching against~|R|
cannot ever fail and is assuredly compiled away.
In OCaml, too, it
\fi is obvious to the compiler that
pattern matching cannot fail, because there is no
pattern matching. Evaluation can only fail to yield a value
due to interpreting |fix|.
\ifshort
(The source code shows a total interpreter |L| that measures
the size of each object term.)
\fi
\ifshort
We can also generalize from~|R| to all interpreters;
these propositions follow immediately from the soundness of the
metalanguage's type system.
\fi
\begin{proposition}
If an object term~$e$ encoded in the metalanguage has type~$t$,
then evaluating~$e$ in the interpreter~|R| either continues
indefinitely or terminates with a value of the same type~$t$.
\end{proposition}
\ifshort\else
Generalizing from~|R| to all interpreters, we have the following
broader and more useful, if also more obvious, proposition.
\fi
\begin{proposition}
  If an implementation of |Symantics| never gets stuck, then
  the type system of the object
  language is sound with respect to the dynamic semantics defined by
  that implementation.
\end{proposition}
\ifshort\else
These propositions follow immediately from the soundness of the
metalanguage's type system.
\fi
\ifshort\else
For variety, we show another interpreter, which measures the \emph{size}
of each object term, defined as the number of term
constructors.
\begin{code}
module L = struct
  type ('c,'dv) repr = int

  let int  (x:int)  = 1
  let bool (b:bool) = 1
  let lam  f        = f 0 + 1
  let app  e1 e2    = e1 + e2 + 1
  let fix  f        = f 0 + 1
  let add  e1 e2    = e1 + e2 + 1
  let mul  e1 e2    = e1 + e2 + 1
  let leq  x y      = x + y + 1
  let if_  eb et ee = eb + et () + ee () + 1
end
\end{code}
Now the OCaml expression
|let module E = EX(L) in E.test1 ()|
evaluates to |3|. This interpreter is not only tagless but also
total. It ``evaluates'' even seemingly divergent terms, for instance
|app (fix (fun self -> self)) (int 1)| evaluates to $3$.
\fi

\begin{comment}
\begin{code}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{code}
\end{comment}

\section{A tagless compiler (or, a staged interpreter)}\label{S:compiler}

Besides immediate evaluation, we can compile our object language
into OCaml code using MetaOCaml's staging facilities. MetaOCaml
represents future-stage expressions of type~$t$
as values of type |('c, |$t$|) code|, where |'c| is the
environment classifier \citep{WalidPOPL03,calcagno-ml-like}. Code values are created
by a \emph{bracket} form |.<|$e$|>.|, which quotes the expression~$e$ for
evaluation at a future stage. The \emph{escape} |.~|$e$ must occur
within a bracket and specifies that the expression~$e$ must be evaluated
at the current stage; its result, which must be a code value, is
spliced into the code being built by the enclosing bracket. The \emph{run} form |.!|$e$ evaluates
the future-stage code value~$e$ by compiling and linking it at run time.
Bracket, escape, and run are akin to
quasi-quotation, unquotation, and |eval| of Lisp.

% this code does not have the 'sv parameter. It shows up later.
\ifshort
\begin{SaveVerbatim}{5a}
module C = struct type ('c,'dv) repr = ('c,'dv) code
  let int (x:int)  = .<x>.            let bool (b:bool) = .<b>.
  let lam f        = .<fun x -> .~(f .<x>.)>.
  let app e1 e2    = .<.~e1 .~e2>.
  let fix f =  .<let rec self n = .~(f .<self>.) n in self>.
  let add e1 e2    = .<.~e1 + .~e2>.  let mul e1 e2 = .<.~e1 * .~e2>.
  let leq x y      = .<.~x <= .~y>.
  let if_ eb et ee = .<if .~eb then .~(et ()) else .~(ee ())>. end
\end{SaveVerbatim}
\else
\begin{SaveVerbatim}{5a}
module C = struct
  type ('c,'dv) repr = ('c,'dv) code

  let int (x:int)   = .<x>.
  let bool (b:bool) = .<b>.
  let lam f         = .<fun x -> .~(f .<x>.)>.
  let app e1 e2     = .<.~e1 .~e2>.
  let fix f         = .<let rec self n = .~(f .<self>.) n in self>.
  let add e1 e2     = .<.~e1 + .~e2>.
  let mul e1 e2     = .<.~e1 * .~e2>.
  let leq x y       = .<.~x <= .~y>.
  let if_ eb et ee  = .<if .~eb then .~(et ()) else .~(ee ())>.
end
\end{SaveVerbatim}
\fi
\begin{SaveVerbatim}{5b}
let module E = EX(C) in E.test1 ()
\end{SaveVerbatim}
\begin{SaveVerbatim}{5c}
let module E = EX(C) in E.testpowfix7
\end{SaveVerbatim}
\begin{SaveVerbatim}{5d}
.<fun x_1 -> (fun x_2 -> let rec self_3 = fun n_4 ->
   (fun x_5 -> if x_5 <= 0 then 1 else x_2 * self_3 (x_5 + (-1)))
   n_4 in self_3) x_1 7>.
\end{SaveVerbatim}

To turn the evaluator~|R| into a simple compiler, we bracket the
computation on values to be performed at run time, then escape the code
generation from terms to be performed at compile time.  Adding these
stage annotations yields the compiler~|C|
\ifshort in Fig.~\ref{fig:interpreter-C}(a).\else below.
\UseVerbatim{5a}
\fi
\ifshort
\begin{figure}[t]
(a) \BUseVerbatim{5a}

\smallskip
(b) \BUseVerbatim{5b}

\smallskip
(c) \BUseVerbatim{5c}

\smallskip
(d) \BUseVerbatim{5d}

\medskip
\caption{The tagless staged interpreter \texttt{C}}
\label{fig:interpreter-C}
\end{figure}
\fi
This is a straightforward staging of
|module R|.
This compiler produces
unoptimized code. For example, interpreting our |test1| with
\ifshort Fig.~\ref{fig:interpreter-C}(b) \else \UseVerbatim{5b}\fi
gives the code value |.<(fun|~|x_6|~|->|~|x_6)| |true>.|
of inferred type |('c,|~|bool)| |C.repr|.  Interpreting |testpowfix7|
with
\ifshort Fig.~\ref{fig:interpreter-C}(c) \else \UseVerbatim{5c}\fi
gives a code value with many apparent $\beta$- and $\eta$-redexes\ifshort,
Fig.~\ref{fig:interpreter-C}(d). \else: \UseVerbatim{5d}\fi
This compiler does not incur
any interpretive overhead: the
code produced for $\fun{x}x$ is simply |fun|~|x_6|~|->|~|x_6|\ifshort\else\
and does not
call the interpreter, unlike the recursive calls to |eval0| and
|eval| in the |L e| lines in \S\ref{tagproblem}\fi.
The resulting code obviously contains no tags and no pattern matching.
The environment classifiers here, like the tuple types in \S\ref{ourapproach},
make it a type error to run an open expression.
\ifshort
The accompanying code shows the Haskell implementation. 
\else
\begin{proposition}
    The compiler |C| generates tagless code free of pattern matching.
\end{proposition}
This is syntactically clear from the body of |C|, and follows
readily by structural induction and the properties of |.~| and
|.< >.|.

We have also implemented this compiler in Haskell. 
Since Haskell
has no (convenient, typed) staging facility, we had to emulate
it: we defined a data type |ByteCode| with
constructors such as |Var|, |Lam|, |App|, |Fix|, and |INT|.
(Alternatively, we could use Template Haskell as our staging facility:
\texttt{ByteCode} can be mapped to the abstract syntax of Template
Haskell. The output of our compiler will then be assuredly type-correct
Template Haskell.)
Whereas our representation of object terms uses HOAS,
our bytecode uses integer-named
variables to be realistic. 
We then define 
\begin{code}
newtype C t = C (Int -> (ByteCode t, Int)) 
\end{code}
where |Int| is the counter for creating fresh variable
names. We define the compiler by making |C| an instance of the
class |Symantics|.
The implementation is quite similar (but slightly more
verbose) than the corresponding MetaOCaml code above. (The implementation uses
GADTs because we also wanted to write a typed interpreter for 
the \texttt{ByteCode} \emph{data type}.) The
accompanying code gives the full details.
\fi

%------------------------------------------------------------------------
% This is the version for the APLAS paper. No Haskell
\ifshort
\input{tagless-final-aplas}
%------------------------------------------------------------------------
% This is the longer version of the section
\else

\section{A tagless partial evaluator}\label{PE}
Surprisingly, we can write a partial evaluator using the idea above,
namely to build object terms using ordinary functions rather than data
constructors.  We present this partial evaluator in a sequence of three
attempts. It uses no universal type and no tags
for object types.  We then discuss residualization and binding-time
analysis.  Our partial evaluator is a modular extension of the evaluator
in~\S\ref{S:interpreter-RL} and the compiler in~\S\ref{S:compiler}, in
that it uses the former to reduce static terms and the latter to build
dynamic terms.

\subsection{Avoiding polymorphic lift}
\label{S:PE-lift}

Roughly, a partial evaluator interprets each object term to yield either
a static (present-stage) term (using the evaluator~|R|) or
a dynamic (future-stage) term (using the compiler~|C|).  To
distinguish between static and dynamic terms, we might try to define
|repr| in the partial evaluator (module |P0|) as
\begin{code}
type ('c,'dv) repr = S0 of ('c,'dv) R.repr | D0 of ('c,'dv) C.repr
\end{code}
(the digit zero in the names of the phase tags |S0| and |D0| is the
version suffix).
To extract a dynamic term from this type, we create the function
\begin{code}
let abstrI0 (e:('c,int) repr) : ('c,int) C.repr =
  match e with
  | S0 e -> C.int e
  | D0 e -> e
\end{code}
and the similar function |abstrB0| for dynamic boolean terms. Here, 
|C.int| is used to convert from the static term |('c,int) R.repr|, 
which is just |int|, to a dynamic term. We can now define the following
components of module |P0| required by the |Semantics| signature:
\begin{code}
let int  (x:int)  = S0 (R.int x)
let bool (x:bool) = S0 (R.bool x)
let add e1 e2 = 
  match (e1,e2) with
  | (S0 e1, S0 e2) -> S0 (R.add e1 e2)
  | (e1, e2)       -> D0 (C.add (abstrI0 e1) (abstrI0 e2))
\end{code}
Integer and boolean literals are immediate, present-stage
values. Addition yields a static term (using~|R.add|) if and only 
if both operands are static; otherwise we extract the dynamic terms 
from the operands and add them using~|C.add|.

Whereas |mul| and |leq| are as easy to define as |add|, we encounter
a problem with |if_|.  Suppose that the first argument to |if_| 
is a dynamic term
(of type |('c,bool) C.repr|), the second a static term 
(of type |('c,'a) R.repr|), and the third a
dynamic term (of type |('c,'a) C.repr|). We then need to convert
the static term to dynamic, but there is no polymorphic ``lift''
function, of type |'a -> ('c,'a) C.repr|, to send a value to a future stage
\citep{xi-guarded,WalidPOPL03}.
\begin{comment}
(By the way, if we
were to add polymorphic \texttt{lift} to the type class
\texttt{Symantics repr}, then \texttt{repr} would become an instance of
\texttt{Applicative} and thus \texttt{Functor}:\texttt{ fmap
f = app (lift f)~}.)
\end{comment}

Our |Symantics| only includes separate lifting methods |bool| and
|int|, not a parametrically polymorphic lifting method, for good reason:
When compiling to a first-order target language such as machine code,
booleans, integers, and functions may well be represented differently.
Compiling a polymorphic lift function thus requires intensional type
analysis.  To avoid needing polymorphic lift, we turn to
Asai's technique \citep{asai-binding-time,sumii-hybrid} of
building a dynamic term alongside every static term.

\subsection{Delaying binding-time analysis}
\label{S:PE-problem}

We start building the partial evaluator anew (module |P1|) and
switch to the data type
\begin{code}
type ('c,'dv) repr = P1 of ('c,'dv) R.repr option * ('c,'dv) C.repr
\end{code}
so that a partially evaluated term always contains a dynamic
component and sometimes contains a static component.  By 
distributivity, the two
alternative constructors of an |option| value, |Some| and |None|,
tag each partially evaluated term with a phase: either present or
future.  This tag is not an object type tag: all pattern matching below
is exhaustive. Because the future-stage component is always available, we
can now define the polymorphic function
|let abstr1 (P1 (_,dyn)) = dyn| of type
\texttt{('c,'dv) repr -> ('c,'dv) C.repr}
to extract it without requiring polymorphic lift into~|C|.  We then try
to define the interpreter |P1|---and get as far as the first-order
constructs of our object language, including |if_|.
\begin{code}
module P1 : Symantics = struct
  let int (x:int) = P1 (Some (R.int x), C.int x)
  let add e1 e2 = match (e1,e2) with
    | (P1 (Some n1,_),P1 (Some n2,_)) -> int (R.add n1 n2)
    | _ -> P1 (None,(C.add (abstr1 e1) (abstr1 e2)))
  let if_ = function
    | P1 (Some s,_) -> fun et ee -> if s then et () else ee ()
    | eb -> fun et ee -> P1 (None, C.if_ (abstr1 eb) 
                                   (fun () -> abstr1 (et ()))
                                   (fun () -> abstr1 (ee ())))
\end{code}
However, we stumble on functions.  According to our
definition of~|P1|, a partially evaluated object function, such as the
identity $\fun{x}x$ embedded in OCaml as |lam (fun x -> x)|\texttt{ :
}|('c,'a->'a) P1.repr|, consists of a dynamic part 
(type |('c,'a->'a) C.repr|) and
maybe a static part (type |('c,'a->'a) R.repr|).  The dynamic part is useful
when this function is passed to another function that is only
dynamically known, as in $\fun{k}k(\fun{x}x)$.  The static part is
useful when this function is applied to a static argument, as in
$(\fun{x}x)\True$.  Neither part, however, lets us \emph{partially}
evaluate the function, that is, compute as much as possible statically
when it is applied to a mix of static and dynamic inputs.  For example,
the partial evaluator should turn $\fun{n}(\fun{x}x)n$ into $\fun{n}n$
by substituting $n$ for~$x$ in the body of $\fun{x}x$ even though $n$ is
not statically known.  The same static function, applied to
different static arguments, can give both static and dynamic results: we
want to simplify $(\fun{y}x\times y)0$ to~$0$ but $(\fun{y}x\times y)1$
to~$x$.

To enable these simplifications, we delay binding-time analysis
for a static function until it is applied, that is, until |lam f|
appears as the argument of |app|.  To do so, we have to incorporate |f|
as it is into the |P1.repr| data structure: the representation
for a function type |'a->'b| should be one of
\begin{code}
S1 of ('c,'a) repr -> ('c,'b) repr | E1 of ('c,'a->'b) C.repr
P1 of (('c,'a) repr -> ('c,'b) repr) option * ('c,'a->'b) C.repr
\end{code}
unlike |P1.repr| of |int| or |bool|.
That is, we need a nonparametric data type, something akin to
type-indexed functions and type-indexed types, which
\citet{oliveira-typecase} dub the \emph{typecase} design pattern.
Thus, typed partial evaluation, like typed CPS transformation
(see \S\ref{S:CPS}),
inductively defines a map from source types to target types that
performs case distinction on the source type. In Haskell, typecase
can be implemented using either GADTs or
type-class functional dependencies
\citep{oliveira-typecase}. The accompanying code shows both
approaches (|Incope.hs| and |incope1.hs|), 
neither of which is portable to OCaml. In addition,
the problem of non\hyp exhaustive pattern\hyp matching reappears in
the GADT approach because GHC 6.6.1 cannot see that a particular
type of a GADT value precludes certain constructors.
Thus GADTs fail to
make it \emph{syntactically} apparent that pattern matching is exhaustive.
\jacques{what about GHC 6.8?}


%% \subsection{Eliminating tags from typecase}
%% \label{S:PE-GADT}

%% Two common ways to provide typecase in Haskell are
%% GADTs and type-class functional dependencies
%% \citep{oliveira-typecase}.  These
%% methods are equivalent, and here we use GADTs; |incope1.hs|
%% in the accompanying source code shows the latter.
%% We introduce a GADT with four data constructors.
%% \begin{code}
%% data P t where
%%   VI :: Int  -> P Int
%%   VB :: Bool -> P Bool
%%   VF :: (P a -> P b) -> P (a -> b)
%%   E  :: C t -> P t
%% \end{code}
%% The constructors |VI|, |VB|, and |VF| build static terms (like |S0|
%% in~\S\ref{S:PE-lift}), and |E| builds dynamic terms (like |D0|).  However,
%% the type |P t| is no longer parametric in~|t|: the constructor |VF| takes an
%% operand of type |P a -> P b| rather than |a -> b|.  We define a function
%% like |abstr1| above to extract a future-stage computation from a 
%% value of type |P t|.
%% \begin{code}
%% abstr :: P t -> C t
%% abstr (VI i) = int i
%% abstr (VB b) = bool b
%% abstr (VF f) = lam (abstr . f . E)
%% abstr (E x)  = x
%% \end{code}
%% The cases of this function |abstr| are type-indexed.  In particular, the |VF f|
%% case uses the method |lam| of the |C| interpreter to compile~|f|.

%% We may now make |P| an instance of
%% |Symantics| and implement the partial evaluator as follows. We elide
%% |mul|, |leq|, |if_|, and |fix|.
%% \begin{code}
%% instance Symantics P where
%%   int x               = VI x
%%   bool b              = VB b
%%   add (VI n1) (VI n2) = VI (n1 + n2)
%%   add e1 e2           = E (add (abstr e1) (abstr e2))
%%   lam                 = VF
%%   app (VF f) ea       = f ea
%%   app (E f)  ea       = E (app f (abstr ea))
%% \end{code}
%% The implementations of |int|, |bool|, and |add| are like 
%% in~\S\ref{S:PE-problem}. The interpretation of |lam f| is |VF f|, 
%% which just wraps the HOAS function |f|. 
%% We can always compile |f| to a code value,
%% but we delay it to apply |f| to concrete arguments. The interpretation of
%% |app ef ea| checks to see if |ef| is such a delayed
%% HOAS function |VF f|. If it is, we apply |f| to the
%% concrete argument |ea|, giving us a chance to perform static
%% computations (see example |testpowfix7| in~\S\ref{S:PE-solution}). If |ef| is a
%% dynamic value |E f|, we residualize.

%% This solution using GADTs works but is not quite satisfactory. First, it
%% cannot be ported to MetaOCaml, as GADTs are unavailable there.  Second,
%% the problem of nonexhaustive pattern\hyp matching reappears in
%% |app| above: the type |P t| has four constructors, of which the pattern in
%% |app| matches only |VF| and~|E|. One may say that the
%% constructors |VI| and |VB| obviously cannot occur because they do not
%% construct values of type |P (a -> b)| as required by the type of |app|.
%% Indeed, the metalanguage implementation could reason thus:
%% if we use inductive families (as in Coq) or logical
%%   frameworks with canonical forms (as in Twelf with its coverage checker),
%%   we can prove the pattern matching to be exhaustive.
%% Then again, the metalanguage implementation may not reason thus:
%% GHC cannot and issues warnings.
%% Although this point may seem minor, it is the heart of
%% the tagging problem and the purpose of tag elimination. A typed tagged
%% interpreter contains many pattern\hyp matching forms that look partial
%% but never fail in reality. The
%% goal is to make this exhaustiveness \emph{syntactically} apparent.


\subsection{The ``final'' solution}
\label{S:PE-solution}

Let us re-examine the problem in~\S\ref{S:PE-problem}. What we
would ideally like is to write
\begin{code}
type ('c,'dv) repr = P1 of (repr_pe ('c,'dv)) R.repr option * ('c,'dv) C.repr
\end{code}
where |repr_pe| is the type function defined
% inductively because P below depends on repr_pe
by
\begin{code}
repr_pe ('c,int)    = ('c,int)
repr_pe ('c,bool)   = ('c,bool)
repr_pe ('c,'a->'b) = ('c,'a) repr -> ('c,'b) repr
\end{code}
Although we can use type classes to define this type function
in Haskell, that is not portable to MetaOCaml. However,
these three typecase alternatives are already present in existing
methods of |Symantics|.
A simple and portable solution\footnote{though somewhat long-winded} thus
emerges: we bake |repr_pe| into the signature |Symantics|. 
We recall from Figure~\ref{fig:ocaml-simple} in~\S\ref{encoding} that 
the |repr| type constructor took two arguments |'c| and~|'dv|.
We add an argument |'sv| for the result of applying |repr_pe| to~|'dv|.
Figure~\ref{fig:ocaml} shows the new signature.
\begin{figure}
\begin{floatrule}
\begin{code2}
module type Symantics = sig
  type ('c,'sv,'dv) repr

  val int : int  -> ('c,int,int) repr
  val bool: bool -> ('c,bool,bool) repr

  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x)
            -> ('c,'x,'da -> 'db) repr
  val app : ('c,'x,'da -> 'db) repr
            -> (('c,'sa,'da) repr -> ('c,'sb,'db) repr as 'x)
  val fix : ('x -> 'x) -> (('c, ('c,'sa,'da) repr -> ('c,'sb,'db) repr,
                                'da -> 'db) repr as 'x)

  val add : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val mul : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val leq : ('c,int,int) repr -> ('c,int,int) repr -> ('c,bool,bool) repr
  val if_ : ('c,bool,bool) repr
            -> (unit -> 'x) -> (unit -> 'x) -> (('c,'sa,'da) repr as 'x)
end
\end{code2}
\end{floatrule}
\caption{A (Meta)OCaml embedding of our object language that supports
  partial evaluation}
\label{fig:ocaml}
\end{figure}

\begin{figure}
\begin{floatrule}
\begin{code2}[commandchars=\@\[\]]
module P = struct
  type ('c,'sv,'dv) repr = {st: 'sv option; dy: ('c,'dv) code}
  let abstr {dy = x} = x
  let pdyn x = {st = None; dy = x}

  let int  (x:int ) = {st = Some (R.int  x); dy = C.int  x}
  let bool (x:bool) = {st = Some (R.bool x); dy = C.bool x}

  let add e1 e2 = match e1, e2 with
                  | {st = Some 0}, e | e, {st = Some 0} -> e
                  | {st = Some m}, {st = Some n} -> int (R.add m n)
                  | _ -> pdyn (C.add (abstr e1) (abstr e2))
  let if_ eb et ee = match eb with
                     | {st = Some b} -> if b then et () else ee ()
                     | _ -> pdyn (C.if_ (abstr eb) (fun () @!->@! abstr (et @!()))
                                                   (fun () @!->@! abstr (ee @!())))

  let lam f = {st = Some f; dy = C.lam (fun x -> abstr (f (pdyn x)))}
  let app ef ea = match ef with
                  | {st = Some f} -> f ea
                  | _ -> pdyn (C.app (abstr ef) (abstr ea))
  let fix f = let fdyn = C.fix (fun x -> abstr (f (pdyn x)))
              in let rec self = function
                                | {st = Some @!_} as e -> app (f (lam self)) e
                                | e -> pdyn (C.app fdyn (abstr e))
                 in {st = Some self; dy = fdyn}
end
\end{code2}
\end{floatrule}
\caption{Our partial evaluator (\texttt{mul} and \texttt{leq}
  are elided)}
\label{fig:pe}
\end{figure}

The interpreters |R|, |L| and~|C| in \S\ref{S:interpreter-RL},\S\ref{S:compiler}
only use the old
type arguments |'c| and~|'dv|, which are treated by the new signature
in the same way.  Hence, all that needs to change in these interpreters
to match the new signature is to add a phantom type
argument~|'sv| to~|repr|.
For example, the compiler |C| now begins
\begin{code}
module C = struct
  type ('c,'sv,'dv) repr = ('c,'dv) code
\end{code}
with the rest the same.
Predictably, the partial evaluator~|P| crucially relies on the type argument
|'sv|.

Figure~\ref{fig:pe} shows the partial evaluator~|P|.
Its type |repr| literally expresses the type equation for |repr_pe| above.
The function |abstr|
extracts a future-stage code value from the result of
partial evaluation.  Conversely, the function |pdyn| injects a
code value into the |repr| type. As
in~\S\ref{S:PE-problem}, we build dynamic terms alongside
any static ones to avoid polymorphic lift.

The static portion of the interpretation of |lam f| is |Some f|, 
which just wraps the HOAS
function |f|. The interpretation of |app ef ea| 
checks to see if |ef| is such a wrapped
HOAS function. If it is, we apply |f| to the
concrete argument |ea|, giving us a chance to perform static
computations (see the example below). If
|ef| has only a dynamic part, we residualize.

To illustrate how to add optimizations, we improve |add| (and |mul|,
elided) to simplify the generated code using the monoid (and ring)
structure of~|int|: not only is addition performed statically
(using~|R|) when both operands are statically known, but it is
eliminated when one operand is statically~$0$; similarly for
multiplication by~$0$ or~$1$.  
Such algebraic simplifications are easy
to abstract over the specific domain (such as monoid or ring) where they
apply.  These simplifications and abstractions help a lot
in a large language with more base types and primitive operations.
Incidentally, the code actually contains a more general implementation
mechanism for such features, inspired in part by previous work in
generative linear algebra~\citep{CaretteKiselyov05}.

Any partial evaluator must decide how much to unfold recursion
statically: unfolding too little can degrade the residual code, whereas
unfolding too much risks nontermination.  Our partial evaluator is no
exception, because our object language includes |fix|.  The code in
Figure~\ref{fig:pe} takes the na\"\i ve approach of ``going all the
way'', that is, whenever the 
argument is static, we unfold |fix| rather than residualize it.
A conservative alternative is to unfold recursion only once, then residualize:
\begin{code}
let fix f = f (pdyn (C.fix (fun x -> abstr (f (pdyn x)))))
\end{code}
Many sophisticated approaches have been developed to decide how much to unfold
\citep{Jones-Mix,jones-partial}, but this issue is 
orthogonal to our presentation.
A separate concern in our treatment of |fix| is possible code bloat in
the residual program, which calls for let-insertion
\citep{BondorfDanvy,SwadiTahaKiselyovPasalic2006}.


Given this implementation of~|P|, our running example
\begin{code}
let module E = EX(P) in E.test1 ()
\end{code}
evaluates to
\begin{code}
{P.st = Some true; P.dy = .<true>.}
\end{code}
of type |('a, bool, bool) P.repr|.  Unlike with~|C| in~\S\ref{S:compiler},
a $\beta$-reduction has been statically performed to yield |true|.  More
interestingly, whereas |testpowfix7| compiles to a code value with many
$\beta$-redexes in~\S\ref{S:compiler}, the partial evaluation
\begin{code}
let module E = EX(P) in E.testpowfix7
\end{code}
gives the desired result
\begin{code}
{P.st = Some <fun>;
 P.dy = .<fun x -> x * (x * (x * (x * (x * (x * x)))))>.}
\end{code}

\noindent All pattern\hyp
matching in~|P| is \emph{syntactically} exhaustive, so it is patent to the
metalanguage implementation that |P| never gets stuck.  Further, all
pattern\hyp matching occurs
during partial evaluation, only to check if a value is known statically,
never what type it has.  In other words, our partial evaluator tags
phases (with |Some| and |None|) but not object types.

Our typed partial evaluator is online and polyvariant.  It reuses the
compiler~|C| and the evaluator~|R| by composing them.  This situation is
simpler than \citets{SperberThiemann:TwoForOne} composition of a partial
evaluator and a compiler, but the general ideas are similar.
\fi


% don't even have space for CPS
\ifshort\else
\section{Continuation\hyp passing style}\label{variations}

Our approach accommodates
several variants, including
a call-by-name CPS interpreter and a call-by-value CPS
transformation.
\ifshort\label{state}\label{S:CPS}%
This lets us decouple the evaluation strategy of the object language
from that of the metalanguage. The accompanying code shows the CBN CPS
interpreter (module |RCN| implementing |Symantics|) and a CBV CPS
transformer |CPST|. The latter explicitly maps CPS interpretations to
(direct) interpretations performed by the base interpreter~|S|. All
these interpreters are typed, tagless and \emph{type-preserving} (as
well as fully polymorphic in the answer-type). The type preservation
is the consequence of the type soundness of the metalanguage.  We can
modify the CBV CPS transformation to pass a piece of state along with
the continuation. This technique lets us support mutable state.  Due
to the severe lack of space we cannot describe these interpreters and
refer the reader to the accompanying code.

\else

\subsection{Call-by-name CPS interpreters}\label{S:CPS}

The object language generally inherits the evaluation strategy from
the metalanguage---call-by-value (CBV) in OCaml, call-by-name (CBN) in
Haskell.\footnote{To be more precise, most Haskell implementations
use call-by-need, which is observationally equivalent to call-by-name
because sharing is not observable \citep{ariola-call-by-need-popl}.}
To represent a CBN object language in a CBV metalanguage,
\citet{reynolds-definitional,reynolds-relation} and \citet{PlotkinCBN}
introduce CPS to make the evaluation strategy of a definitional
interpreter indifferent to that of the metalanguage. To achieve the same
indifference in the typed setting, we build a CBN CPS interpreter for
our object language in OCaml.

The interpretation of an object term is a function
mapping a continuation~|k| to the answer
returned by~|k|.
\begin{code}
let int (x:int) = fun k -> k x
let add e1 e2 = fun k -> e1 (fun v1 -> e2 (fun v2 -> k (v1 + v2)))
\end{code}
In both |int| and |add|, the interpretation has type 
|(int -> 'w)|\texttt{ }|-> 'w|, where |'w| is the (polymorphic) answer type.

Unlike CBV CPS, the CBN CPS interprets
abstraction and application as follows:
\begin{code}
let lam f = fun k -> k f
let app e1 e2 = fun k -> e1 (fun f -> f e2 k)
\end{code}
Characteristic of CBN, |app e1 e2|
does not evaluate the argument~|e2| by applying it to the
continuation~|k|. Rather, it passes |e2| unevaluated to the abstraction.
Interpreting $\fun{x} x+1$ yields type
\begin{code}
((((int -> 'w1) -> 'w1) -> (int -> 'w1) -> 'w1) -> 'w2) -> 'w2
\end{code}

We would like to collect those interpretation functions into a module
with signature |Symantics|, to include the CBN CPS interpreter within our
general framework. Alas, as in~\S\ref{S:PE-problem}, the type of
an object term inductively determines the type of its interpretation:
the interpretation of an object term of type~$t$ may not have type
|(|$t$|->'w)->'w|, because $t$ may be a function type.  Again we
simulate a type function with a typecase distinction, using an extra
type argument to |repr|. Luckily, the type function |repr_pe| needed for
the partial evaluator 
in~\S\ref{S:PE-solution} is precisely the same type function we
need for CBN CPS\@.
\begin{code}
module RCN = struct
  type ('c,'sv,'dv) repr = {ko: 'w. ('sv -> 'w) -> 'w}
  let int (x:int) = {ko = fun k -> k x}
  let add e1 e2 = {ko = fun k ->
      e1.ko (fun v1 -> e2.ko (fun v2 -> k (v1 + v2)))}
  let if_ eb et ee = {ko = fun k ->
      eb.ko (fun vb -> if vb then (et ()).ko k else (ee ()).ko k)}
  let lam f = {ko = fun k -> k f}
  let app e1 e2 = {ko = fun k -> e1.ko (fun f -> (f e2).ko k)}
  let fix f = let rec fx f n = app (f (lam (fx f))) n in lam (fx f)
  let run x = x.ko (fun v -> v)
end
\end{code}

This interpreter~|RCN| is fully polymorphic over the answer type,
using higher-rank polymorphism through OCaml record types.
It could also be a functor parameterized over
the answer type.

Because |RCN| has the signature |Symantics|, we can instantiate our previous
examples with it, and all works as expected.  More interesting
is the example $(\fun{x}1)\bigl((\fix{f}f)\mathinner2\bigr)$, which terminates
under CBN but not CBV\@.
\begin{code}
module EXS(S: Symantics) = struct open S
 let diverg () = app (lam (fun x -> int 1)) 
                     (app (fix (fun f->f)) (int 2))
end
\end{code}
Interpreting |EXS| with the |R| interpreter of
\S\ref{S:interpreter-RL} does not terminate.
\begin{code}
let module M = EXS(R) in M.diverg ()
\end{code}
In contrast, the CBN interpreter gives the result~|1|.
\begin{code}
let module M = EXS(RCN) in RCN.run (M.diverg ())
\end{code}

\subsection{CBV CPS transformers}

Changing one definition turns our CBN CPS interpreter into CBV\@.
\begin{code}
module RCV = struct include RCN
  let lam f = {ko = fun k -> k
      (fun e -> e.ko (fun v -> f {ko = fun k -> k v}))}
end
\end{code}
Now an applied abstraction
evaluates its argument before proceeding. This approach is in
line with \citets{reynolds-relation}, albeit typed. The
interpreter~|RCV| is useful for CBV evaluation of the object language
whether the metalanguage is CBV or CBN\@.

We turn to a more general approach to CBV CPS: a CPS transformer that
turns any implementation of |Symantics| into a CPS version of that
evaluator.
This functor on interpreters performs a textbook
CPS transformation on the object language.
\begin{code}
module CPST(S: Symantics) = struct
  let int i = S.lam (fun k -> S.app k (S.int i))
  let add e1 e2 = S.lam (fun k -> S.app e1 (S.lam (fun v1 ->
                                  S.app e2 (S.lam (fun v2 ->
                                  S.app k (S.add v1 v2))))))
  let lam f = S.lam (fun k -> S.app k
              (S.lam (fun x -> f (S.lam (fun k -> S.app k x)))))
  let app e1 e2 = S.lam (fun k -> S.app e1 (S.lam (fun f ->
                                  S.app e2 (S.lam (fun v ->
                                  S.app (S.app f v) k)))))
  let fix = S.fix
end
\end{code}
This (abbreviated) code explicitly maps CPS interpretations to
(direct) interpretations performed by 
the base interpreter~|S|.

The module returned by |CPST| does not define |repr|
and thus does not have signature |Symantics|.
The reason is again the type of |lam f|. Whereas
|int| and |add| return the (abbreviated) type
|('c, ..., (int -> 'w) -> 'w) S.repr|,
the type of \texttt{lam (add (int~1))} is
\begin{code}
('c, ..., ((int -> (int -> 'w1) -> 'w1) -> 'w2) -> 'w2) S.repr
\end{code}
Hence, to write the type equation defining |CPST.repr| we again need
a type function with a typecase distinction, similar to |repr_pe|
in~\S\ref{S:PE-solution}. Alas, the type function we need is not
identical to |repr_pe|, so we need to add another type argument to
|repr| in the |Symantics| signature. As in~\S\ref{S:PE-solution}, the
terms in previous implementations of |Symantics| stay unchanged, but the
|repr| type equations in those implementations have to take a new
(phantom) type argument.

For brevity, we just
use the module returned by |CPST| as is. Because it does not
match the signature |Symantics|, we cannot apply the |EX| functor to it.
Nevertheless, we can write the tests.
\begin{code}
module T = struct
  module M = CPST(C)
  open M
  let test1 () = (* same as before *)
       app (lam (fun x -> x)) (bool true)
  let testpowfix () = ... (* same as before *)
  let testpowfix7 = (* same as before *)
       lam (fun x -> app (app (testpowfix ()) x) (int 7))
end
\end{code}
We instantiate |CPST| with the desired base interpreter~|C|,
then use the result |M| to
interpret object terms. Those terms are \emph{exactly} as before.
Having to textually copy the terms is the
price we pay for this simplified treatment.
Our discussion of self\hyp interpretation in~\S\ref{selfinterp} shows
that this copying is not frivolous but represents plugging a term into
a context, which is one of the many faces of polymorphism.

With 
|CPST| instantiated by the compiler~|C| above,
|T.test1| gives
\begin{code}
.<fun x_5 -> (fun x_2 -> x_2 (fun x_3 x_4 -> x_4 x_3))
             (fun x_6 -> (fun x_1 -> x_1 true)
                         (fun x_7 -> x_6 x_7 x_5))>.
\end{code}
This output is a na\"{\i}ve CPS transformation of $(\fun{x}x)\True$,
containing several apparent $\beta$-redexes.  To reduce these
redexes, we just change~|T| to instantiate |CPST| with |P| instead.
\begin{code}
{P.st = Some <fun>; P.dy = .<fun x_5 -> x_5 true>.}
\end{code}

\subsection{State and imperative features}
\label{state}

\begin{figure}
    \begin{floatrule}
    \begin{proofrules}
        \[ \justifies \deref:t_s \]
        \[ e:t_s \justifies \set e:t_s \]
        \[ e_1:t_1 \quad \[ [x:t_1] \proofoverdots e_2:t_2 \] \justifies \lapp{e_1}{x}{e_2}:t_2 \]
    \end{proofrules}
    \end{floatrule}
    \caption{Extending our typed object language with mutable state of type~$t_s$}
    \label{fig:state}
\end{figure}

We can modify a CBN or CBV CPS transformation to pass a piece of state
along with the continuation. This technique lets us support mutable
state. As Figure~\ref{fig:state} shows, we extend our object language
with three imperative features.
\begin{enumerate}
    \item ``$\deref$'' gets the current state;
    \item ``$\set e$'' sets the state to the value of~$e$ and returns
        the previous value of the state;
    \item the let-form ``$\lapp{e_1}{x}e_2$'' evaluates $e_1$
        before~$e_2$ even if $e_2$ does not use~$x$.
\end{enumerate}
If $x$ does not appear in~$e_2$, then ``$\lapp{e_1}{x}e_2$'' is same as
the more familiar sequencing form ``$e_1;e_2$''.
We can embed this extended object language into OCaml by extending the
|Symantics| signature in Figure~\ref{fig:ocaml}.
\begin{code}
module type SymSI = sig
  include Symantics
  type state
  type 'c states       (* static version of the state *)
  val lapp : (('c,'sa,'da) repr as 'x) -> ('x -> 'y)
             -> (('c,'sb,'db) repr as 'y)
  val deref : unit -> ('c, 'c states, state) repr
  val set   : (('c, 'c states, state) repr as 'x) -> 'x
end
\end{code}
In HOAS\@, we write the term ``$\lapp{e_1}{x}e_2$'' as |lapp e1 (fun x -> e2)|;
the type of |lapp| is that of function application with
the two arguments swapped.  We can encode the term
``$\lapp{\deref}{x} (\set 2;\, x+\deref)$''
as the OCaml functor
\begin{code}
module EXSI_INT(S: SymSI
  with type state = int and type 'c states = int) = struct open S
  let test1 () = lapp (deref ()) (fun x -> 
                  lapp (set (int 2)) (fun _ -> add x (deref ())))
end
\end{code}
The accompanying source code shows several more tests, including
a test for higher-order state and a power function that uses state
as the accumulator.
Because the |SymSI| signature extends |Symantics|, any encoding of
a term in the pure object language (that is, any functor that takes
a |Symantics| module as argument) can also be used as a term in the
extended object language (for example, applied to an implementation of
|SymSI|).

The state-passing interpreter extends the CBN CPS
interpreter |RCN| of~\S\ref{S:CPS}.
\begin{code}
module RCPS(ST: sig 
  type state 
  type 'c states 
  type ('c,'sv,'dv) repr = 
      {ko: 'w. ('sv -> 'c states -> 'w) -> 'c states -> 'w}
end) = struct
  include ST
  type ('c, 'sv, 'dv) result = 'c states -> 'sv
  ... 
  let lapp e2 e1 = {ko = fun k ->
      e2.ko (fun v -> (app (lam e1) {ko = fun k -> k v}).ko k)}
  let deref () = {ko = fun k s -> k s s}
  let set e = {ko = fun k -> e.ko (fun v s -> k s v)}
  let get_res x = fun s0 -> x.ko (fun v s -> v) s0
end
\end{code}
The implementations of |int|, |app|, |lam|, and so on are
\emph{identical} to those of |RCN| and elided. New are the extended type
|repr|,
which now includes the state, and the functions
|lapp|, |deref|, and |set| representing imperative features. The
interpreter is still CBN, so evaluating |app ef ea| might not
evaluate |ea|, but evaluating |lapp ea ef| always does.
For first-order state, such as of type~$\ZZ$, we 
instantiate the interpreter as
\begin{code}
module RCPSI = RCPS(struct 
  type state = int
  type 'c states = int
  type ('c,'sv,'dv) repr = 
      {ko: 'w. ('sv -> 'c states -> 'w) -> 'c states -> 'w}
end)
\end{code}
If the state has a higher-order type, then the types |state| and |'c states|
are no longer the same, and |'s states| is mutually
recursive with the type |('c,'sv,'dv) repr|, as demonstrated in the
accompanying source code.

|RCPSI| matches the |Symantics| signature and implements the
unextended object language: we can pass |RCPSI|
to the functor |EX| (Fig.~\ref{fig:ocaml-simple}) and run 
the example |test1| from there. The main use for |RCPSI| is to interpret the 
extended object language.
\begin{code}
module EXPSI_INT = EXSI_INT(RCPSI)
let cpsitesti1 = RCPSI.get_res (EXPSI_INT.test1 ()) 100
val cpsitesti1 : int = 102
\end{code}

It is worthwhile reiterating how close this implementation is to the CPS
interpreter, and that adding state and imperative features needed no new
techniques.
We can also add mutable references to the object language using
mutable references of the metalanguage, as shown in the accompanying
code.
\fi
\fi

\ifshort\else
\section{Self-interpretation}\label{selfinterp}

We turn to interpreting the object language in the object language, to
clarify how expressive our typed object language can be and to argue that our
partial evaluator is Jones\hyp optimal.

Given an \emph{encoding} of each object term~$e$ as an \emph{object} term~$\Encode{e}$,
a \emph{self\hyp interpreter} is usually defined as an object
function~$\si$ such that any object term~$e$ is observationally
equivalent to the object application $\si\Encode{e}$
\citep{jones-partial,taha-tag,Danvy-tagging-encoding}.
A partial evaluator~$\pe$ maps object terms~$e$ to observationally
equivalent object terms~$\pe(e)$.  It is said to be
\emph{optimal} with respect to~|si| \citep{jones-challenging} if $\pe(\si\Encode{e})$
is equal to~$e$ (up to $\alpha$\hyp conversion, or in some accounts, no
less efficient than~$e$).

Intuitively, self\hyp interpretation is straightforward in our
framework: the functions comprising the interpreters
in~\S\ref{S:interpreter-RL} may just as well be written in our object
language:~Fig.~\ref{fig:self-eval}.
\begin{figure}
\begin{align*}
    \ident{int} &= \fun{x} x \\
    \ident{add} &= \fun{x} \fun{y} x+y \\
    \ident{if\_}&= \fun{b} \fun{t} \fun{e} \cond{b}{t\,0}{e\,0}
        \displaybreak[0] \\
    \ident{lam} &= \fun{f} f \\
    \ident{app} &= \fun{f} \fun{x} fx \\
    \ident{fix} &= \fun{g} \fix{f} \fun{x} gfx
\end{align*}
\caption{The \emph{object} functions implementing an
evaluator.  (We use the number~$0$ in lieu of a unit value.)}
\label{fig:self-eval}
\end{figure}
We thus map each object term~$e$ to an object term~$\encode{e}$ as follows.
We call this mapping \emph{pre-encoding}.
\begin{align*}
    \encode{x} &= x \\
    \encode{n} &= \ident{int}\, n \\
    \encode{e_1 + e_2} &= \ident{add} \encode{e_1} \encode{e_2} \\
    \encode{\cond{b}{t}{e}} &= \rlap{$\ident{if\_} \encode{b}
        \left(\fun{\_}\encode{t}\right) \left(\fun{\_}\encode{e}\right)$}
        \displaybreak[0] \\
    \encode{\fun{x}e} &= \ident{lam} (\fun{x} \encode{e}) \\
    \encode{fx} &= \ident{app} \encode{f} \encode{x} \\
    \encode{\fix{f}e} &= \ident{fix} (\fun{f} \encode{e})
\end{align*}
The metavariables $x$ and~$n$ stand for a variable and an integer,
respectively.
This pre-encoding is just like how we represent object terms in the
metalanguage in the preceding sections, but it produces
terms in the object language rather than the metalanguage.

To evaluate~$\encode{e}$, then, we
instantiate the free variables in~$\encode{e}$ such as $\ident{int}$,
$\ident{lam}$, and $\ident{add}$ by their definitions given in
Fig.~\ref{fig:self-eval}.  For
example, the familiar object term $(\fun{x}x)\True$ pre-encodes to
\begin{equation*}
    \encode{(\fun{x}x)\True} = \ident{app}
    (\ident{lam} (\fun{x} x))\, (\ident{bool} \True),
\end{equation*}
and to evaluate this pre-encoded term is to evaluate the object term
\begin{equation*}
    (\fun{f} \fun{x} fx) \,
    ((\fun{f} f) (\fun{x} x))\, ((\fun{b} b) \True).
\end{equation*}
Because the evaluator in Fig.~\ref{fig:self-eval} mostly consists 
of glorified identity
functions, our simple partial evaluator reduces the
result of this instantiation to~$e$.  In general, to
interpret~$\encode{e}$ using an interpreter is to instantiate its free
variables by that interpreter's definitions.

\subsection{Avoiding higher polymorphism}

Any approach to self\hyp interpretation needs to spell out first how to
encode object terms~$e$ to object terms~$\Encode{e}$, and then how to
interpret~$\Encode{e}$ in the object language.  For our approach, we
want to define encoding in terms of pre-encoding, and interpretation
using some notion of instantiation.  Unfortunately, the simple type
structure of our object language hinders both tasks.  To continue with
the example term above, we could try to define
\begin{equation*}
    \Encode{e} =
    \fun{\ident{app}} \fun{\ident{lam}} \fun{\ident{bool}} \encode{e},
\end{equation*}
in particular
\begin{equation*}
    \Encode{(\fun{x}x)\True} =
    \fun{\ident{app}} \fun{\ident{lam}} \fun{\ident{bool}}
    \ident{app} (\ident{lam} (\fun{x} x))\, (\ident{bool} \True).
\end{equation*}
To type-check this encoded term, we give the variable $\ident{lam}$ the
simple type $(\BB\to\BB)\to\BB\to\BB$.
We then define the self\hyp interpreter
\begin{equation*}
    \si = \fun{e} e
    (\fun{f} \fun{x} fx)
    (\fun{f} f)
    (\fun{b} b)
\end{equation*}
and apply it to the encoded term.  The result is the object term
\begin{equation*}
    \bigl(\fun{e} e (\fun{f} \fun{x} fx) (\fun{f} f) (\fun{b} b)\bigr)
    \bigl(
    \fun{\ident{app}} \fun{\ident{lam}} \fun{\ident{bool}}
    \ident{app} (\ident{lam} (\fun{x} x))\, (\ident{bool} \True)
    \bigr),
\end{equation*}
which partially evaluates to~$\True$ easily.
However, encoding fails on a term with multiple $\lambda$\hyp
abstractions at different types.  For example, the pre-encoding
\begin{equation*}
    \encode{\fun{f}\fun{x}fx}
    = \ident{lam} (\fun{f} \ident{lam} (\fun{x} \ident{app} f x))
\end{equation*}
does not type-check in any typing environment, because $\ident{lam}$ needs
to take two incompatible types.  In sum, we need more polymorphism in the
object type system to type $\ident{lam}$, $\ident{app}$, $\ident{fix}$,
and~$\ident{if\_}$ (and $\Encode{e}$ and~$\si$).
(The polytypes in |Symantics| given by Haskell's type classes and OCaml's
modules supply this polymorphism.)  Moreover, we need to encode any
polymorphism of the object language \emph{into} the object language to achieve
self\hyp interpretation.

\subsection{Introducing let-bound polymorphism}

Instead of adding higher-rank and higher-kind polymorphism to our object
language (along with polymorphism over kinds!), we add let-bound polymorphism.
As usual,
we can add a new typing rule
\begin{equation*}
    \let\vcenter\vbox
    \begin{prooftree}
        e_1:t_1 \quad \subst{e_2}{x}{e_1}:t_2
        \justifies \be{x=e_1} e_2 : t_2
    \end{prooftree}
    .
\end{equation*}
The pre-encoding of a let\hyp expression is trivial.
\begin{align*}
    \encode{\be{x=e_1}e_2} \quad &= \quad \be{x=\encode{e_1}} \encode{e_2}
\intertext{A \emph{context} is an object term with a hole~$[~]$.  The
hole may occur under a binder, so plugging a term into the context may
capture free variables of the term.  By pre-encoding a hole to a hole,
we extend pre-encoding from a translation on terms to one on
contexts.}
    \encode{[~]} \quad &= \quad [~]
\end{align*}
We define an interpreter in the object language to be not a term but
a context.  For example, the evaluator is the context
\begin{align*}
    \si[~] &=
    \begin{tabular}[t]{@{}Ml@{}>{{}}Ml@{}Ml@{}}
        \be{\ident{int} &= \fun{x} x&} \\
        \be{\ident{add} &= \fun{x} \fun{y} x+y&} \\
        \be{\ident{if\_}&= \fun{b} \fun{t} \fun{e} \cond{b}{t\,0}{e\,0}&} \\
        \be{\ident{lam} &= \fun{f} f&} \\
        \be{\ident{app} &= \fun{f} \fun{x} fx&} \\
        \be{\ident{fix} &= \fun{g} \fix{f} \fun{x} gfx&} [~],
    \end{tabular}
\intertext{and the size\hyp measurer at the end of~\S\ref{S:interpreter-RL} is the context}
    \mathrm{SZ}[~] &=
    \begin{tabular}[t]{@{}Ml@{}>{{}}Ml@{}Ml@{}}
        \be{\ident{int} &= \fun{x} 1&} \\
        \be{\ident{add} &= \fun{x} \fun{y} x+y+1&} \\
        \be{\ident{if\_}&= b + t\,0 + e\,0&} \\
        \be{\ident{lam} &= \fun{f} f\,0 + 1&} \\
        \be{\ident{app} &= \fun{f} \fun{x} f + x + 1&} \\
        \be{\ident{fix} &= \fun{g} g\,0 + 1&} [~].
    \end{tabular}
\end{align*}
To interpret an object term~$e$ using an interpreter $I[~]$ is to
evaluate the object term~$I[\encode{e}]$.  $\si$~is
a self\hyp interpreter in the following sense.
\begin{proposition}
    $\si[\encode{e}]$ is observationally equivalent to~$e$.
\end{proposition}
As a corollary, we can pre-encode $\si$ itself as
a context: the term $\si[\encode{\si[\encode{e}]}]$ is observationally
equivalent to $\si[\encode{e}]$, and in turn to~$e$.  In other words,
$\si$ can interpret itself.  Our partial evaluator is optimal with respect to
the self\hyp interpreter~$\si$.
\begin{proposition}
    Let $\pe$ be the partial evaluator~|P| in~\S\ref{S:PE-solution}.
    Then the object terms $\pe(\si[\encode{e}])$ and~$\pe(e)$ are
    either both undefined or both defined and
    equal up to $\alpha$\hyp conversion.
\end{proposition}

\subsection{Contexts clarify polymorphism}
\label{S:clarify}

We always type-check a pre-encoded term~$\encode{e}$ and its interpreter~$I[~]$
together, never separately.  This treatment has the drawback that
we must duplicate a pre-encoded term in order to interpret it in
multiple ways.  The meta-notions of contexts and plugging may seem ad
hoc, but in fact they just reflect the type-class and module
machinery that we have been using in this paper all along.

In the presence of let-bound polymorphism, we can understand a term
waiting to be plugged into a context as a higher-rank and higher-kind
abstraction over the context.  Even though our object language does not support
higher abstraction, our metalanguages do, so they can type-check an object term
separately from its interpreter---either as a functor from a |Symantics| module
containing a type constructor
(in OCaml), or a value with a |Symantics| constraint over a type
constructor (in Haskell).  Thus,
``context'' is a euphemism for a polymorphic argument, and ``plugging''
is a euphemism for application.

\begin{comment}
\jacques{But how 
do you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?}
\oleg{Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.}
Again, what is different from the above is that we can
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).

\oleg{Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).}
\end{comment}

\begin{comment}

The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
\texttt{app \_add (app \_int 1) (app \_int 2)} where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is
$$ 
  (\ZZ \rightarrow r \ZZ) \rightarrow
  (\BB \rightarrow r \BB) \rightarrow
  \forall \alpha \beta. (r \alpha \rightarrow r \beta)
      \rightarrow r (\alpha\rightarrow\beta) \rightarrow ... r \tau
$$
% (Int -> r Int) ->
% (Bool -> r Bool) ->
% (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau

or, if we denote the sequence of initial arguments as |S r|, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.

\end{comment}
\fi

\section{Related work}\label{related}

\ifshort
Our initial motivation came from several papers 
\citep{WalidICFP02,taha-tag,xi-guarded,peyton-jones-simple}
that use embedded interpreters to justify advanced
type systems, in particular GADTs.
\else
Our initial motivation came from several papers that justify advanced
type systems, in particular GADTs, by embedded interpreters
\citep{WalidICFP02,taha-tag,xi-guarded,peyton-jones-simple} and
CPS transformations \citep{Guillemette-Monier-PLPV,shao-type-toplas,chen-typeful}.
\fi
We admire all this technical machinery, but
these motivating examples do not need it.
Although GADTs may indeed be simpler and more flexible, they are
unavailable in mainstream ML, and their implementation in GHC
6.6.1 fails to
detect exhaustive pattern matching.  We also wanted to find the minimal
set of widespread language features needed for tagless
type-preserving interpretation.

Even a simply typed $\lambda$-calculus obviously supports self\hyp
interpretation, provided we use universal types \citep{taha-tag}.  The
ensuing tagging overhead motivated \citet{Makholm-TagElim,taha-tag} 
to propose tag
elimination, which however does not statically guarantee that all tags
will be removed \citep{WalidICFP02}.

\Citet{WalidICFP02}, \citet{taha-tag}, \citet{xi-guarded}, and
\citet{peyton-jones-simple} seem to argue as follows that a self\hyp
interpreter of a typed language cannot be tagless or Jones\hyp optimal:
\begin{shortlist}
\item One needs to encode a typed language in a typed language based on
a sum type (at some level of the hierarchy)\\
\item A \emph{direct} interpreter 
for such an encoding of a typed language
in a typed language requires either
advanced types or tagging overhead\\
\item Thus, an indirect interpreter is necessary, which needs a universal
  type and hence tagging\ifshort\else\\
\item Thus, any self-interpreter must have tags and cannot be 
  Jones-optimal\fi.
\end{shortlist}
While the logic is sound, we (following \citet{yang-encoding}) showed that the
first step's premise is not valid.

\citet{Danvy-tagging-encoding} discuss Jones optimality at length and
apply HOAS to typed self\hyp interpretation.  However, their source
language is untyped.  Therefore, their object\hyp term encoding has
tags, and their interpreter can raise run-time errors.
Nevertheless, HOAS lets the partial
evaluator remove all the tags. In contrast, our object encoding and
interpreters do not have tags to start with and obviously cannot
raise run-time errors.

\ifshort\else
A lot of effort has gone into ``typing dynamic typing'': to statically
type-check dynamically\hyp typed values
\citep{baars-typing,WalidICFP02,Guillemette-Monier-PLPV,haskell-list},
using the host language's type system to varying extents.
Our object terms are statically typed, so we would
need one of these techniques to interpret dynamically\hyp typed
terms such as those read from a file.
\oleg{First, I suggest we remove the subjunctive mood: `we would
  need...' We have the code already, IncopeTypecheck.hs. We know we
  can typecheck untyped terms so that we do the typechekcing only
  once, and interpret many times. All these interpretation is guranteed to
  be free from any type errors. And this holds even for higher-order
  terms encoded in HOAS. I also suggest we drop
  \cite{haskell-list}. We included the citation solely to show how 
  we could go on. Both links in haskell-list are not actually
  appropriate: first, the object language is \emph{first-order}
  and second they use GADTs. Our IncopeTypecheck uses only
  Typeable and existentials, and no GADTs.
  Finally, we need to find a way to talk about IncopeTypecheck more
  directly, probably in the Introduction -- Sumii-san has directly
  suggested we emphasize that result. For some reason, people (Walid
  and Sumii-san, for ones) consider it important. Incidentally, I
  have the comparison of IncopeTypecheck with `Typing Dynamic
  Typing', which we can use if needed.}
\fi

Our partial evaluator establishes a bijection |repr_pe| between static
and dynamic types (the valid values of |'sv| and |'dv|), and between
static and dynamic terms.  It is customary to implement such a bijection
using an injection\hyp projection pair, as done for interpreters
\ifshort \citep{Ramsey-ML-module-mania,Benton-embedded-interpreters}\else
by \citet{Ramsey-ML-module-mania} and \citet{Benton-embedded-interpreters}\fi,
partial evaluation \ifshort \citep{Danvy-TDPE}\else by \citet{Danvy-TDPE}\fi,
and type-level functions \ifshort \citep{oliveira-typecase}\else by
\citet{oliveira-typecase}\fi.  As explained in~\S\ref{S:PE-solution}, we
avoid injection and projection at the type level by adding an argument
to |repr|.
Our solution could have been even more straightforward if MetaOCaml
provided total type-level functions such as |repr_pe| in
\S\ref{S:PE-solution}---simple type-level computations 
ought to become mainstream.

At the term level, we also avoid converting between static and dynamic
terms by building them in parallel, as \citet{asai-binding-time} does
in his partial evaluator.
This method type-checks in Hindley-Milner once we
deforest the object term representation.  Put another way, we
manually apply type-level partial evaluation to our type
functions (see \S\ref{S:PE-solution}) to obtain simpler types 
acceptable to MetaOCaml.

\Citets{JFP-Mogensen} self-reducer for the untyped
$\lambda$-calculus preceded Asai's partial evaluator in constructing
static and dynamic terms in parallel.  However, this self-reducer
constructs a static term for every object term, even a bound variable,
so (in terms of Fig.~\ref{fig:pe}) it moves some work from |app|
to~|pdyn|.  In contrast, we follow Asai and construct the static term
optionally, so as to perform type-directed lifting without requiring the
encoding of an object term to juggle explicit type indices.

\Citet{sumii-hybrid} also use Asai's construction,
to combine online
and offline partial evaluation.
They predate us in deforesting the object term representation to enable tagless
partial evaluation.
We strive for modularity by reusing interpreters for individual stages
\citep{SperberThiemann:TwoForOne}: our partial evaluator~|P|
reuses our tagless evaluator~|R| and tagless compiler~|C|, so it
is patent that the \emph{output} of~|P| never gets
stuck.  It would be interesting
to try to derive a \emph{cogen} \citep{Thiemann:cogeninsixlines}
in the same manner.

It is common to implement an embedded DSL by providing multiple
interpretations of host\hyp language pervasives such as addition and
application.  It is also common to use phantom types to rule out
ill-typed object terms, as done in Lava
\citep{Lava} and by \citet{Rhiger-thesis}. However, these approaches
are not tagless because they still use universal types, such as Lava's
\texttt{Bit} and \texttt{NumSig}, and Rhiger's \texttt{Raw} (his Fig.~2.2)
and \texttt{Term} (his Chap.~3), which incur the attendant overhead of
pattern matching.  The universal type also
greatly complicates the soundness and completeness proofs of embedding
\citep{Rhiger-thesis}, whereas our proofs are trivial.
Rhiger's approach does not support typed CPS transformation (his~\S3.3.4).
\begin{comment}
Rhiger's But Fig 2.2, p33: universal type Raw.  He uses phantom type
upon the Exp datatype. But that is cheating: phantom type means
essentially we can easily do coerce. We use real types.  That's why he
had to do tedious proofs in Sec 2 of soundness and completeness of
embedding. Whereas our proofs are obvious.  His sec 3 is based on data
representation of terms. They have type tags.  We do nothing of that
kind: See Sec 3.1.2. See numerous "data Term" in Sec3, which is the U
type.  In Sec 3.3.4 (p76) Rhiger specifically says that his encoding
cannot do typed CPS transformation -- whereas our does. BTW, Rhiger
thesis contains the definitions of the interpreter and the compiler,
in the beginning. Use this in response to Rev1)
\end{comment}


We are not the first to implement a typed interpreter for a typed
language.  \Citet{laod93} use type classes to implement a metacircular
interpreter (rather than a self\hyp interpreter) of a
typed version of the SK language, which is quite different from our
object language.  Their interpreter
appears to be tagless, but they could not have implemented a
compiler or partial evaluator in the same way, since they rely
heavily on injection\hyp projection pairs.

\Citet{fiore:nbe-ppdp2002} and \citet{balat:tdpe-popl2004} also build
a tagless partial evaluator, using delimited control operators.  It is
type-directed, so the user must represent, as a term, the type of every
term to be partially evaluated.  We shift this work to the type checker
of the metalanguage.  By avoiding term-level type representations, our
approach makes it easier to perform algebraic simplifications (as
in~\S\ref{S:PE-solution}).

\oleg{The following are almost unedited excerpts from our
  correspondence with Sumii about Birkedal's thesis. We definitely
  should include this discussion and the reference to his thesis.}
\Citet{Birkedal-PE-ML} have hand-written a compiler generator (cogen)
for Core Standard ML, which transforms a program into its tagless
generating extension (and therefore serves as a tagless partial
evaluator, intepreter, and compiler in our sense). They still have to
use the "universal type" at the cogen level, which is removed from
compile-time, so that the end result is tagless indeed.  The main
difference between the work of \Citet{Birkedal-PE-ML} and ours is how
\emph{early} it becomes \emph{manifest} to the metalanguage
that our code is safe.  The fact that
\Citet{Birkedal-PE-ML} 's generating extension never generates a
specialized program that is ill-typed is not manifest to the SML
compiler until the SML compiler sees each particular specialized
program.  Moreover, and in a similar vein, the fact that
\Citet{Birkedal-PE-ML} 's cogen never generates a generating extension
that is ill-typed (or that may generate a specialized program that is
ill-typed) is not manifest to the SML compiler until the SML compiler
sees each particular generating extension (or each particular
specialized program).  This early and manifest type safety is the
payoff of our treating an object program not as a data structure but
as calls to abstract-syntax constructor functions.  The tradeoff, as
we currently see it, is that we have to swear off intensional analysis
of code. \oleg{Unless we convert to our representation to GADT, which
  is always possible, if GADTs are available.}



% The following paragraph was not in the version submitted to APLAS,
% yet was not commented out.  Of course, if we had space, we should add
% this, but it is difficult to see how.
\ifshort\else
Using Haskell, \citet{Guillemette-Monier-PLPV} implement a CPS transformation
for HOAS terms and statically assure that it preserves object types.
They represent proofs of type preservation as terms of a GADT, which is not
sound (as they admit in \S4.2) without a separate totality check because
any type is trivially inhabited by a nonterminating term in Haskell.
In contrast, our CPS transformations use simpler types than GADTs and
assure type preservation at the (terminating) type level rather than
the term level of the metalanguage.
\Citeauthor{Guillemette-Monier-PLPV} review other
type\hyp preserving CPS transformations (mainly in the context of typed intermediate
languages), in particular
\citets{shao-type-toplas} and \citets{chen-typeful}.
These approaches use de Bruijn indices and fancier
type systems with type-level functions, GADTs, or type\hyp equality
proofs.
\fi


We encode terms in elimination form, as a coalgebraic structure.
\Citet{Pfenning-Lee} first described this basic idea and applied it to
metacircular interpretation.
Our approach, however, can be implemented in mainstream ML and supports
type inference, typed CPS transformation and partial evaluation. In contrast,
\citeauthor{Pfenning-Lee} conclude that partial evaluation and program
transformations ``do not seem to be expressible'' even using their
extension to~$F_\omega$, perhaps because their avoidance of general
recursive types compels them to include the polymorphic lift that we
avoid in~\S\ref{S:PE-lift}.
\begin{comment}
It seems that Pfenning and Lee embed $F_2$ with type constructions in 
(pure) $F_3$.  We embed $F_1$ in (weak?) $F_2$, as I see it.  In a way, what 
we do is very similar to what they do (Figure 1, p.152), except that we 
do it in standard programming languages.  It is unclear if their work can 
be implemented (yet) in any language.  And we preserve type-inference, 
while their solution needs explicit types!
The following line of their conclusion is worth citing: "... this does 
not imply that the same language is also suitable for type 
metaprogramming. ... such as partial evaluation... do not seem to be 
expressible".
I suspect you're right, but I'm still reading the paper.  See also page
146: "for a term M in $F_1$ (a simply-typed term), the representation
$\bar{M}$ will be in $F_2$".  The move from $F_2$ to $F_3$ 
and beyond reminds me
strongly of our attempts at self-interpretation without the notion of a
syntactic hole.
\end{comment}

Our encoding of the type function |repr_pe| in \S\ref{S:PE-solution}
emulates type-indexed types and is related to intensional type analysis
\citep{Morrisett-intensional,Generic-Haskell}. However, our object
language and running examples in HOAS include |fix|,
which intensional type analysis cannot handle
\citep{xi-guarded}.  Our final approach
seems related to \citearound{'s approach to HOAS using catamorphisms and
  anamorphisms}\citet{Washburn-Weirich-boxes}.


We could not find work that establishes that
the \emph{typed} $\lambda$-calculus has a final coalgebra structure.
\ifshort
(See \Citet{honsell99coinductive} for the untyped case.)
\else
\Citet{HonsellLenisa,honsell99coinductive}
investigate the untyped $\lambda$-calculus
along this line.  
In particular, they use
contexts with a hole \citep[p.\,13]{honsell99coinductive} to define
\emph{observational equivalence}
(see our~\S\ref{selfinterp}).
\citearound{'s bibliography}\Citet{honsell99coinductive} refers to the
foundational work in this important area.  
Particularly intriguing is the link to the
coinductive aspects of B\"{o}hm trees, as pointed out by
\citet{berarducci-models} and Jacobs \citeyearpar[Example 4.3.4]{jacobs-coalgebra}.
\fi

\ifshort\else
One way to understand our main idea is to eschew sum types and sum kinds
for their dual, record types and record kinds.
For the self\hyp interpreter, we then proceed to use a Church encoding for
recursive data types \citep{bohm-automatic}.
\fi

\ifshort We observe that \else As \S\ref{S:clarify} observes, \fi
higher-rank and higher-kind
polymorphism lets us type-check and compile object terms separately from
interpreters.  This \ifshort\else observation \fi is consistent with the role of
polymorphism in the separate compilation of modules
\citep{shao-typed}.

\section{Conclusions}\label{conclusion}

We solve the problem of embedding a typed object language in a typed
metalanguage without using GADTs, dependent types, or a universal type.
Our family of interpreters includes an evaluator, a compiler, a partial
evaluator, and CPS transformers.  It is patent that they never get stuck,
because we represent object types as metalanguage types.  This work
makes it safer and more efficient to embed DSLs
in practical metalanguages such as Haskell and ML\@.

Our main idea is to represent object programs not in an initial algebra
but using the existing coalgebraic structure of the $\lambda$-calculus.
More generally, to squeeze more invariants out of a type system as
simple as Hindley-Milner, we shift the burden of representation and
computation from consumers to producers: encoding object terms as calls
to metalanguage functions (\S\ref{ourapproach}); build dynamic terms
alongside static ones (\S\ref{S:PE-lift}); simulating type functions for
partial evaluation (\S\ref{S:PE-solution}) and CPS
transformation\ifshort\else~(\S\ref{S:CPS})\fi.
This shift also underlies fusion,
functionalization, and amortized complexity analysis.
\ifshort\else
When the metalanguage does provide higher-rank and higher-kind
polymorphism, we can type-check and compile an object term separately
from any interpreters it may be plugged into.
\fi

Our representation of object terms in elimination form encodes
primitive recursive folds over the terms. 
\ifshort\else
This encoding makes operations like interpretation trivial to implement.
\fi
We still
have to understand if and how non-primitively 
recursive operations can be supported.

