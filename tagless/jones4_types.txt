[The current draft of the reply to Neil Jones. It still uses "I"]

	Regarding computability and types, I don't think the question
is entirely about Church vs Curry. I have recently become aware that
logicians have been investigating this very question for a long time:
I mean Goedel's Dialectica Interpretation (his System T) and the work
by Georg Kreisel, among others, on computability at higher
types. System T (from the point of view of terms and reductions) is
what we (in CS) call simply typed lambda-calculus with constants. I
should also point out a good article relating types and computability:

    http://math.andrej.com/2006/03/27/sometimes-all-functions-are-continuous

I think though the author should have, perhaps, spent more time on the
issue of representation. Once we move from Mathematics to programs, we
no longer deal with functions -- rather, we deal with
_representations_ of functions. The choice of representation matters a
great deal; for example, there is a representation for total
continuous functions that makes modulus expressible in a programming
language. We have recently done that, most likely rediscovering an old
result.

	Coming back to Church vs. Curry, I prefer taking a position
that a type is a static approximation of program's behavior: a type of
the program describes the set that surely contains the result of a
program (if there is one) as well as the set of all possible reduction
traces of the program. Frequently the most useful knowledge is not of
what is in this set -- rather, what is not in these sets of result
values and possible traces. For example, it could be comforting to
know that in every environment (given any inputs) the reductions of a
program do not include the transition that reformats the hard disk.

> ML versus SCHEME ?
> Somewhere at the heart of things lies some essential difference  
> between, for instance, the languages ML and SCHEME.
> Somehow SCHEME manages true self-interpretation, and a true self- 
> application of specialisers,

I'm puzzled at that contra-position. Surely ML can embed Scheme,
entirely. And in fact, I have done so, some eight years ago:
	http://okmij.org/ftp/Scheme/misc.html#ML-as-Scheme

> - You can read an arbitrary S-expression in at run time in SCHEME/ 
> LISP, even _without knowing_ which program it is that you are executing.
>
> - However in interaction with an ML program, the provider of input either
> ...
>    o  the program has to go through (expensive and cumbersome)  
> conversions between
>               --  a datatype of character strings and
>               -- whatever datatypes the main part of the program  
> really works with (balanced binary trees or whatever).

Here again I'm puzzled: the `read' procedure of Scheme has to do all
these cumbersome conversions between strings and internal
representation of values: symbols are interned, lists are represented
as linked structures (perhaps even cdr-coded) with no parentheses,
etc. I have seen the code for 'read' procedure in some Scheme systems,
and can testify it is quite involved.

Regarding self-interpretation: any typed language with fixpoint that
permits either recursive or higher-rank types permits
self-interpretation. I can say that because such a language permits
Church-encoding of algebraic data-types; the latter can represent AST
of any language that can be defined with AST, including itself. Thus
the language permits encoding of its own terms and the manipulation on
such encoded terms -- reductions. The problem of course is that most
often such an encoding of terms is `too wide': the encoding permits
more terms than there are _well-typed_ terms, and so the interpreter
must, at least potentially, deal with the `junk'.

Let me take another example: let's consider a language like
lambda-calculus in which each expression is its own type. That is not
too outlandish a suggestion: there are languages whose type system is
Turing Complete (Haskell with popular extensions, for example) and
there are systems whose language of types and the language of terms
are the same (e.g., Epigram). We can write a self-interpreter of this
language, right? So, we can't say that types per se preclude
self-interpretation. The type system of this language gives the
perfect approximation of programs' behavior -- and thus it is useless.


> Walid Taha's work has also confused me somewhat in this regard,  
> perhaps because his level-shifting all seems to occur _within_ the  
> context of one, single, surrounding programming language.

If you prefer, here's the rendition of MetaOCaml in Scheme:
	http://okmij.org/ftp/Computation/staging/meta-scheme.scm
It can represent any MetaOCaml code -- in Scheme, disregarding all
types. Operationally it is quite close to MetaOCaml (e.g., in
regards to cross-staged persistence).
