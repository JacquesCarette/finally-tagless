Reply to the message of Neil Jones as of Mar 31, 2008

> 1. Use of "Cogen functions" is said to be your main idea in the  
> abstract. Google and Wikipedia don't give relevant hits, and in the  
> partial evaluation literature, "cogen" has a meaning that doesn't  
> seem to fit here:
>
>       [[cogen]] (p) = p-gen, a generating extension of program p, ie  
> p-gen is a program such that for any static data
>
>       [[p-gen]] (s) = [[spec]] (p,s)
>
> Just how are your "Cogen functions" defined? Are they perhaps some  
> sort of final algebraic dual to initial "gen functions"? Has someone  
> else used them in the literature?

We will rephrase our abstract to try to clarify it, but we actually did
mean the sense of "cogen" in the PE literature: instead of representing
an object expression as a data structure and feeding it to a compiler or
PE, we represent an object expression _as its own generating extension_,
which maps static input to residual code.  We suspect that the ease of
typing that we encounter is related to the ease of writing cogen instead
of mix that you mention in "Mix Ten Years Later", for instance.

> 2. A main point is that your approach eliminates any data type of  
> object terms, so no dispatch on syntax is necessary. This is achieved  
> by mapping (representing) an object program as an ML term built from
>
> - ML variables
>
> - ML "fun" and application
>
> - functions app, lam, bool, fix etc that are given types by a module  
> Symantics and meanings by a module INT.

We use ML variables and "fun" only if we represent object programs using
higher-order abstract syntax.  We could just as well use de Bruijn
indices.  We do need to use ML application and the module system as you
describe.

> 5. A propos, a note: your approach seems at first sight to depend  
> strongly on a couple of principles:
>
> - that one interpreted program's variable is represented by exactly  
> one ML variable, and
>
> - that application in the interpreted program is implemented by ML  
> application
>
> 6. I'm curious how the approach would need modification if these  
> assumptions are broken, for example
> say an extension to lambda calculus over the rationals, where each  
> object program value v is represented by a pair of integer values (a  
> numerator and a denominator).

We're not sure what "extension to lambda calculus over the rationals"
you have in mind (other than adding the rationals as a primitive data
type, which is easy), but we certainly don't want to be tied down to
any concrete object representation such as "a pair of integer values".
It is of course possible for a particular implementation of Symantics
to represent object terms by integers; please see the "size measurer"
module L at the end of our Section 2.2.

The two principles you name are not necessary.  First, an object
variable is represented by a metalanguage variable only if we use the
higher-order abstract syntax, but we may instead represent object
language variables with de Bruijn indices (built using `constructors'
varZ and varS).  Second, we encode an object-language application using
the `constructor' app, which may have nothing to do with ML application.
For example, the L interpreter defines "app e1 e2 = e1 + e2 + 1".  Of
course, we do use metalanguage application to encode object terms, even
those with no application; for example, we encode the object term "1" by
applying "int" to "1" in the metalanguage.

> 3. Your approach resembles the startpoint of denotational abstract  
> interpretation, see the 1994 overview article by me and Flemming  
> Nielson:
>
>     http://www.diku.dk/forskning/topps/bibliography/1994.html#D-58
>
> Section 2.7 (just a few pages, extract attached) seems like basically  
> the same idea; it derives from two papers I wrote with Alan Mycroft  
> in 1985-1986.

Thank you for sending the excerpt! It is indeed relevant to our work
and helpful. The particularly relevant part is this quotation from
p3 of the excerpt:

> The idea is to decompose a denotational language definition
> into two parts:
>     - a core semantics, containing semantic rules and their types, but using
>       some uninterpreted domain names and function symbols, and
>     - an interpretation, filling out the missing definitions of domain names
>       and function symbols

Yes, we too partition the meaning of the program into two parts. An
interpretation may vary but the core stays the same. What we make
explicit is that our `core' is syntax! Indeed, the syntax of the
language determines how semantics composes, because, by the principle
of compositionality, semantics is `syntax directed'. Syntax has the
`universality property'. To express that insight more concretely, the
file jones-ai1.ml implements the example of factorized semantics (p4
of the excerpt) in our typed approach. It is astounding how literally
the OCaml code reproduces the example in the except. The code likewise
provides two interpretations in a uniform framework: the standard
interpretation (more concrete) and the even-odd interpretation (more
abstract).

> 4. Analogous with your approach, this can be used to eliminate the  
> syntax of the program to be "interpreted" by mapping it into a term  
> later to be evaluated. However our approach does not remove the need  
> for some sort of representation of the data domains involved. Eg, for  
> a lambda-calculus analysis, something corresponding to the universal  
> type would still be needed.

We avoid the universal type in denotations in two steps: first, we use
a typed object language; second, we let an interpretation provide an
infinite family of denotation domains, one for each object type.  In
the terminology of modules, the interpretation is `translucent' in the
sense that the family is opaque but the object type is manifest.  In
the terminology of types, an interpretation provides an abstract type
_constructor_.  That's the main trick.

