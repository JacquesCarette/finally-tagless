\documentclass[11pt]{llncs}

% Reduce display spacing
\divide\abovedisplayskip 2
\divide\belowdisplayskip 2
\divide\abovedisplayshortskip 2
\divide\belowdisplayshortskip 2

\usepackage{hyphenat}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{url}

\usepackage{ifpdf}
\ifpdf
    \pdfpageheight=11in
    \pdfpagewidth=8.5in
\fi

%\usepackage{refrange}

\usepackage{fancyvrb}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=\mathindent}
\setlength{\parskip}{0pt}
\newlength{\mathindent}
\setlength{\mathindent}{1em}

% Reduce list spacing
%% \makeatletter
%% \renewcommand\@@listI{\leftmargin\leftmargini
%% \parsep \z@@
%% \topsep 3\p@@ \@@plus\p@@ \@@minus 2\p@@
%% \itemsep 2\p@@ \@@plus\p@@ \@@minus\p@@}
%% \let\@@listi\@@listI
%% \@@listi
%% \makeatother

\newcommand{\omitnow}[1]{}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}

\begin{document}
\title{Functors, CPS and monads, or how to generate efficient
code from abstract designs}
\author{Jacques Carette\inst{1}
 \and
Oleg Kyselyov\inst{2}
}
\institute{McMaster University,
1280 Main St. West, Hamilton, Ontario Canada L8S 4K1
\and
Monterey, CA 93943}


\maketitle

\begin{abstract}
We use monads and Ocaml's advanced module system to
demonstrate that, in a generative context, it is
possible to totally eliminate the overhead of abstraction.
This lets one use extreme forms of \textit{information hiding}
at no run-time cost. Furthermore the typed nature of
the generative context provide static guarantees about the
generated code. The various extensions
(aspects) can be made completely orthogonal and compositional,
even in the presence of name-generation for temporaries and
other bindings and ``interleaving'' of aspects. We also
show how to encode some domain-specific knowledge so that
``clearly wrong'' compositions can be statically rejected by the
compiler.
\end{abstract}

\section{Introduction}

In high-performance and numeric computing, there is a well-known
issue of balancing between maximal performance and the level of
abstraction at which code is written.  Furthermore, already in
linear algebra, there is a wealth of different concerns that 
\emph{may} need to be addressed.  While it is possible to manually
inline and simplify such code, this is labour-intensive and 
error-prone.  Furthermore the resulting code is frequently unreadable,
and thus unmaintainable.

There are a number of projects that have attempted to address these issues
\cite{Czarnecki,Veldhuizen:1998:ISCOPE,Veldhuizen:1998:OO98,musser94algorithmoriented,BOOST,POOMA,ATLAS}, 
and they have generally succeeded in generating
efficient code, but the cost was frequently the loss of modularity
of the design.  In other words, to allow the various aspects to
weave with each other \emph{efficiently}, information which should
have been kept hidden leaked from module to module.  This seriously
affects maintainability and scalability.  Furthermore, current architectures
demand more and more frequent tweaks which, in general, cannot be done by the
compiler because the tweaking often involves domain knowledge.  

Our goal was to implement a completely modular, textbook-style 
modular design \cite{journals/cacm/parnas72a}, yet 
have our code be very efficient.  Gaussian elimination, as it is 
well-known, already contains many different aspects \cite{carette04},
was chosen as a representative example.

Whether one prefers to code in an imperative, object-oriented, functional, 
or even in a mixed style, all abstraction mechanisms in current
languages have a non-zero run-time cost.  Whether it is procedure,
method or function call, these have a cost; worse still, higher level
and/or higher order abstractions (like Functors or AspectJ's aspects)
frequently have a larger cost still.  Of course, if one is faced with
writing 60 different implementations of the ``same'' algorithm, this
abstraction cost starts to look cheap!  Although bad programmers have
been known to resort to cut\&paste programming, no self-respecting
programmer would be caught doing that\footnote{Sadly, there are still
many programmers who do not see the harm of cut\&paste programming}.
The common solution is to turn to code generation.

While many people still make do with string-based code generation,
this is not much different than writing code in a dynamically typed
interpreted scripting language.  It works, it is convenient, but it
does not scale well, it is a debugging nightmare, and usually suffers
most from Software Aging \cite{parnas_aging}.  As with other tasks in
programming, the answer is to turn to statically typed languages,
which provide strong guarantees about the soundness of the resulting
code.  MetaOCaml (cite website?) \cite{CTHL03} in particular stands
out: not only it guarantees the absense of problems such as
accidental variable capture -- so-called hygiene cite? \oleg{should we
  also mention automatic well-formedness}, MetaOcaml also
statically ensures that both the code generator and the generated code
are well-typed \cite{TahaSheard97,TahaThesis}.  While Template
programming in C++ and Template Haskell do offer some amount of static
typing \cite{conf/dagstuhl/CzarneckiOST03}, MetaOCaml\footnote{and
  MetaML.}  offers stronger guarantees.  As well, since MetaOCaml is
implemented on top of the very solid Ocaml language, one accrues
additional benefits.

While it is theoretically feasible in a strings-based
generative system to re-parse the generative code to inspect it, such
an approach rapidly degenerates into complete chaos.  In languages
that natively offer reflection and reification (either at no cost, 
such as in LISP and Scheme, or at a modest cost in languages like 
Maple), this is actually feasible, even easy.  But this is fragile
and error-prone, since the languages offer little support for catching
errors early.

In MetaOCaml, generated code cannot be examined and optimized at all. It must
be generated just right.  

- The FFT paper
showed one way: abstract interpretation. But more problems remain
(cite Padua et al paper at Haskell Workshop04): generating names in
the presence of `if', 'while' etc. control operators (simple problem
of generating names solved in the FFT paper); making CPS code clear
(note that many authors are afraid of CPS code because it quickly
becomes unreadable; we need CPS for name generation); compositionality
of various (often interleaving: explain: determinant computation
requires attention at several places) aspects and expressing
dependencies among them; expressing more static guarantees about the
generated code (e.g., not attempting full (fractional) division for
the integer domain; generating full-pivoting code for the rational
domain without |better_than| operation). In short: can we achieve all
of the abstractions needed for the flexible Gaussian Elimination? Can
we eliminate all of that abstraction's overhead in the generated code
(without using intensional code analysis -- that is, maintaining the
guarantees of MetaOCaml and using pure generative approach).

[Note why it is important to maintain generativity: stronger
equational theory: Walid's paper. Also, intensional code analysis
(e.g., SPIRAL project) essentially requires one to insert an
optimizing compiler into the code generating system. That is extremely
complex, quite error-prone, and very difficult to ascertain the
correctness.]

\oleg{explictly enumerate our contributions, not foiregtting the mdo
  notation. The subsubsection that outlines what the other sections
  are for.}

\section{CPS and the problem of generating names}
\omitnow{
In this paper, we are concerned with building and composing
generators. We start with the code expressing the algorithm (in
OCaml), and, adding MetaOCaml annotations, turn that code into a
\emph{code expression} that will generate the code when runs. We then
modify that code expression to add various parameters and aspects, to
yield the complete generator.

Here we illustrate the MetaOCaml constrcutions and code generators.

Turning code into the code expression is relatively straightforward
until we come to binding constructs such as |let|. For example, here
is a piece of code that defines a function that searches a given array
|arr| for the maximal (in absolute value) element, and the index of
that element and the found absolute value. This code is representative
of pivot determination. The code uses mutations and destructive
updates -- partly for illustrative purposes and mainly because many
linear algebra algorithms are expressed via an imperative code (so it
is less error-prone to translate the ``canonical'' algorithms with
minimal changes).

\begin{code}
let fp = fun arr n ->
   let pv = ref (0,arr.(0)) in
   for i = 1 to n-1 do
     if abs_float arr.(i) > snd !pv then
        pv := (i, abs_float arr.(i))
   done;
   !pv
\end{code}

This function has the type |float array -> int -> int * float|. By
enclosing the code into MetaOCaml brackets, we obtain the simplest
generator expression.

\oleg{later on, we can write the following just let fpc = .<...>.}

\begin{code}
let fpc = .<fun arr n ->
   let pv = ref (0,arr.(0)) in
   for i = 1 to n-1 do
     if abs_float arr.(i) > snd !pv then
        pv := (i, abs_float arr.(i))
   done;
   !pv>.
\end{code}

This value has the type |('a, float array -> int -> int * float) code|.
Here |'a| type parameter is needed (ask Walid for the reference).
The main difference between |fp| and |fpc| is that |fp| is a function,
which we can immediately use as \verb^fp [|1.0; -4.; 3.|] 3^. In
contrast, |fpc| is not a function: it is a \emph{code} value that denotes
that function. We would say that the code expression is the
``second-stage'' value. The ability to manipulate code expressions as
regular values is the distinguished fature of MetaOCaml.
We can ``compile'' the code by using MetaOCaml |run| command, or |.!|:
|.! fpc|. The result is equivalent is |fp|. Although |fpc| doesn't
seem to do much, it is already useful as we can use offshoring (cite)
to translate |fpc| into C or Fortran code (which we can dynamically
link into the running MetaOCaml code -- or use separately).

The code |fpc| obviously lacks generality: the name |abs_float|
implies that that code deals with |float| arrays. We wish to handle
arrays in various domains, integer, rationals, etc. We can make the
code generic by parametrizing over the |abs| function:

\begin{code}
let fpca absf = .<fun arr n ->
   let pv = ref (0,arr.(0)) in
   for i = 1 to n-1 do
     if absf arr.(i) > snd !pv then
        pv := (i, absf arr.(i))
   done;
   !pv>.
\end{code}

and so |fpca abs_float| will manipulate |float array| and |fpca abs|
will do for the integer arrays. Here we used the value from the first
stage, |absf|, to incorporate into the code at the second stage:
croos-stage persistent values.

We should now consider the function |fpca| in a browder context: of a
function that computes the pivot and then swaps the pivot withe 0-th
element of the array:

\begin{code}
let upv = .<fun arr n ->
   let pv = .~(fpca abs_float) arr n in
   let i = fst pv in
   (if not (i = 0) then
      let t = arr.(0) in
      begin arr.(0) <- arr.(i); arr.(i) <- t end);
   arr>.
\end{code}

Here, we see the third MetaOCaml operator, escape |.~|. It does ...
MetaOcaml can print out the code, which looks like
|.<fun arr n -> let pv = (fun arr n -> fpcs code) arr n in ... >.|
That is, the compiler inlined the code for fpca -- which is a
functional expression. The |upv| code shows the functional
application, |(fun arr n -> ...)| applied to arguments |arr n|. That
application will be executed at run-time, unless a sufficiently smart
compiler can inline it. As we stated, our aim is to not to rely on
post-generation optimizations and sufficiently smart compiler. We
strive to generate the efficient code ourselves. We can do that by
modifying the definition of |fpca| as

\begin{code}
let fpca absf arr n = .<
   let pv = ref (0,(.~arr).(0)) in
   for i = 1 to .~n-1 do
     if absf (.~arr).(i) > snd !pv then
        pv := (i, absf (.~arr).(i))
   done;
   !pv>.
\end{code}
\begin{code}
let upv = .<fun arr n ->
   let pv = .~(fpca abs_float .<arr>. .<n>.) in
   let i = fst pv in
   (if not (i = 0) then
      let t = arr.(0) in
      begin arr.(0) <- arr.(i); arr.(i) <- t end);
   arr>.
\end{code}
% (.! upv) [|1.0; -4.; 2.|] 3;;
We see that we pass code fragment |.<arr>.| as arguments to the
function. Now when we look at the code for |upv|, we see
|.<fun arr n -> let pv1 = let pv = ref (0,arr.(0)) in ...>.|
So the fpca code got really inlined.

The generator |fpca| shows one particular way of searching for pivot.
In the domain of exact rational numbers, for example, we would be
satisfied with the first non-zero element as a pivot, and so we would
write a different pivot-searching generator. The body of |upv| moves
the found pivot to the ``canonical'' location. Again, depending on the
container (a dense matrix, a sparse matrix, etc), there are different
ways of doing that. So, we would like to be able to ``compose'' pivot
generating function with the swap generating function. We would like
to write something like this

\begin{code}
let fpca absf arr n pv = .<
   for i = 1 to .~n-1 do
     if absf (.~arr).(i) > snd ! (.~pv) then
        (.~pv) := (i, absf (.~arr).(i))
   done>.
let swapper arr n pv = .<
   let i = fst !(.~pv) in
   if not (i = 0) then
      let t = (.~arr).(0) in
      begin (.~arr).(0) <- (.~arr).(i); (.~arr).(i) <- t end>.
let seqv a b c = .<begin .~a; .~b; .~c end>.
let pv_swap absf arr n =
   let pv = .<ref (0,(.~arr).(0))>. in
   seqv (fpca absf arr n pv)
        (swapper arr n pv)
        arr
let top =  .<fun arr n -> .~(pv_swap abs_float .<arr>. .<n>.)>.
\end{code}

The function |pv_swap| shows our ideal -- composing code gerenators
|fpca| and |swapper|, using a higher-order combinator `seqv'
(sequence of code fragments). We lifted the pivot declaration out of
old version of |fpca| so that both |fpca| and |swapper| could be
parameterized by the pivot (and |fpca| and similar function will deal
only with pivot determination). |top| puts it all together. The above
code represents the pattern we will be following for the rest of the
paper.

And it \emph{almost} works. It is type correct.  When we look at the
generated code for |top|, we notice the |(ref (0, arr_1.(0)))| is
inlined wherever pivot was expected. For example, the swapping part of
the code begins with |let i_3 = (fst (! (ref (0, arr_1.(0))))) in ...|. 
That is certainly not what we want. We meant for pivot searching
and swapping parts of the code to communicate via a shared mutable
state, variable |pv|. In the generated code, each piece of code gets
its own copy of |pv|. Rather then inline |(ref (0, arr_1.(0)))|
everywhere, we should have generated |let pv = ref (0,(.~arr).(0)) in ...|
and inlined |pv|. Incidentally, even in a pure functional code we
often wish to generate such a |let| statement so we can compute a
complex expression only once and use it value in several other
places. Again, we would rather not leave the job of detecting such
common subexpressions to the compiler (because in the complex code the
compiler may fail to detect the common subexpressions). The problem
with generating such |let| expressions is that we wish our geenrator
to return just |.<let pv = ref (0,(.~arr).(0)) in>.|, without the `body'
of the |let|, which we would fill in later. In languages which
generate code in the string form this is easy. We generate the string
|''let pv = ref (0,(.~arr).(0)) in''| and then append to it the string
representting the body, wcich would give us the complete |let|
statement. In MetaOCaml, we can't generate incomplete expressions, for
a good reason. 

To solve the problem of writing a generator for a |let| expression 
when the generated body is not yet known, we use CPS.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% And now, a better -- and shorter -- write-up.

The essence of our approach is assembling the code of the program by
combining instances of various aspects together. In the case of
Gaussian Elimination, the aspects are pivot determination, swapping of
rows and columns, determinant tracking, etc. A primitive code
generator yields a code expression.  A combinator takes (complete and
well-typed) code expressions and combines them into a composite expression
(which is too statically guaranteed to be well-formed and well-typed).

We will be using MetaOcaml, which, as the instance of a multi-stage
programming system \cite{TahaThesis}, provides exactly the needed
features: to construct a code expression, to combine them, and to
execute them.

For example, the following is probably the simplest code generator:
\begin{code}
let one = .<1>.
\end{code}

(in the floating-point instance of that aspect, we would have
|.<1.0>.| instead). We use MetaOCaml brackets |.<...>.| to generate
code expressions -- in other words, to construct future-stage
computations. The simplest combination is inlining:
\begin{code}
let plus x y = .<.~x + .~y>.
\end{code}
Here we see the second component of MetaOCaml -- escapes |.~|. They
let us perform an immediate code generating computation while we are
building the future-stage computation (code expression). In the
example above, the immediate computation |.~x| is trivial -- obtaining
the value of the variable |x|, which should be a code expression, and
inlining it. The following shows an example of using the simple
generators:

\begin{code}
let simplest_code =
  let gen x y = plus x (plus y one) in
  .<fun x y -> .~(gen .<x>. .<y>.)>.
\end{code}

Here, the immediate computation |gen .<x>. .<y>.| is less trivial --
it includes the evaluation of the function |gen|, which in turn
applies the function |plus|. The function receives code expressions
|.<x>.| and |.<y>.| as arguments. At the generating stage, we can
manipulate code expressions as values. The function |gen| returns a
code expression, which will be inlined in the place of the
escape. MetaOCaml can print out code expressions, so we can see the
final generated code:\\
  |.<fun x_1 -> fun y_2 -> (x_1 + (y_2 + 1))>.|

We must point out that |gen x y| looks like a combinator; its body 
involves several applications of the function |plus|. When
we look at the generated code however, we see no traces of function
|plus| and no such administrative applications. Instead, we see the
fully inlined expression. These administative applications are done at
the code generation stage. 

The final MetaOCaml feature, |.!| (pronounced ``run'') can be used to
execute the code expression: |.! simplest_code| is a function of two
integers, which we can apply: |(.! simplest_code) 1 2|. The original
|simplest_code| is not the function on integers -- it is a code
expression.

To see the benefit of code generation, we notice that we can easily
parameterize our code:

\begin{code}
let simplest_param_code plus one =
  let gen x y = plus x (plus y one) in
  .<fun x y -> .~(gen .<x>. .<y>.)>.
\end{code}

and use it to generate code that operates on integers, floating point
numbers or booleans -- in general, any domain that implements |plus|
and |one|:
\begin{code}
let plus x y = .<.~x +. .~y>. and one = .<1.0>. in
  simplest_param_code plus one
let plus x y = .<.~x || .~y>. and one = .<true>. in
  simplest_param_code plus one
\end{code}
Running the former expression yields the function on |float|s, whereas
the latter expression is the code expression for a bollean function.

We can see the separation of concerns, namely of that for domain
operations.

Let us consider a more complex expression:
\begin{code}
let param_code1 plus one =
  let gen x y = plus (plus y one) (plus x (plus y one)) in
  .<fun x y -> .~(gen .<x>. .<y>.)>.
\end{code}

We notice two occurrences of |plus y one|. Depending on the
implementation of |plus| that may be quite a complex computation, and
so we would rather not do it twice. We may be tempted to rely on the
compiler's common-subexpression elimination optimization. We would
rather not relied on a ``sufficiently smart compiler'': when the
generated code is very complex, the compiler may overlook common
subexpressions. The subexpressions may occur in such an imperative
context that the compiler might not be able to determine if lifting
them is sound. So, being conservative, the optimizer will leave the
duplicates as they are. We may attempt to write

\begin{code}
let param_code1 plus one =
  let gen x y = 
     let ce = (plus y one) in  plus ce (plus x ce) in
  .<fun x y -> .~(gen .<x>. .<y>.)>.
\end{code}

However, the result of |param_code1 plus one|, which prints as\\
|.<fun x_1 -> fun y_2 -> ((y_2 + 1) + (x_1 + (y_2 + 1)))>.|,
still exhibits duplicate sub-expressions. Our |let|-insertion
optimization saved the computation on the generating stage. 
We need a combinator that inserts the |let| expression in the
generat\emph{ed} code. We need a combinator |letgen| to be used
as\\|let ce = letgen (plus y one) in plus ce (plus x ce)| 
yielding the code like |.<let t = y + 1 in t + (x + t)>.|
But that seems impossible because |letgen exp| has to generate
the expression\\|.<let t = exp in body>.| but |letgen| does not have
the |body| yet. The body needs that temporary identifier |.<t>.| that
is supposed to be the result of |letgen| itself. 
Certainly |letgen| cannot generate only a part of a let-expression,
without the |body|.  All generated expressions in
MetaOCaml are well-formed and complete.

The key is to use continuation-passing style (CPS). It was first
pointed out by \cite{Bondorf:92} in the context
of partial evaluation, and extensively used by \cite{KiselyovTaha} for code
generation.

\begin{code}
let letgen exp k = .<let t = .~exp in .~(k .<t>.)>.
let param_code2 plus one =
  let gen x y k = letgen (plus y one) (fun ce -> k (plus ce (plus x ce)))
  and k0 x = x
  in .<fun x y -> .~(gen .<x>. .<y>. k0)>.
\end{code}

Now, |param_code2 plus one| gives us the desired code\\
|.<fun x_1 -> fun y_2 -> let t_3 = (y_2 + 1) in (t_3 + (x_1 + t_3))>.|.


\section{Monadic notation, making CPS code clear}

Comparing the code that did let-insertion at the generating stage\\
|let ce = (plus y one) in  plus ce (plus x ce)|\\
with the corresponding code inserting let at the generated code stage\\
|letgen (plus y one) (fun ce -> k (plus ce (plus x ce)))|\\
clearly shows the difference between the direct-style and CPS code.
What was |let ce = init in ...| in direct style became
|init' (fun ce -> ...)| in CPS. For one thing, |let| became
inverted. For another, what used to be an expression that yields
value, |init|, now became an expression that takes an extra argument,
the continuation, and invokes it. The differences look negligible in
the above example. In larger expressions with many let-forms, the
number of paranetheses around |fun| increases, the need to add and
then invoke the |k|, continuation argument, become annoying. The
inconvenience is great enough for some people to explicitly avoid CPS
or claim that numerical programmers (our users) cannot or will not
program in CPS. Clearly a better notation is needed.


The |do|-notation of Haskell (cite report?) shows that it is possible
to write essentially CPS code in the conventional-looking style. The
|do| notation is the notation for writing monadic code (cite Moggi?).
The benefit of monadic notation is not that it can represent CPS (cite
Filinski), but it helps in composability by offering to add different
layers of effects (state, exception, non-determinism, etc) to the
basic monad (cite Hudak, Peyton-Jones).

A monad (cite Moggi?) is an abstract datatype representing
computations that yield a value and may have an \emph{effect}.
The datatype must have at least two operations, |return| to build
trivial effect-less computations and |bind| for combining
computations. These operations must satisfy \emph{monadic laws}:
|return| being the left and the right unit of |bind| and |bind| being
associative. Figure XXX defines the monad in the present paper
and shows its implementation.

\begin{figure*}
\begin{code}
type ('v,'s,'w) monad = 's -> ('s -> 'v -> 'w) -> 'w
let ret a = fun s k -> k s a
let bind a f = fun s k -> a s (fun s' b -> f b s' k)
let fetch s k = k s s
let store v s k = k (v::s) ()

let k0 s v = v
let runM m = m [] k0

let retN a = fun s k -> .<let t = .~a in .~(k s .<t>.)>.
\end{code}
\caption{Our monad}
\end{figure*}

Our monad represents two kinds of computational effects: reading and
writing a computation-wide state, and control effects. The latter are
normally associated with exceptions, forking of computations, etc. --
in general, whenever a computation ends with something other than
invoking its natural continuation in the tail position. In our case
however the control effects manifest themselves as code generation.

In Figure XXX, the monad is implemented as a function of two
arguments: the state (of type |s|) and the continuation. The
continuation receives the current state, the value (of the type |v|) and
yields the answer of the type |w|.  The monad is polymorphic over all
these three type parameters. Other implementations are
possible. Except the code in Figure XXX, the rest of our code treats
the monad as a truly abstract data type. The implementation of basic
monadic operations |ret| and |bind| are conventional. It is easy to
see that the monadic laws are satisfied. Other monadic operations
construct computations that do have effects. Operations |fetch| and
|store v| construct computations that read and write the state. In our
case the state is a list (of polymorphic variants), which models an
open discriminated union, as we shall see later.

The operation |retN a| is the let-insertion operation (whose simpler
version was called |letgen| before). It is the first computation with
a control effect: indeed, the result of |retN a| is \emph{not} the
result of invoking its continuation |k|. Rather, its result is a |let|
code expression. Mention similarity with reset?

Finally, |runM| runs our monad, that is, performs the computation of
the monad and get the result, which in our case is the code
expression. We run the monad by passing it the initial state and the
initial continuation |k0|. We can now re-write our |param_code2|
example of the previous section as

\begin{code}
let param_code3 plus one =
  let gen x y = bind (retN (plus y one)) (fun ce -> 
                ret (plus ce (plus x ce)))
  in .<fun x y -> .~(runM (gen .<x>. .<y>.))>.
\end{code}
% param_code3 plus one;;

That does not seem like the improvement. But here we can introduce the
do notation (Appendix A), which is patterned after the do-notation of Haskell.

\begin{code}
let param_code4 plus one =
  let gen x y = mdo { ce <-- retN (plus y one);
                      ret (plus ce (plus x ce)) }
  in .<fun x y -> .~(runM (gen .<x>. .<y>.))>.
\end{code}

The function |param_code4| is totally equivalent to |param_code3| --
the |mdo|-notation is just the syntactic sugar. In fact, the camlp4
preprocessor (with our extension) will transform the |param_code4|
into |param_code3|. And yet, |param_code4| looks far more
conventional, as if it were indeed in direct style.



\oleg{Should we rename lif into ifL?}
Now we can build more complex monadic operations.
\begin{code}
let lif test th el = ret .< if .~test then .~th else .~el >.
\end{code}


ifM and ifL: show example. Mention 'ifM' and `reset'. Investigating
the connection with the delimited continuations: for future work.

What is state and why we need it: making aspects interleave: use det
as an example.

\section{Functors}

Simple parameterization: by the domain, by the container
 
Functors and the generated return type: the Out structure.

Making state modular and composable: several aspects can contribute
to the state, without interference (show: Rank, Det, Permutation
aspects).

Expressing domain constraints in functors (Div and integer domain).
Show the type error that occurs in improper instantiation.

Show several examples (e.g., code with NullPivot and nodet: no traces
of either det nor pivoting). Then the code with RowPivoting, Det and
float domain.

\section{Related work}

Note that the monad is similar to the one used in \cite{KiselyovTaha}.
However, the latter work used only |retN| of all monadic operations,
and used fixpoints (for performing iterations at the generation time).
In this paper we do not use monadic fixpoints (because the generator
is not recursive) but we make extensive use of monadic operations for
generating conditional and looping operations.

\section{Future work}
connections with delimited continuations: making notation
more direct-style and potentially clearer.

More aspects that can be handled: Input variations (augmented
matrices). Fit into the larger program family, where there are
more aspects still.

\section{Conclusion}

Mention the mdo notation. Mention it into the introduction.

\bibliography{metamonads}
\bibliographystyle{plain}

\section{Appendix A}
The do notation for Ocaml.
We use camlp4. Show the grammar. Explain the difficulty: although
nice, the do notation cannot actually be expressed in a LR(n) grammar.
Perhaps give the command line how to load the camlp4 module into
ocaml. Point to the code.
\end{document}
