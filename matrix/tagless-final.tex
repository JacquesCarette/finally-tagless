\documentclass[preprint]{sigplanconf}
\usepackage{amsmath}
\usepackage{natbib1}
\usepackage{comment}

\newcommand{\jacques}[1]{{\it [Jacques says: #1]}}
\newcommand{\oleg}[1]{{\it [Oleg says: #1]}}
\newcommand{\ccshan}[1]{{\it [Ken says: #1]}}

\usepackage[compact]{fancyvrb1}
\DefineShortVerb{\|}
\DefineVerbatimEnvironment{code}{Verbatim}{xleftmargin=1em,fontsize=\small}

\begin{document}

\conferenceinfo{ICFP '07}{Freiburg, Germany} 
\copyrightyear{2007} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Staged typed type-preserving tagless interpreters, \emph{finally}}
\subtitle{Subtitle Text, if any}

\authorinfo{Jacques Carette}
           {McMaster University}
           {carette@mcmaster.ca}
\authorinfo{Oleg Kiselyov}
           {FNMOC}
           {oleg@pobox.com}
\authorinfo{Chung-chieh Shan}
           {Rutgers University}
           {ccshan@cs.rutgers.edu}

\maketitle

\begin{abstract}
We built a tagless partial evaluator for a typed object language,
placing minimal requirements on the type system of the metalanguage:
we need let-bound polymorphism and staging, but not dependent
types, generalized algebraic data types, or a tag elimination as a
postprocessing step.
The trick is the "final" approach: we encode the
higher-order abstract syntax of object terms not using constructors for
an algebraic data type but by invoking cogen functions, which occur as
free variables in the encoded term.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}
%\terms term1, term2
%\keywords keyword1, keyword2

\section{Introduction}

See Section 1 of Walid02. Our subject area is that of a definitional DSL
interpreter.
Define Staging, meta-language and object language. (Walid uses
`subject' language).

We have programmed out interpreters in OCaml/MetaOCaml (cite
www.metaocaml.org) and Haskell. The complete code is available 
% connot give URL in the ICFP paper due to double-blind reviews
in the supplemental material to the paper. We will use both
(Meta)OCaml and Haskell for our examples throughout the paper.

\oleg{should say somewhere in the Introduction that we are interested
  in the typed subject language embedded in the typed
  meta-language. Refer to Sec 2 (see the remark below) why this is
  good. We assume the DSL setting: we enter EDSL terms in the
  meta-language program (e.g., GHCi or (Meta)OCaml top-level). A
  different approach is reading EDSL terms from a file. In that case, we
  need to typecheck them first. We do not deal with that problem here,
  because it has been solved. First we mention Walid02 where they use
  dependent typed. However, these don't seem to be needed: refer to
  `typing dependent typing', and the related work section.}

\jacques{I quite agree that the point that needs to be revisited is
  that a \emph{direct} interpreter for a (typed) language "cannot" be
  done. see discussion in related work about the direct interpreter,
  with regards to Walid's paper. Perhaps bring some of it here?}
\jacques{I have looked at a fair bit of the literature, and I have not
  seen this done, especially not in the typed 'online' setting.  The
  one question that seems to be open is, how accurate can we make the
  PE without a tough BTA?}
Should we mention some of these points in our contributions subsection
below?

        Taha et al and Xi et al (both POPL2003) argued that writing an
interpreter of a typed language in a typed language is an interesting
problem. The known solutions include the use of the Universal type, or
GADTs. The former is not quite satisfactory for performance reasons;
and also because of the presence of partial pattern matching in the
interpreter; the inexhaustiveness of the pattern matching should never
be triggered when executing a well-typed term. Alas, this fact cannot
be made obvious to the compiler. That also exlains unoptimality.


\subsection{The Tag Problem}

\oleg{see tagless\_interp1.ml, module Tagfull for the complete code.
  If you change the code in here, please adjust the .ml file
  accordingly. Let the paper and the accompanying code be in sync.}


Consider the sample language of abstraction, application and boolean
literals with deBruin indices as variables. This is the language used
in the body of Walid02, from which we borrow the first part of 
this example.

The language can be described by the following grammar (using OCaml
syntax)


\begin{code}
  type var = VZ | VS of var
  type exp = V of var | B of bool | A of exp * exp | L of exp
\end{code}

and a sample term is
\begin{code}
  let test1 = A ((L (V VZ)),(B true))
\end{code}

The first attempt at the interpreter, the function eval. It takes the
object language term such as test1 and gives us its value.
The first argument to |eval| is the environment, initially empty,
which is the sequence of values associated with free variables in the
interpreted code.
\begin{code}
  let rec eval0 env = function 
  | V VZ -> List.hd env
  | V (VS v) -> eval0 (List.tl env) (V v)
  | B b -> b 
  | L e -> fun x -> eval0 (x::env) e
  | A (e1,e2) -> (eval0 env e1) (eval0 env e2) 
\end{code}

Suppose our our meta-language (the language in which we wrote test1
and that interpreter) were untyped. The above code would be acceptable.
The |L e| line exhibits interpretative overhead (see descr in Walid02
paper): |eval0 env' e| will be executed every time the result of
evaluating |L e| is applied. Staging can be used to remove the
interpretative overhead. Cite some paper -- see Walid02, Sec 1.1 and
1.2. They say it well.

If we use OCaml or other typed language as the meta-language, 
the function |eval0| is ill-typed. The line |B b|
tells that the function returns a boolean, whereas the next line says
the result of |eval0| is a function. Clearly, the result of the
function can't be both of these types. 
A related problem is the type of the env: since variables
in the language may be either of boolean or function types, a regular
OCaml list cannot be used for holding the corresponding values. 

The solution is to introduce the Universal type (see Walid02 Section
1.3)

\begin{code}
  type u = UB of bool | UA of (u -> u)
\end{code}

We can then write the interpreter, which is accepted by the typecheker
\begin{code}
  let rec eval env = function
  | V VZ -> List.hd env
  | V (VS v) -> eval (List.tl env) (V v)
  | B b -> UB b
  | L e -> UA (fun x -> eval (x::env) e)
  | A (e1,e2) -> match eval env e1 with UA f -> f (eval env e2)
\end{code}
and whose inferred type is |u list -> exp -> u|. Now we can evaluate
our sample |test1| term:

\begin{code}
  let test1r = eval [] test1
  val test1r : u = UB true 
\end{code}

Unfortunately, the result is tagged: |UB true| rather than just
|true|. We also note that the eval function is partial in two
respects: first there is an inexhaustive pattern-matching in the 
line |A (e1,e2)|.
Second, the occurrences of |List.hd| and |List.tl|.
The failure of the latter corresponds to the evaluation of `open'
terms. One can see that if all our terms are closed, |List.hd| and
|List.tl| above will never be applied to the empty list. The partial
pattern match in the |A (e1,e2)| line raises an exception when we try to
evaluate something like |A (B true, B false)|, that is, the first
operand is not a function. 

Suppose our object language were typed, too. Then terms like
|A (B true, B false)| would be ill-typed. Thus, if our evaluator
accepted only typed terms, then the missing alternatives in the |A|
line would not occur. Alas, what is obvious to us is not obvious to
the meta-language. There is still the pattern match, and we still have
to attach and eliminate the UB and UA tags, even when there is no
longer a need for them. See Walid02, Sec 1.4 for more detail.

Of course there is also the issue how to design a type system to type
terms like test1 (GADT seem necessary). Also, how to design the
typesystem to keep track of open/closed status of terms, so we can
statically assure that only closed terms are passed to eval and that
the empty list checking in |List.hd| and |List.tl| can be eliminated
as well. The latter is even more interesting problem requiring even
more advanced type systems (cite env classifiers, Nanevski, Rowan
Davies).

The problem is also outlined in Simon Peyton-Jones GADT paper, but
only in the context of a first-order language

In short: (put into really introduction or the conclusions?)
It is not satisfactory
to use the Universal type because tagging hurts performance and
necessitates partial pattern matching in the metalanguage, even for a
well-typed object term.  


\subsection{Other solutions}
The tag problem motivated GADTs (SPJ, Xi) and dependent types
(WalidICFP02, Concoqtion). Or, perhaps just refer to related work?
But we need to say something here, to give the reader the feeling of
how advanced solutions were proposed and how rarely they are implemented.


\subsection{The `Final' Proposal}
We have realized that, if we change the encoding of the term from data
constructors to functions, then we can encode a tagless interpreter of a
typed object language in a metalanguage with a very simple type system.

\oleg{see tagless\_interp1.ml, module Tagless for the complete code.
If you change the code in here, please adjust the .ml file
accordingly. Let the paper and the accompanying code be in sync.}

Now, let us define our object language (embedded DSL) in a different
way. Instead of using algebraic datatypes to encode the terms of the
language in the meta-language (reminiscent to the initial approach),
we will be using functions as term constructors (the final approach):

\begin{code}
  let b (bv:bool) env = bv
  let varZ (vc,_) = vc
  let varS vp (_,envr) = vp envr
  let app e1 e2 env = (e1 env) (e2 env)
  let lam e env = fun x -> e (x,env)
\end{code}

That is all of the interpreter. Our sample term now reads
\begin{code}
  let testf1 = app (lam varZ) (b true)
\end{code}
which is almost the same as before, only written with lower-case
identifiers. To evaluate the term, we merely need to apply 
the term to the empty environment

\begin{code}
  let testf1r = testf1 ()
  val testf1r : bool = true
\end{code}

The result now has no tags. The above functions |b|,
|varZ|, |app| constitute the evaluator. It is patent that the
interpreter has no tags and no pattern matching. The term |b true| is
evaluated to a boolean and a term |lam varZ| is evaluated to a
function, with no tags attached. The application rule applies that
function without any need for pattern-matching. The evaluator has
another feature: evaluation of open terms leads to a type error rather
than a run-time error. Indeed, the following gives a type error

\begin{code}
  let testf1bad = app (lam (varS varZ)) (b true)
  let testf1badr = testf1bad ()
\end{code}
that the initial environment should be a tuple rather than () --
meaning the term requires non-empty initial environment: the term is
open.

Thus all the errors that were previously reported at run-time (open terms,
applications of non-functions) are now reported at compile
time. Running of the interpreter assuredly gives no run-time errors
because its code uses no operations that could possibly raise a
run-time error.



The interpreter uses no Universal type or pattern matching, and it
is obvious to the compiler that non-termination can only result from
interpreting |fix|.  If the source language is strongly normalizing, then
our interpreter is total.

We should emphasize that not only our code can't have pattern-match
errors, that fact is apparent to the compiler itself. It is one thing
to write a code with pattern-matching and claim (and prove) that
missing cases cannot occur. However true that may be, the fact the
missing cases cannot occur is not evident to the compiler, which still
has to generate code to report missing cases or compile in the error
generation. That's where efficiency is lost, and Walid's papers on tag
elimination have numbers showing that performance is lost indeed. 
In our code, there is simply no pattern-mismatch opportunity: the
proof of no bad match is expressed in the code itself. 


In the above code, the interpreter is wired in, built into the
functions |b|, |app|, etc. We explain how to abstract the interpreter
below, to make it possible to `evaluate' the same term in many different
ways (e.g., interpret term, compile it, compute its size or depth,
etc.)

\subsection{Contributions}

        We have realized that if we change the encoding of the term --
from data constructors to functions, we can encode the tagless
interpreter of the type language in a typed language with no GADTs and
a very simple type system. The interpreter uses no universal type, no
pattern matching and is patently obviopus to the compiler that the
only non-termination can come from the interpreting fix. If the source
language is strongly normalizing, our interpreter is total.


The term `constructor' functions lam, app, etc. look like free
variables in the encoding of an object term.  Defining these functions
differently gives rise to different interpreters.  Given the same term
representation but varying the interpreters, we can
	- evaluate the term to a value in the metalanguage
	- determine the size or depth of a term
        - compute a \emph{CBN} CPS transformation of a term
	- `compile' the term. This requires staging, as in MetaOCaml.
	- Surprisingly, it is also possible to write a partial
	evaluator! 

We have implemented the above in (Meta)OCaml and Haskell.

What are our contributions:
0) we interpret/compile/pe the typed source language
   [Ken says that Danvy's source language is untyped]
1) a much clearer implementation
2) a nicely comparable Haskell/MetaOCaml version
3) a functorial Symantics that deals with interp/comp/pe uniformly
4) clear and easy extensibility to more features
- only closed terms can be evaluated
Making sure that we interpret terms that are well-typed and closed --
and we can do that in far simpler type system (e.g., Haskell or
OCaml). It was thought that doing either requires quite advanced
extensions (GADT, Nanevski, Concoqtion).

The structure of our paper is as follows. Section XXX ...
We finally conclude.


\section{Our base language: Tagless interpreter}

Introduce the language, The language is simply-typed lambda-calculus
with fixpoint, integers, booleans and comparison, whose BNF grammar is

\begin{code}
  e ::= Lam hoas_fn | App e e | Fix hoas_fn |
  I int_literal | B bool_literal | Add e1 e2 | Mul e1 e2 | Leq e1 e2 |
  IF b e-then e-else
\end{code}

This time we use the higher-order abstract syntax (cite: but not
Elliott. Cite Dale Miller!) rather than deBruijn indices.  The
language is close to the language of Xi03, without the polymorphic
lift but with more constants so we can write better examples, enough to
express Fibonacci, factorial, and power.

How to make encoding flexible: abstract the interpreter
We have implemented the above in (Meta)OCaml and Haskell.  In Haskell,
the term `constructor' functions are defined as methods in a type class
Symantics (the name means that the class interface gives the syntax for
the source language and its instances give the semantics).



\begin{code}
class Symantics repr where
    int :: Int -> repr Int                -- int literal
    bool :: Bool -> repr Bool             -- bool literal

    lam :: (repr a -> repr b) -> repr (a->b)
    app :: repr (a->b) -> repr a -> repr b
    fix :: (repr a -> repr a) -> repr a

    add :: repr Int -> repr Int -> repr Int
    mul :: repr Int -> repr Int -> repr Int
    if_ :: repr Bool -> repr a -> repr a -> repr a
    leq :: repr Int -> repr Int -> repr Bool
\end{code}

Our sample code test1 now reads |app (lam (\x -> x)) (bool True)|,
whose inferred type is |Symantics repr => repr Bool|.

encoding of a few terms.

\begin{code}
testpowfix () = 
  lam (\x -> fix (\self -> lam (\n ->
        if_ (leq n (int 0)) (int 1)
            (mul x (app self (add n (int (-1))))))))
testpowfix7 () = 
  lam (\x -> app (app (testpowfix ()) x) (int 7))
\end{code}
The dummy unit |()| argument above is to avoid the monomorphism
restriction and so that the type of |testpowfix| and |testpowfix7|
would remain polymorphic in |repr|. Instead of supplying the dummy
argument, we could have given the terms explicit polymorphic
sugnature. We however prefer if the typechecker of the meta-language
(Haskell) inferred the types of the object terms for us. We could also
avoid the |()| argument by switching off the monomophism restriction
with a compiler flag.

Note that the subject language (DSL) is typed. If we make a mistake
(for example, replace |int 7| with |bool True| in |testpowfix7|, the
compiler will highlight that term and complain that it couldn't match
the expected |Int| against the inferred |Bool|). The type error
also results when entering |lam (\x -> app x x)|, with the compiuler
complaining about the infinite type. The type-checker of the
meta-language not only flags meaningless DSL (subject level) terms --
before any of their evaluation. If we forget, for example, |app| in
some of the terms above, we again get an error message with a precise
location. So the type-checker also flags syntactically invalid
subject-level terms.\oleg{Should we perhaps move this up, in the
  intro? To justify why typed-metalanguages and the typed DSL are
  good. Or keep the explanation here but refer to it from the Introduction.}



OCaml: functors.

\begin{code}
module type Symantics = sig
  type ('c,'dv) repr
  val int  : int  -> ('c,int) repr
  val bool: bool -> ('c,bool) repr
  val add : ('c,int) repr-> ('c,int) repr-> ('c,int) repr
  val mul : ('c,int) repr-> ('c,int) repr-> ('c,int) repr
  val leq : ('c,int) repr-> ('c,int) repr-> ('c,bool) repr
  val if_ : ('c,bool) repr ->
             (unit -> ('c,'da) repr) ->
             (unit -> ('c,'da) repr) -> ('c,'da) repr 

  val lam : (('c,'da) repr -> ('c,'db) repr) 
          -> ('c,'da->'db) repr
  val app : ('c,'da->'db) repr
    -> ('c,'da) repr -> ('c,'db) repr
  val fix : (('c,'da->'db) repr -> ('c,'da->'db) repr) 
            -> ('c,'da->'db) repr
end;;
\end{code}

Some differences are in the additional type parameter |c| (the
environment classifier, Walid03) to be used later when we get to code
generation (required by MetaOCaml, part of the type of the
code). MetaOCaml is a call-by-value language, and thus we have to 
$\eta$-expand
the fix-point combinator and encapsulate the branches of the |if_|
into thunks to prevent their evaluation. 

The power example and our sample term |test1| are encoded as
\begin{code}
module EX(S: Symantics) = struct
 open S
 let test1 () = app (lam (fun x -> x)) (bool true)
 let testpowfix () = 
   lam (fun x ->fix (fun self -> lam (fun n ->
         if_ (leq n (int 0)) (fun () -> int 1)
             (fun () -> mul x (app self 
                                   (add n (int (-1))))))))
 let testpowfix7 = 
    lam (fun x -> app (app (testpowfix ()) x) (int 7))
end;;
\end{code}
The dummy unit argument in |testpowfix| is the artefact of MetaOCaml
(which we will be using to compile our tests in Sections below). To be
more precise, the environment qualifier (which appears as the |'c|
type parameter in the signature of Symantic functions) must remain
uninstantiated in order for us to `run' the code. That means, the type
must be polymorhic. The value restriction then dictates 
the definitions of our object terms must look syntactically like
values. Alternatively to giving the |()| argument, we could have used
rank-2 record types of OCaml to maintain the necessary polymorphism.

Thus the object expressions (expressions of the embedded DSL) have in
OCaml the type of the functor, from |Symantics| to the value. This
essentially the same as |Symantics repr => | constraint in the case of
the embedding of the DSL in Haskell.

The first instatiation of the Symantics structure is the interpreter:
evaluating the object (DSL) term to its value in the meta-language. 
The following module essentially interprets (gives one-to-one
correspondence of) each operation of the object language as the
corresponding operation in the meta-language.
\begin{code}
module R  = struct
  type ('c,'dv) repr = 'dv    (* absolutely no wrappers *)
  let int (x:int) = x
  let bool (b:bool) = b
  let add e1 e2 = e1 + e2
  let mul e1 e2 = e1 * e2
  let leq x y = x <= y
  let if_ eb et ee = if eb then (et ()) else (ee ())

  let lam f = f
  let app e1 e2 = e1 e2
  let fix f = let rec self n = f self n in self
end;;
\end{code}

This interpreter is patently tagless: the operation |add| is really
OCaml's addition, and |app| is OCaml's application. We can run our
examples by instatiating the |EX| functor with |R|.
\oleg{Jacques: you mentioned "Boxes go Bananas" by Washburn and
  Weirich. Do you want to comment more about that paper here?}

\begin{code}
  module EXR = EX(R);;
\end{code}

and so |EXR.test1 ()| evaluates to untagged boolean value |true|.
In Haskell, we define a |newtype R a = R a| and 
|instance Symantics R|
with the straightforward implementation. We must stress that |R| is
a |newtype|. Although it may look like a tag, it has no run-time
representation (erased during the compilation) and pattern-matching
against |R| cannot ever fail and so is operationally identity and
compiled that way.

Proposition: If |e| is the term in the meta-language has the type
|t|, evaluation of that term in the interpreter R will either continue
idenfinetely or termninate with a value of the same type |t|.

We'd like show a different interpreter: it interprets each terms of
the object language to its size (defined as the number of term
constructors). The following is a slightly abbreviated code (see the
accompanying source code for the complete definition)
 
\begin{code}
module L = struct
  type ('c,'dv) repr = int
  let int (x:int) = 1
  let mul e1 e2 = e1 + e2 + 1
  let if_ eb et ee = eb + et () + ee () + 1

  let lam f = (f 0) + 1
  let app e1 e2 = e1 + e2 + 1
  let fix f = (f 0) + 1
end;;
\end{code}

Now the expression
\begin{code}
  let module E = EX(L) in E.test1 ()
\end{code}
evaluates to |3|. This interpreter is not only tagless, it is also
total. It `evaluates' even seemingly divergent terms like\\
|app (fix (fun self -> self)) (int 1)|.

\begin{comment}
module EX1(S: Symantics) = struct
 open S
 let tfix () = app (fix (fun self -> self)) (int 1)
end;;
let module E =EX1(R) in E.tfix ();;
let module E =EX1(L) in E.tfix ();;
\end{comment}

\section{Tagless compiler (aka, staged interpreter)}
Staged code in OCaml... Don't show the Haskell code (too much)


\section{Tagless partial evaluator}

Surprisingly, it is also possible to write a partial evaluator!  The
partial evaluator does have tags: whether a value is static or dynamic.
The pattern-matching on this \emph{phase} tag is always exhaustive; we still
have no universal type and no tags for object types.  The partial
evaluator uses the previously mentioned compiler to `interpret' dynamic
terms.  In this sense, the partial evaluator is a modular extension to
the compiler.

Compare the output of some of the ctest with the corresponding
ptest. Use power?

One should note that the partial evaluator has fully reused the code
of the compiler. It is truly the `composition' of the interpreter and
the compiler.


the sv type problem: the inductive type function 'dv -> 'sv
its GADT solution. 
its type-level function solution: incope1.hs 

We need GADT -- or, to be more precise,
representation that varies with the type. We need a value of the type
|P Int| to have a different representation than the value of the type 
|P (Int->Int)|. GADT give you that (although we don't use the GADT in
their intended meaning for that). 

Drawbacks of the GADT solution. GADTs, although work, are still not
quite satsifactory: let us look at the following lines in incope.hs
\begin{code}
    app ef ea = case ef of
                        VF f -> f ea
                        E  f -> E (app f (abstr ea))
\end{code}
The datatype has four constructors, |VI|, |VB|, |VF| and |E|. How come we used
only two of them in the above expression. It is obvious, one may say,
that the constructors |VI| and |VB| just can't occur, because of the
typing consideration. This is obvious to us -- but it is not obvious
to the compiler. The case matching above does look \emph{partial}. That
seems like a tenuous point -- but it is precisely the main point of
the whole tag elimination approach. In a typed tagged interpreter
there are lots of pattern-matching that looks partial but in reality
total. The ultimate goal is to make this totality syntactically
apparent.


How can we do without GADT
Now show the Symantics signature in full, with the 'sv types




In the ideal world, the MetaOCaml signature (or Haskell type class)
Symantics would start with
\begin{code}
    kind tp = int | bool | (->) tp tp
    type ('c, 'dv:tp) repr : tp
    val int : int -> ('c, int) repr
    val bool : bool -> ('c, bool) repr
    val lam : (('c, 'da) repr -> ('c, 'db) repr) -> ('c, 'da -> 'db) repr
    val add : ('c, int) repr -> ('c, int) repr -> ('c, int) repr
\end{code}

Here "repr" is a tp-to-tp function, defined inductively in the case of
partial evaluation:

\begin{code}
    ('c, int     ) repr_pe = VI of int
                           | E of ('c, int) code
    ('c, bool    ) repr_pe = VB of bool
                           | E of ('c, bool) code
    ('c, 'a -> 'b) repr_pe = VF of ('c, 'a) repr -> ('c, 'b) repr
                           | E of ('c, 'a -> 'b) code
\end{code}

Unfortunately, we can't define type-level functions inductively.
Instead we bake |repr_pe| above into the signature Symantics, so repr
takes three type arguments |('c,'sv,'dv)|, where the extra argument 
|'sv| is equal to |('c,'dv) repr_pe.|
It seems Ken has just showed how to define type-level functions
inductively... This shows also how a limited form of GADTs (the ones
used for the sake of type-dependent representations) can be
implemented with just functions. 

So, the full signature of Symantics looks like (cf Section 2)

\begin{code}
module type Symantics = sig
  type ('c,'sv,'dv) repr
  val int  : int  -> ('c,int,int) repr
  val bool : bool -> ('c,bool,bool) repr
  val add  : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val mul  : ('c,int,int) repr -> ('c,int,int) repr -> ('c,int,int) repr
  val leq  : ('c,int,int) repr -> ('c,int,int) repr -> ('c,bool,bool) repr
  val if_ : ('c,bool,bool) repr ->
             (unit -> ('c,'sa,'da) repr) ->
             (unit -> ('c,'sa,'da) repr) -> ('c,'sa,'da) repr 

  val lam : (('c,'sa,'da) repr -> ('c,'sb,'db) repr)
    -> ('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr),'da->'db) repr
  val app : ('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr),'da->'db) repr
    -> ('c,'sa,'da) repr -> ('c,'sb,'db) repr
  val fix : (('c,(('c,'sa,'da) repr -> ('c,'sb,'db) repr) as 's,'da->'db) repr 
             -> ('c,'s,'da->'db) repr)  -> ('c,'s,'da->'db) repr
end
\end{code}



Also, unlike the GADT
solution, there is no longer any seemingly partial
pattern-matching. The pattern-match only occurs in PE (to check if the
code is static or dynamic), it is only a phase rather than a type tag
match, and the pattern match is \emph{patently} exhaustive. That is,
totality is assured and it is assured \emph{syntactically}, in the way that
is apparent to the compiler.


It's relation to PE and CPS. Currently, our CPS is CBN. Need to
investigate CBV.
The type-level function |'da -> 'sa| is important because it gives 
the CPS transform. So, the type-function
that enables the PE is exactly the same type function that enables
CPS....


Say that when doing PE on fix, we made a deliberate decision to `go
all the way' -- so long as the computation can proceed at the PE stage
(`static value'), we do that rather than residualizing. We have also
an alternative where fix is unrolled only once and them we
residualize. We have used the first approach, even it (unlike the
latter) may cause non-termination at the PE stage -- but \emph{only} if the
computation we are partially evaluating is non-terminating and enough
static information is available to trigger this non-termination.
Say that our treating of fix may lead to the code bloat in the
residual program. That is an issue investigated elsewhere (cite our
PEPM2006 paper). 


\section{Variations and discussion}


state and imperative features

CPS transformation (interpreting/compiling the term to CBN CPS)

We must formulate some propositions: a typed term makes progress in
any interpreter. The reason it is typed in any interpreter:
|forall r. Symantics r => r tau| directly says that in every model the
term is typed. 

The PE in incope is really a mixture of online and off-line; it looks 
online, but it has a sort of "just in time" binding-time analysis built in.

If we just add a method 'lift' to Symantics with a type
|lift :: a -> repr a| (cf polymorphic lift of Xi03, of MetaML and
MetaOCaml: CSP)
then |Symantics repr| implies |Functor repr|. We can write then
\begin{code}
  instance Semantics repr => Functor repr where
  -- fmap :: (a->b) -> repr a -> repr b
  fmap f ra = app (lift f) ra
\end{code}

\subsection{Translucent types}
The crucial role of the higher-order type parameter r

The type constructor "r" above represents a particular interpreter.  The
meta-type "r tau" hides how the interpreter represents the object type
"tau" yet exposes enough of the type information so we can type-check
the encoding of an object term without knowing what "r" is.  The checked
term is then well-typed in any interpreter.  Each instance of Symantics
instantiates "r" to interpret terms in a particular way. The L
interpreter is quite illustrative. We need the above semantics to be
able to represent both R and L (the latter returns only Int as the
values) in the same framework.

We encode a term like |add 1 2| as
|app _add (app _int 1) (app _int 2)| where |_add| and |_int| are just
`free variables'. Now, how to typecheck such a term? Some type should
be assigned to these free variables. The goal is to complete the work
without needing any type annotations (so we don't have to introduce any
type language), with all types inferred and all terms typed. It seems
the second-order type R neatly separates the typechecking part from
the representation of R: it hides aspects that depend on the
particular interpreter, and yet lets enough type information through
(via its type argument) to permit the typechecking of terms, and infer
all the types. 



The only approach that does seem to work
is the one in incope.hs or incope.ml. If we de-sugar away records and
type-classes, the type of a term L of the inferred type tau is

\begin{code}
  (Int -> r Int) ->
  (Bool -> r Bool) ->
  (forall alpha beta. (r alpha -> r beta) -> r (alpha->beta)) -> ... r tau
\end{code}

or, if we denote the sequence of initial arguments as S r, terms have
the type |S r -> r tau|
The interpreter has the type
|(forall r. S r -> r tau) -> r' tau|

The higher-order type (variable) of kind |*->*| seems essential. So, at
least we need some fragment of Fw (somehow our OCaml code manages to
avoid the full Fw; probably because the module language is separated
from the term language). Thus, we seem to need a fragment of Fw. It
seems the inference is possible, as our Haskell and OCaml code
constructively illustrates. Perhaps we need to characterize our
fragment.


\section{Self-interpretation}

Taha et al define a self-interpreter si to be an object-language term
such that, for any object-language term e,

\begin{code}
  si "e"    is observationally equivalent to    e
\end{code}

where |"e"| is an encoding of |e|.  Under the co-approach, we want to encode
each object-language term as a function from a record of cogen methods
to a generated rep.  (Thanks to currying, the object language need not
actually support records.)  In the case of a self-interpreter, that
record is simply si!  So we should define a self-interpreter si to be an
object-language record-term such that, for any object-language term e,

\begin{code}
  "e" si    is observationally equivalent to    e
\end{code}

The self-interpreter then is just like the interpreter
|R| in Section 2, except written at the object rather than meta level.

Note on terms and special forms. In our class Symantics, |add|
is not a term -- because it does not have the type repr t. We could
have introduced |add| to be of the type |repr (Int->Int->Int)| -- but we
didn't. So, |add| is a special form. It is to be applied in
meta-language (using the Haskell application -- white space) rather
than in the object language (app). 

Mention the RR interpreter: for any term E, (RR E) is equivalent to E.
RR is not an identity: it is an interpreter that encapsulates another
interpreter. The RR interpreter can be made self if we treat RR as a
special form and interpret it always with itself (similar to let and
hole below).


Self-interpretation in our framework is
a bit tricky due to a need for polymorphism.  An encoded term should
be polymorphic in the interpreter type-constructor "r".  Further, the
functions "lam", "app", etc. above should be polymorphic in the object
types "a" and "b".  Thus the object language seems to require rank-2
polymorphism; and so type annotations will be inevitable.


To neatly bypass the need for rank-2 polymorphism, we use the notion
of let-polymorphism and a `hole', as follows.  A partial evaluator (or
a compiler or interpreter) is an evaluation context that (let-)binds
variables named "app", "lam", etc.  For the object language to
take advantage of let-bound polymorphism in the metalanguage, it
is crucial that we encode object "let" to meta "let".  

We also (unfortunately need to) define a self-interpreter slightly
differently.  A self-interpreter |SI[]| is an object expression with a
hole |[]| waiting to be plugged with the encoding of an object expression.
The self-interpreter binds the free variables |_lam|, |_app|, |_int|, |_bool|,
etc. in the plugged encoding.  We require |SI["E"]| to be equivalent to |E|,
where |"E"| is the object encoding of the object expression |E|.

This strategy of course limits what kinds of processing on encoded
object expressions can be expressed in the object language.  For
example, unlike if we had simply defined |"E"| to be the `identity
encoding' E, we \emph{can} write a size-measurer |SZ[]|, which like |SI[]| is
an object expression with a hole |[]| waiting to be plugged with |"E"|.
However, this notion of size cannot distinguish |let x = E1 in E2[x]|
from |"E2[E1]"|.  Cf. one way to type-check polymorphic let:
\begin{code}
        E1:T    E2[E1]:T'
    ------------------------
    let x = E1 in E2[x] : T'
\end{code}
That analogy shows that the self-interpreter with a hole is a
euphemism for a higher-rank abstraction. That makes it clear where
the difficulty lies, and how to simulate the higher-rank application
(that is, higher-rank arrow elimination) `by hand'.


We also need to duplicate the encoding of an object expression if we
want to process the same object expression in multiple ways (e.g., both
measure its size and interpret it).  But all this is consistent with (in
fact, amplifies) our `coalgebraic/final/cogen' spirit, so our revised
notion of self-interpretation (and, some might say, of size measurement)
may just squeak by the reader's skepticism as we explain our desire for
kind polymorphism.

I guess the let-and-hole argument works out if we don't typecheck the
encoded terms. We can't typecheck the interpreter with the whole
either. We insert the encoded term into the interpreter -- and then
typecheck the whole thing. That'll work. Yet a bit unsatisfactory...


We achieve
self-interpretation in that the following evaluation context in the
object language defines an interpreter for encoded object terms.
\begin{code}
    let lam = \f -> f in
    let app = \f x -> f x in
    let add = \m n -> m + n in
    let int = \i -> i in
    [hole]
\end{code}
The obvious notion of optimality in this setup is easy to achieve.  For
example, given the context above, our partial evaluator turns
\begin{code}
    app (lam (\x -> add x (int 1))) (int 3)
\end{code}
into the number 4, as desired.  We can also encode the self-interpreter
above as the following evaluation context in the object language.
\begin{code}
    let lam = lam (\f -> f) in
    let app = lam (\f -> lam (\x -> app f x)) in
    let add = lam (\m -> lam (\n -> add m n)) in
    let int = lam (\i -> i) in
    [hole]
\end{code}
The "let" and the hole "[hole]" are meta-constructions, which must map to
themselves in any interpretation.  

The 'result' of applying an interpreter to itself, needs to be an 
interpreter too!  In fact, if it is Jones-optimal, then you should be 
able to apply it to itself again and get the same answer.  
Our code does that. First, |twice_inc_3_e| is the
metalanguage encoding of the object term |si["e"]|
where si is the self-interpreter of the object language (actually, only
lam,app,add,int), |""| means the object-to-object encoding, and e is the
object term
\begin{code}
  let twice_ f x = f (f x) in let inc_ n = n + 1 in twice_ inc_ 3
\end{code}

Second, |twice_inc_3_ee| is the metalanguage encoding of the object term
|si["si[e]"]|
which is the same as
|si["si"["e"]]|
The partial evaluator yields 5 on both |twice_inc_3_e| and 
|twice_inc_3_ee|,
as desired.



``But how 
to you create, in either Haskell or MetaOCaml, an untypechecked 
interpreter-with-a-hole [UIH] ?''
Well, one can make an argument that we already have such an
interpreter with polymorphic let and the hole: incope. In Haskell, the
declaration of an instance of Symantics is like the sequence of
polymorphic lets. We construct terms where lam, add, etc, are free
variables. We apply the interpreter to the semantics (plug the hole)
by instantiating these terms (binding the free variables lam, etc. to
the particular instance of Symantics). The unRR construction does
this plugging in explicitly.
Again, what is different from the above is that we \emph{can}
typecheck terms separately, without inserting them first within the
hole of a particular interpreter. Rank-2 type of |repr| helps. It lets
enough of the type information out so the typechecking can
proceed. So, |repr| is the representation of the polymorphic
interpreter context with the hole, which permits separately
typecheckable terms (the evaluation still entails `duplication' so to
speak -- which is one way how polymorphism is resolved).



\section{Related work}
Walid's Jones optimality paper. See his own ICFP02 paper that
described what he achieved then and what he \emph{did not}. He did not
eliminate all the tags. Ken's and mine impression of it:
(i) we can't write a \emph{direct} interpreter of the type language
           -- the only known to them, and \emph{partial} solution, was 
              that by Zhe Yang;
(ii) we have to use indirect interpreter, which needs the universal
           type and hence tagging;
(iii) that means any self-interpreter must have tags
(iv) that means not Jones-optimal.
It seems we should attack the point1 (in which case we don't have to
bother with self-interpreters). We say that we \emph{can} write the direct
interpreter, and without GADT or their equivalents (typeclasses).
Meta-language computes types, and the compiler and interpreter respect
those types and needs no tags.


Walid's ICFP02

Walid's PEPM07 (Concoqtion paper)? At leats at the Concoqtion
presentation at WG2.11, Emir used tagless interpreter as the main and
only example.

Danvy's self-interpreter
Ken has read Danvy et al's paper and said that the paper
doesn't seem to even make the claim that the source language is
typed. The interpreter is typed, but the source language doesn't seem
to be.
In sharp contrast to Danvy's paper, all the evaluators in our case
cannot produce any pattern-match error! Except for the partial
evaluator, there are NO patterns to match. And this is critical for
tag elimination -- pattern matching means there are no tags. And in
PE, the pattern-match deals with phases rather than with types. Also,
in our case the exhaustiveness of the pattern match is apparent to the
compiler -- that's why there are no tags.

Simon Peyton-Jones GADTs (the motivation for GADT is tagless
interpreter, but only for the first-order language)

Xi's POPL03 (He uses a higher-order language, one of the motivations
for his GRDT)

Asai; 
It [dynamic part when dealing with bool, int, lam in PE] is
Asai's trick for reducing the amount of reify/reflect back-and-forth
in a PE for an untyped language that allows you back-and-forth.  Or at
least that's how I read those papers.
Asai's approach is related to our PE (emulating typelevel functions):
Ken: ``In some sense we're just observing that Asai's code type-checks
-- in Hindley-Milner as soon as we deforest the static code
representation.''




 Simii et al.

"Boxes go Bananas" by Washburn and Weirich (intended or otherwise) Ken
wrote about this: ``but a glance reveals no type classes.  Do you mean
then that if we hide the definition |newtype R a = R a| from a client,
and export only the interpreter |instance Symantics R R|, then the
client would see R as an HOAS datatype a la Washburn and Weirich and
only be able to create parametric ("non-exotic") terms?''  Jacques
commented: ``I mean the HOAS parts, and how to deal with it (like in
the instance for |Symantics R R|).  There are other nice tricks for
dealing with HOAS in that paper worth a closer look.  It's all about
plain polymorphism, not type classes, but there are still some clever
ideas there, and they are about issues of tagging (an lack thereof).''




Typing Dep Typing plus two messages on Haskell-Cafe: typechecking 
from untyped to the typed form, ready to be interpreted. Walid's ICFP02
uses dependent types


Regarding our implementation of the type-level function |'sv -> 'dv|.
Gibbons et al paper on
the implementations of typecase may be relevant. I have not read it,
but I have the feeling some of their typecase designs may be
relevant (at least to cite). 
However, the typecase paper relies on having a 
bijection between types -- our |'sv| and |'dv| in this case, which is not 
our situation.  We have a one-sided view of things, where |'sv -> 'dv| is 
easy, and |'dv| "perfectly reflects" an |'sv|, but you still can't get at 
that |'sv|.


Self-Interpretation and Reflection in a Statically Typed Language 
\url{http://webpages.cs.luc.edu/~laufer/papers/reflection93.pdf}
He uses the SK language and some typeclasses. Their interpreter is a
meta-circular interpreter -- rather than self-interpreter. We can
claim to have implemented a far better interpreter for a far richer
language.

\section{Conclusions}

Our approach showed better the meaning of GADT and when they can be
avoided.

\subsection*{Acknowledgments}
We woul like to tank Martin Sulzmann for helpful discussions.

\end{document}
